{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's chat with a friend and I have a brain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo chat with Leolani that combine face recognition and storage in the BRAIN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/selbaez/Documents/PhD/CLTL/cltl-chatbots/venv-cltl-chatbots/lib/python3.7/site-packages/rdflib_jsonld/__init__.py:12: DeprecationWarning: The rdflib-jsonld package has been integrated into rdflib as of rdflib==6.0.1.  Please remove rdflib-jsonld from your project's dependencies.\n",
      "  DeprecationWarning,\n",
      "[nltk_data] Downloading package punkt to /Users/selbaez/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# general imports for EMISSOR and the BRAIN\n",
    "import emissor as em\n",
    "from cltl import brain\n",
    "from cltl.triple_extraction.api import Chat, UtteranceHypothesis\n",
    "from emissor.persistence import ScenarioStorage\n",
    "from emissor.representation.annotation import AnnotationType, Token, NER\n",
    "from emissor.representation.container import Index\n",
    "from emissor.representation.scenario import Modality, ImageSignal, TextSignal, Mention, Annotation, Scenario\n",
    "from cltl.brain.long_term_memory import LongTermMemory\n",
    "from cltl.combot.backend.api.discrete import UtteranceType\n",
    "\n",
    "# specific imports\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "import requests\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "src_path = os.path.abspath(os.path.join('..'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "    import sys\n",
    "import os\n",
    "\n",
    "src_path = os.path.abspath(os.path.join('..'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "#### The next utils are needed for the interaction and creating triples and capsules\n",
    "import util.driver_util as d_util\n",
    "import util.capsule_util as c_util\n",
    "import intentions.talk as talk\n",
    "import util.face_util as f_util\n",
    "import intentions.get_to_know_you as friend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Link your camera\n",
    "camera = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise the BRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-03 17:04:07,353 -     INFO -    cltl.brain.basic_brain.LongTermMemory - Uploading ontology to brain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-03 17:04:07.353 INFO basic_brain - upload_ontology: Uploading ontology to brain\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-03 17:04:08,978 -     INFO -   cltl.brain.basic_brain.TrustCalculator - Computed trust for all known agents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-03 17:04:08.978 INFO trust_calculator - compute_trust_network: Computed trust for all known agents\n"
     ]
    }
   ],
   "source": [
    "# Initialise the brain in GraphDB\n",
    "import pathlib\n",
    "\n",
    "log_path = pathlib.Path('./logs')\n",
    "my_brain = brain.LongTermMemory(address=\"http://localhost:7200/repositories/sandbox\",\n",
    "                                log_dir=log_path,\n",
    "                                clear_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard initialisation of a scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  ../../data/2021-11-03-17:04:09  Created \n",
      "Directory  ../../data/2021-11-03-17:04:09/image  Created \n"
     ]
    }
   ],
   "source": [
    "from random import getrandbits\n",
    "\n",
    "##### Setting the location\n",
    "place_id = getrandbits(8)\n",
    "location = requests.get(\"https://ipinfo.io\").json()\n",
    "\n",
    "##### Setting the agents\n",
    "AGENT = \"Leolani2\"\n",
    "HUMAN_NAME = \"Stranger\"\n",
    "HUMAN_ID = \"stranger\"\n",
    "\n",
    "### The name of your scenario\n",
    "scenario_id = datetime.today().strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "\n",
    "### Specify the path to an existing data folder where your scenario is created and saved as a subfolder\n",
    "scenario_path = \"../../data\"\n",
    "\n",
    "### Define the folder where the images are saved\n",
    "imagefolder = scenario_path + \"/\" + scenario_id + \"/\" + \"image\"\n",
    "\n",
    "### Create the scenario folder, the json files and a scenarioStorage and scenario in memory\n",
    "scenarioStorage = d_util.create_scenario(scenario_path, scenario_id)\n",
    "scenario = scenarioStorage.create_scenario(scenario_id, datetime.now().microsecond, datetime.now().microsecond, AGENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the docker containers for face detection and face property detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If the docker images are running you can skip the next part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You only need to load the dockers once. The first time you load the docker, the images will be donwloaded from the DockerHub. This may take a few minutes depending on the speed of the internet connection. The images are cached in your local Docker installation.\n",
    "\n",
    "One the images are in your local Docker, they are loaded instantaniously. Once the docker is started you do not need to start it again and you can skip the next commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#container_fdr = f_util.start_docker_container(\"tae898/face-detection-recognition:v0.1\", 10002)\n",
    "#container_ag = f_util.start_docker_container(\"tae898/age-gender:v0.2\", 10003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is a problem starting the dockers, you may need to kill them and start them again. Use the following command to kill and rerun the previous command. Note that if there are running already you should not restart. Starting it again gives an error that the port is occupied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!docker kill $(docker ps -q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting and storing triples from an utterance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function processes any textSignal using the baseline language model from the Leolani platform.\n",
    "This model uses a context-free grammar and closed-class lexicons specifically designed for social robot interaction.\n",
    "\n",
    "The function creates a capsule for posting any triples and perspective to the BRAIN.\n",
    "If no triples are extracted, a dummy prompt is returned: \"Any gossip?\".\n",
    "\n",
    "Any thoughts coming from the BRAIN are returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new friend.\n",
    "\n",
    "The functions in *intentions/get_to_know_you.py* are needed to get the properties and visual information for identifying a new friend.\n",
    "\n",
    "The visual information is based on the camera images of the uses from which we extract an averaged embedding.\n",
    "These embeddings are store in the *friend_embeddings* folder. \n",
    "\n",
    "By comparing an image with the stored embeddings, the system decides whether a person is a *stranger*.\n",
    "In case the user is a *stranger*, the system will try to get to know him/her.\n",
    "\n",
    "If you delete someone's embeddings from the *friend_embeddings* folder. This person will become a *stranger* again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-03 17:04:09.288 INFO face_util - load_binary_image: ../../data/2021-11-03-17:04:09/image/275270.png image loaded!\n",
      "2021-11-03 17:04:09.776 INFO face_util - run_face_api: got <Response [200]> from server!...\n",
      "2021-11-03 17:04:09.776 INFO face_util - run_face_api: 0 faces deteced!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'run_age_gender_api' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/k1/bs53rbv901qc_hqgfx1cqm5c0000gn/T/ipykernel_44813/1828202404.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mfaces_detected\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mdet_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     ) = f_util.do_stuff_with_image(imagepath)\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Here we assume that only one face is in the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/PhD/CLTL/cltl-chatbots/src/util/face_util.py\u001b[0m in \u001b[0;36mdo_stuff_with_image\u001b[0;34m(image_path, url_face, url_age_gender)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0mfaces_detected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m     \u001b[0mages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_age_gender_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl_age_gender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"annotating image ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_age_gender_api' is not defined"
     ]
    }
   ],
   "source": [
    "#First step is to identify the speaker\n",
    "\n",
    "imagepath = \"\"\n",
    "uuid_name = \"\"\n",
    "# First signal\n",
    "success, frame = camera.read()\n",
    "\n",
    "if success:\n",
    "    ### check if we know the human\n",
    "    current_time = str(datetime.now().microsecond)\n",
    "    imagepath = imagefolder + \"/\" + current_time + \".png\"\n",
    "    cv2.imwrite(imagepath, frame)\n",
    "    imageSignal = d_util.create_image_signal(scenario, imagepath)\n",
    "    (\n",
    "        genders,\n",
    "        ages,\n",
    "        bboxes,\n",
    "        faces_detected,\n",
    "        det_scores, embeddings,\n",
    "    ) = f_util.do_stuff_with_image(imagepath)\n",
    "\n",
    "    # Here we assume that only one face is in the image\n",
    "    # TODO: deal with multiple people.\n",
    "    for k, (gender, age, bbox, uuid_name, faceprob, embedding) in enumerate(\n",
    "            zip(genders, ages, bboxes, faces_detected, det_scores, embeddings)\n",
    "    ):\n",
    "        age = round(age[\"mean\"])\n",
    "        gender = \"male\" if gender[\"m\"] > 0.5 else \"female\"\n",
    "        bbox = [int(num) for num in bbox.tolist()]\n",
    "    assert k == 0\n",
    "    if uuid_name[\"name\"] is None:\n",
    "        ### This is a stranger\n",
    "        ### We call the get_to_know function to create the data for a new friend\n",
    "        HUMAN_ID, HUMAN_NAME, textSignal = friend.get_to_know_person(scenario, AGENT, gender, age, uuid_name, embedding)\n",
    "        ### We store the data in the BRAIN for the human_id that is returned            \n",
    "        name_thoughts, age_thoughts, gender_thoughts = friend.process_new_friend_and_think(scenario,\n",
    "                                                                                           place_id,\n",
    "                                                                                           location,\n",
    "                                                                                           HUMAN_ID,\n",
    "                                                                                           textSignal,\n",
    "                                                                                           imageSignal,\n",
    "                                                                                           age,\n",
    "                                                                                           gender,\n",
    "                                                                                           HUMAN_NAME,\n",
    "                                                                                           my_brain)\n",
    "\n",
    "        ### The system responds to the processing of the new name input and stores it as a textsignal\n",
    "        print(AGENT + f\": Nice to meet you, {HUMAN_NAME}\")\n",
    "        response = f\": Nice to meet you, {HUMAN_NAME}\"\n",
    "        textSignal = d_util.create_text_signal(scenario, response)\n",
    "        scenario.append_signal(textSignal)\n",
    "\n",
    "    else:\n",
    "        ### We know this person\n",
    "        HUMAN_ID = uuid_name['name']\n",
    "        HUMAN_NAME = HUMAN_ID.split(\"_t_\")[0]\n",
    "        response = f\"Hi {HUMAN_NAME}. Nice to see you again :)\"\n",
    "        print(f\"{AGENT}: {response}\")\n",
    "        textSignal = d_util.create_text_signal(scenario, response)\n",
    "        scenario.append_signal(textSignal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proceed to communicate with an identified friend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next while loop is for continuous communication with the identified user until *stop* or *bye*.\n",
    "We process the user input and the captured image sequentially step by setp in the code block of the while loop.\n",
    "At the end of the code block, a response is generated and new input from the user is requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leolani2: How are you doing Stranger\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " I am great\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stranger: I am great\n",
      "2021-11-03 17:04:20,465 -     INFO - cltl.triple_extraction.api.Chat (stranger)          000 - << Start of Chat with stranger >>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-03 17:04:20.465 INFO api - __init__: << Start of Chat with stranger >>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-03 17:04:20,467 -     INFO - cltl.triple_extraction.api.Chat (stranger)          000 - << Start of Chat with stranger >>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-03 17:04:20.467 INFO api - __init__: << Start of Chat with stranger >>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-03 17:04:20,469 -     INFO -               cltl.triple_extraction.api - Started POS tagger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-03 17:04:20.469 INFO api - __init__: Started POS tagger\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-03 17:04:20,471 -     INFO -               cltl.triple_extraction.api - Started NER tagger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-03 17:04:20.480 INFO ner - _start_server: Started NER server\n",
      "2021-11-03 17:04:20.471 INFO api - __init__: Started NER tagger\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-03 17:04:20,484 -     INFO -               cltl.triple_extraction.api - Loaded grammar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-03 17:04:20.484 INFO api - __init__: Loaded grammar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-03 17:04:21,566 -     INFO - cltl.triple_extraction.api.Chat (stranger)          001 -   stranger: \"I am great\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-03 17:04:21.566 INFO api - add_utterance:   stranger: \"I am great\"\n",
      "2021-11-03 17:04:21.578 INFO ner - _start_server: Started NER server\n",
      "2021-11-03 17:04:22.895 INFO analyzer - __init__: extracted perspective: {'sentiment': 0, 'certainty': 1, 'polarity': 1, 'emotion': <Emotion.NEUTRAL: 7>}\n",
      "2021-11-03 17:04:22.901 INFO api - analyze: RDF    subject: {\"label\": \"stranger\", \"type\": [\"noun.person\"]}\n",
      "2021-11-03 17:04:22.902 INFO api - analyze: RDF  predicate: {\"label\": \"be\", \"type\": [\"verb.stative\"]}\n",
      "2021-11-03 17:04:22.903 INFO api - analyze: RDF     object: {\"label\": \"great\", \"type\": [\"adj.all\"]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-03 17:04:26,041 -  WARNING -      cltl.brain.basic_brain.TypeReasoner - Failed to query Wikidata: HTTPSConnectionPool(host='query.wikidata.org', port=443): Read timed out. (read timeout=3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-03 17:04:26.041 WARNING type_reasoner - _exact_match_wikidata: Failed to query Wikidata: HTTPSConnectionPool(host='query.wikidata.org', port=443): Read timed out. (read timeout=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-03 17:04:29,154 -     INFO -      cltl.brain.basic_brain.TypeReasoner - Reasoned type of great to: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-03 17:04:29.154 INFO type_reasoner - reason_entity_type: Reasoned type of great to: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-03 17:04:29,211 -     INFO -    cltl.brain.basic_brain.LongTermMemory - Triple in statement: stranger_be_great [person_->_])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-03 17:04:29.211 INFO LTM_statement_processing - model_graphs: Triple in statement: stranger_be_great [person_->_])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-03 17:04:29,225 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Entity Novelty: new subject - new object \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-03 17:04:29.225 INFO thought_generator - fill_entity_novelty: Entity Novelty: new subject - new object \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-03 17:04:30,778 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Gaps: 26 gaps as subject: e.g. live-in location - 15 gaps as object: e.g. like-by agent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-03 17:04:30.778 INFO thought_generator - get_entity_gaps: Gaps: 26 gaps as subject: e.g. live-in location - 15 gaps as object: e.g. like-by agent\n",
      "2021-11-03 17:04:30.859 INFO face_util - load_binary_image: ../../data/2021-11-03-17:04:09/image/843105.png image loaded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leolani2: Any gossip?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-03 17:04:31.328 INFO face_util - run_face_api: got <Response [200]> from server!...\n",
      "2021-11-03 17:04:31.329 INFO face_util - run_face_api: 0 faces deteced!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'run_age_gender_api' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/k1/bs53rbv901qc_hqgfx1cqm5c0000gn/T/ipykernel_44813/579361256.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mdet_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         ) = f_util.do_stuff_with_image(imagepath)\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/PhD/CLTL/cltl-chatbots/src/util/face_util.py\u001b[0m in \u001b[0;36mdo_stuff_with_image\u001b[0;34m(image_path, url_face, url_age_gender)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0mfaces_detected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m     \u001b[0mages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_age_gender_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl_age_gender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"annotating image ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_age_gender_api' is not defined"
     ]
    }
   ],
   "source": [
    "### First prompt\n",
    "response = \"How are you doing \" + HUMAN_NAME\n",
    "textSignal = d_util.create_text_signal(scenario, response)\n",
    "scenario.append_signal(textSignal)\n",
    "\n",
    "print(AGENT + \": \" + response)\n",
    "\n",
    "#### Get the input from the user\n",
    "utterance = input(\"\\n\")\n",
    "print(HUMAN_NAME + \": \" + utterance)\n",
    "chat = Chat(HUMAN_ID)\n",
    "\n",
    "replier = None\n",
    "\n",
    "while not (utterance.lower() == \"stop\" or utterance.lower() == \"bye\"):\n",
    "    textSignal = d_util.create_text_signal(scenario, utterance)\n",
    "    scenario.append_signal(textSignal)\n",
    "\n",
    "    # @TODO: also annotate the textSignal\n",
    "    # Apply some processing to the textSignal and add annotations\n",
    "\n",
    "    # We process the utterance and store triples in the brain\n",
    "    # We catch the thoughts as the response\n",
    "    thoughts = talk.process_text_and_think(scenario,\n",
    "                                             place_id,\n",
    "                                             location,\n",
    "                                             textSignal,\n",
    "                                             HUMAN_ID, my_brain, replier)\n",
    "\n",
    "#     thoughts = talk.process_text_and_reply(scenario, place_id, location, HUMAN_ID, textSignal, chat, replier, my_brain)\n",
    "    print(AGENT + \": \" + thoughts)\n",
    "    textSignal = d_util.create_text_signal(scenario, thoughts)\n",
    "    scenario.append_signal(textSignal)\n",
    "\n",
    "    ## We capture the image again\n",
    "    ## For now, we only take pictures of faces of the user\n",
    "    ## @TODO add object recognition and check if there are other people detected than the user\n",
    "\n",
    "    success, frame = camera.read()\n",
    "    if success:\n",
    "        current_time = str(datetime.now().microsecond)\n",
    "        imagepath = imagefolder + \"/\" + current_time + \".png\"\n",
    "        cv2.imwrite(imagepath, frame)\n",
    "        (\n",
    "            genders,\n",
    "            ages,\n",
    "            bboxes,\n",
    "            faces_detected,\n",
    "            det_scores,\n",
    "            embeddings,\n",
    "        ) = f_util.do_stuff_with_image(imagepath)\n",
    "\n",
    "    if success:\n",
    "        imageSignal = d_util.create_image_signal(scenario, imagepath)\n",
    "        container_id = str(uuid.uuid4())\n",
    "\n",
    "        #### Properties are now stored as annotations\n",
    "        #### We do not store these proeprties again to the BRAIN\n",
    "        for gender, age, bbox, name, faceprob in zip(\n",
    "                genders, ages, bboxes, faces_detected, det_scores\n",
    "        ):\n",
    "            age = round(age[\"mean\"])\n",
    "            gender = \"male\" if gender[\"m\"] > 0.5 else \"female\"\n",
    "            bbox = [int(num) for num in bbox.tolist()]\n",
    "\n",
    "        f_util.add_face_annotation(imageSignal, container_id, \"front_camera\", container_id, current_time,\n",
    "                                   bbox, HUMAN_ID, HUMAN_NAME, age, gender, faceprob)\n",
    "\n",
    "        scenario.append_signal(imageSignal)\n",
    "\n",
    "        ### We created a perceivedBy triple for this experience, \n",
    "        ### @TODO we need to include the bouding box somehow in the object\n",
    "        capsule = c_util.scenario_image_triple_to_capsule(scenario,\n",
    "                                                          place_id,\n",
    "                                                          location,\n",
    "                                                          imageSignal,\n",
    "                                                          \"front_camera\",\n",
    "                                                          human_id,\n",
    "                                                          \"perceivedBy\",\n",
    "                                                          imageSignal.id)\n",
    "\n",
    "        response = my_brain.update(capsule, reason_types=True, create_label=False)\n",
    "        ## use this version if you want to use the URI as subject\n",
    "\n",
    "    # Create the response from the system and store this as a new signal\n",
    "    # We could use the throughts to respond\n",
    "    # @TODO generate a response from the thoughts or based on the user query\n",
    "\n",
    "    # Getting the next input signals\n",
    "    utterance = input(\"\\n\")\n",
    "\n",
    "    ### We now have a new input check if the user wants to continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the end time of the scenario, save it and stop the containers\n",
    "\n",
    "After we stopped the interaction, we set the end time and save the scenario as EMISSOR data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scenario.scenario.end = datetime.now().microsecond\n",
    "scenarioStorage.save_scenario(scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stopping the docker containers\n",
    "### This is only needed of you started them in this notebook\n",
    "\n",
    "#f_util.kill_container(container_fdr)\n",
    "#f_util.kill_container(container_ag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Stop the camera when we are done\n",
    "camera.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-cltl-chatbots",
   "language": "python",
   "name": "venv-cltl-chatbots"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
