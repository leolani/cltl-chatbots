{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's chat with a friend and I have a brain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo chat with Leolani that combine face recognition and storage in the BRAIN. This notebook combines the functions of two other notebooks:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tkb/automatic/workspaces/robo/leolani-project/cltl-chatbots/venv/lib/python3.7/site-packages/rdflib_jsonld/__init__.py:12: DeprecationWarning: The rdflib-jsonld package has been integrated into rdflib as of rdflib==6.0.1.  Please remove rdflib-jsonld from your project's dependencies.\n",
      "  DeprecationWarning,\n",
      "[nltk_data] Downloading package punkt to /Users/tkb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# general imports for EMISSOR and the BRAIN\n",
    "import emissor as em\n",
    "from cltl import brain\n",
    "from cltl.triple_extraction.api import Chat, UtteranceHypothesis\n",
    "from emissor.persistence import ScenarioStorage\n",
    "from emissor.representation.annotation import AnnotationType, Token, NER\n",
    "from emissor.representation.container import Index\n",
    "from emissor.representation.scenario import Modality, ImageSignal, TextSignal, Mention, Annotation, Scenario\n",
    "from cltl.brain.long_term_memory import LongTermMemory\n",
    "from cltl.combot.backend.api.discrete import UtteranceType\n",
    "\n",
    "# specific imports\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "import requests\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "src_path = os.path.abspath(os.path.join('..'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "    import sys\n",
    "import os\n",
    "\n",
    "src_path = os.path.abspath(os.path.join('..'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "#### The next utils are needed for the interaction and creating triples and capsules\n",
    "import chatbots.util.driver_util as d_util\n",
    "import chatbots.util.capsule_util as c_util\n",
    "import chatbots.intentions.talk as talk\n",
    "import chatbots.util.face_util as f_util\n",
    "import chatbots.intentions.get_to_know_you as friend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Link your camera\n",
    "camera = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise the BRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "2021-11-11 23:35:23,348 -     INFO -    cltl.brain.basic_brain.LongTermMemory - Uploading ontology to brain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 23:35:23.348 INFO basic_brain - upload_ontology: Uploading ontology to brain\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "2021-11-11 23:35:25,506 -     INFO -   cltl.brain.basic_brain.TrustCalculator - Computed trust for all known agents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 23:35:25.506 INFO trust_calculator - compute_trust_network: Computed trust for all known agents\n"
     ]
    }
   ],
   "source": [
    "# Initialise the brain in GraphDB\n",
    "import pathlib\n",
    "\n",
    "log_path = pathlib.Path('./logs')\n",
    "my_brain = brain.LongTermMemory(address=\"http://localhost:7200/repositories/sandbox\",\n",
    "                                log_dir=log_path,\n",
    "                                clear_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard initialisation of a scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories for 2021-11-11-23:35:25 created in ../../data\n"
     ]
    }
   ],
   "source": [
    "from random import getrandbits\n",
    "\n",
    "##### Setting the location\n",
    "place_id = getrandbits(8)\n",
    "location = requests.get(\"https://ipinfo.io\").json()\n",
    "\n",
    "##### Setting the agents\n",
    "AGENT = \"Leolani2\"\n",
    "HUMAN_NAME = \"Stranger\"\n",
    "HUMAN_ID = \"stranger\"\n",
    "\n",
    "### The name of your scenario\n",
    "scenario_id = datetime.today().strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "\n",
    "### Specify the path to an existing data folder where your scenario is created and saved as a subfolder\n",
    "scenario_path = \"../../data\"\n",
    "\n",
    "### Define the folder where the images are saved\n",
    "imagefolder = scenario_path + \"/\" + scenario_id + \"/\" + \"image\"\n",
    "\n",
    "### Create the scenario folder, the json files and a scenarioStorage and scenario in memory\n",
    "scenarioStorage = d_util.create_scenario(scenario_path, scenario_id)\n",
    "scenario = scenarioStorage.create_scenario(scenario_id, datetime.now().microsecond, datetime.now().microsecond, AGENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the docker containers for face detection and face property detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If the docker images are running you can skip the next part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You only need to load the dockers once. The first time you load the docker, the images will be donwloaded from the DockerHub. This may take a few minutes depending on the speed of the internet connection. The images are cached in your local Docker installation.\n",
    "\n",
    "One the images are in your local Docker, they are loaded instantaniously. Once the docker is started you do not need to start it again and you can skip the next commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# container_fdr = f_util.start_docker_container(\"tae898/face-detection-recognition:v0.1\", 10002)\n",
    "# container_ag = f_util.start_docker_container(\"tae898/age-gender:v0.2\", 10003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is a problem starting the dockers, you may need to kill them and start them again. Use the following command to kill and rerun the previous command. Note that if there are running already you should not restart. Starting it again gives an error that the port is occupied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!docker kill $(docker ps -q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting and storing triples from an utterance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function processes any textSignal using the baseline language model from the Leolani platform.\n",
    "This model uses a context-free grammar and closed-class lexicons specifically designed for social robot interaction.\n",
    "\n",
    "The function creates a capsule for posting any triples and perspective to the BRAIN.\n",
    "If no triples are extracted, a dummy prompt is returned: \"Any gossip?\".\n",
    "\n",
    "Any thoughts coming from the BRAIN are returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new friend.\n",
    "\n",
    "The functions in *intentions/get_to_know_you.py* are needed to get the properties and visual information for identifying a new friend.\n",
    "\n",
    "The visual information is based on the camera images of the uses from which we extract an averaged embedding.\n",
    "These embeddings are store in the *friend_embeddings* folder. \n",
    "\n",
    "By comparing an image with the stored embeddings, the system decides whether a person is a *stranger*.\n",
    "In case the user is a *stranger*, the system will try to get to know him/her.\n",
    "\n",
    "If you delete someone's embeddings from the *friend_embeddings* folder. This person will become a *stranger* again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 23:37:04.172 INFO face_util - load_binary_image: ../../data/2021-11-11-23:35:25/image/136566.png image loaded!\n",
      "2021-11-11 23:37:05.578 INFO face_util - run_face_api: got <Response [200]> from server!...\n",
      "2021-11-11 23:37:05.580 INFO face_util - run_face_api: 1 faces deteced!\n",
      "2021-11-11 23:37:05.688 INFO face_util - run_age_gender_api: got <Response [200]> from server!...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stranger\n",
      "Leolani2: Hi Thomas. Nice to see you again :)\n"
     ]
    }
   ],
   "source": [
    "#First step is to identify the speaker\n",
    "\n",
    "friends_path = \"../../friend_embeddings\"\n",
    "imagepath = \"\"\n",
    "uuid_name = \"\"\n",
    "# First signal\n",
    "success, frame = camera.read()\n",
    "\n",
    "if success:\n",
    "    ### check if we know the human\n",
    "    current_time = str(datetime.now().microsecond)\n",
    "    imagepath = imagefolder + \"/\" + current_time + \".png\"\n",
    "    cv2.imwrite(imagepath, frame)\n",
    "    image_bbox = (0, 0, frame.shape[1], frame.shape[0])\n",
    "    imageSignal = d_util.create_image_signal(scenario, imagepath, image_bbox)\n",
    "    faces = f_util.detect_faces(friends_path, imagepath)\n",
    "\n",
    "    # Here we assume that only one face is in the image\n",
    "    # TODO: deal with multiple people.\n",
    "    if len(faces) != 1:\n",
    "        raise ValueError(\"Only single face is supported, detected \" + len(faces))\n",
    "\n",
    "    face = faces[0]\n",
    "    age = round(face.age[\"mean\"])\n",
    "    gender = \"male\" if face.gender[\"m\"] > 0.5 else \"female\"\n",
    "    bbox = [int(num) for num in face.bbox.tolist()]\n",
    "    uuid_name = face.face_id\n",
    "    faceprob = face.det_score\n",
    "    embedding = face.embedding\n",
    "\n",
    "    if uuid_name[\"name\"] is None:\n",
    "        ### This is a stranger\n",
    "        ### We call the get_to_know function to create the data for a new friend\n",
    "        HUMAN_ID, HUMAN_NAME, textSignal = friend.get_to_know_person(scenario, AGENT, gender, age, uuid_name,\n",
    "                                                                     embedding, friends_path)\n",
    "        ### We store the data in the BRAIN for the human_id that is returned            \n",
    "        name_thoughts, age_thoughts, gender_thoughts = friend.process_new_friend_and_think(scenario,\n",
    "                                                                                           place_id,\n",
    "                                                                                           location,\n",
    "                                                                                           HUMAN_ID,\n",
    "                                                                                           textSignal,\n",
    "                                                                                           imageSignal,\n",
    "                                                                                           age,\n",
    "                                                                                           gender,\n",
    "                                                                                           HUMAN_NAME,\n",
    "                                                                                           my_brain)\n",
    "\n",
    "        ### The system responds to the processing of the new name input and stores it as a textsignal\n",
    "        print(AGENT + f\": Nice to meet you, {HUMAN_NAME}\")\n",
    "        response = f\": Nice to meet you, {HUMAN_NAME}\"\n",
    "        textSignal = d_util.create_text_signal(scenario, response)\n",
    "        scenario.append_signal(textSignal)\n",
    "\n",
    "    else:\n",
    "        print(HUMAN_NAME)\n",
    "        ### We know this person\n",
    "        HUMAN_ID = uuid_name['name']\n",
    "        HUMAN_NAME = HUMAN_ID.split(\"_t_\")[0]\n",
    "        response = f\"Hi {HUMAN_NAME}. Nice to see you again :)\"\n",
    "        print(f\"{AGENT}: {response}\")\n",
    "        textSignal = d_util.create_text_signal(scenario, response)\n",
    "        scenario.append_signal(textSignal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proceed to communicate with an identified friend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next while loop is for continuous communication with the identified user until *stop* or *bye*.\n",
    "We process the user input and the captured image sequentially step by setp in the code block of the while loop.\n",
    "At the end of the code block, a response is generated and new input from the user is requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leolani2: How are you doing Thomas\n",
      "\n",
      "Fine\n",
      "Thomas: Fine\n",
      "2021-11-11 23:40:34,384 -     INFO - cltl.triple_extraction.api.Chat (Thomas_t_1636668679423) 000 - << Start of Chat with Thomas_t_1636668679423 >>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 23:40:34.384 INFO api - __init__: << Start of Chat with Thomas_t_1636668679423 >>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "2021-11-11 23:40:34,387 -     INFO - cltl.triple_extraction.api.Chat (Thomas_t_1636668679423) 000 - << Start of Chat with Thomas_t_1636668679423 >>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 23:40:34.387 INFO api - __init__: << Start of Chat with Thomas_t_1636668679423 >>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "2021-11-11 23:40:35,962 -     INFO - cltl.triple_extraction.api.Chat (Thomas_t_1636668679423) 001 - Thomas_t_1636668679423: \"Fine\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 23:40:35.962 INFO api - add_utterance: Thomas_t_1636668679423: \"Fine\"\n",
      "2021-11-11 23:40:35.965 WARNING analyzer - analyze: Couldn't parse input\n",
      "2021-11-11 23:40:36.085 INFO face_util - load_binary_image: ../../data/2021-11-11-23:35:25/image/49716.png image loaded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leolani2: Any gossip?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 23:40:37.483 INFO face_util - run_face_api: got <Response [200]> from server!...\n",
      "2021-11-11 23:40:37.485 INFO face_util - run_face_api: 1 faces deteced!\n",
      "2021-11-11 23:40:37.575 INFO face_util - run_age_gender_api: got <Response [200]> from server!...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "2021-11-11 23:40:37,758 -     INFO -    cltl.brain.basic_brain.LongTermMemory - Triple in statement: thomas-t-1636668679423_perceivedby_ba7ee751-a462-4cca-96a5-2bd6ed142bf1 [person_->_string])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 23:40:37.758 INFO LTM_statement_processing - model_graphs: Triple in statement: thomas-t-1636668679423_perceivedby_ba7ee751-a462-4cca-96a5-2bd6ed142bf1 [person_->_string])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "2021-11-11 23:40:37,809 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Entity Novelty: new subject - new object \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 23:40:37.809 INFO thought_generator - fill_entity_novelty: Entity Novelty: new subject - new object \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thomas likes pizza\n",
      "2021-11-11 23:40:48,108 -     INFO - cltl.triple_extraction.api.Chat (Thomas_t_1636668679423) 000 - << Start of Chat with Thomas_t_1636668679423 >>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 23:40:48.108 INFO api - __init__: << Start of Chat with Thomas_t_1636668679423 >>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "2021-11-11 23:40:49,806 -     INFO - cltl.triple_extraction.api.Chat (Thomas_t_1636668679423) 001 - Thomas_t_1636668679423: \"Thomas likes pizza\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 23:40:49.806 INFO api - add_utterance: Thomas_t_1636668679423: \"Thomas likes pizza\"\n",
      "2021-11-11 23:40:49.823 INFO ner - _start_server: Started NER server\n",
      "2021-11-11 23:40:51.797 INFO analyzer - __init__: extracted perspective: {'sentiment': '0.75', 'certainty': 1, 'polarity': 1, 'emotion': <Emotion.NEUTRAL: 7>}\n",
      "2021-11-11 23:40:51.806 INFO api - analyze: RDF    subject: {\"label\": \"thomas\", \"type\": [\"noun.person\"]}\n",
      "2021-11-11 23:40:51.807 INFO api - analyze: RDF  predicate: {\"label\": \"like\", \"type\": [\"prep\"]}\n",
      "2021-11-11 23:40:51.808 INFO api - analyze: RDF     object: {\"label\": \"pizza\", \"type\": [\"noun.food\"]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "2021-11-11 23:40:52,235 -     INFO -    cltl.brain.basic_brain.LongTermMemory - Triple in statement: thomas_like_pizza [person_->_food])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 23:40:52.235 INFO LTM_statement_processing - model_graphs: Triple in statement: thomas_like_pizza [person_->_food])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "2021-11-11 23:40:52,521 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Entity Novelty: new subject - new object \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 23:40:52.521 INFO thought_generator - fill_entity_novelty: Entity Novelty: new subject - new object \n",
      "2021-11-11 23:40:55.414 INFO face_util - load_binary_image: ../../data/2021-11-11-23:35:25/image/379092.png image loaded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leolani2: Any gossip?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 23:40:56.980 INFO face_util - run_face_api: got <Response [200]> from server!...\n",
      "2021-11-11 23:40:56.982 INFO face_util - run_face_api: 1 faces deteced!\n",
      "2021-11-11 23:40:57.077 INFO face_util - run_age_gender_api: got <Response [200]> from server!...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "2021-11-11 23:40:57,187 -     INFO -    cltl.brain.basic_brain.LongTermMemory - Triple in statement: thomas-t-1636668679423_perceivedby_b1a90df2-d69f-4031-8f72-4b2b8d906006 [person_->_string])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 23:40:57.187 INFO LTM_statement_processing - model_graphs: Triple in statement: thomas-t-1636668679423_perceivedby_b1a90df2-d69f-4031-8f72-4b2b8d906006 [person_->_string])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "2021-11-11 23:40:57,267 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Entity Novelty: existing subject - new object \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 23:40:57.267 INFO thought_generator - fill_entity_novelty: Entity Novelty: existing subject - new object \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "2021-11-11 23:40:59,724 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Gaps: 26 gaps as subject: e.g. experience taste - 15 gaps as object: e.g. cook-by food\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 23:40:59.724 INFO thought_generator - get_entity_gaps: Gaps: 26 gaps as subject: e.g. experience taste - 15 gaps as object: e.g. cook-by food\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Who likes pizza?\n",
      "2021-11-11 23:41:09,124 -     INFO - cltl.triple_extraction.api.Chat (Thomas_t_1636668679423) 000 - << Start of Chat with Thomas_t_1636668679423 >>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 23:41:09.124 INFO api - __init__: << Start of Chat with Thomas_t_1636668679423 >>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "2021-11-11 23:41:11,016 -     INFO - cltl.triple_extraction.api.Chat (Thomas_t_1636668679423) 001 - Thomas_t_1636668679423: \"Who likes pizza?\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 23:41:11.016 INFO api - add_utterance: Thomas_t_1636668679423: \"Who likes pizza?\"\n",
      "2021-11-11 23:41:11.022 INFO api - analyze: RDF    subject: {\"label\": \"pizza\", \"type\": [\"noun.food\"]}\n",
      "2021-11-11 23:41:11.024 INFO api - analyze: RDF  predicate: {\"label\": \"like\", \"type\": [\"prep\"]}\n",
      "2021-11-11 23:41:11.027 INFO api - analyze: RDF     object: {\"label\": \"\", \"type\": []}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9x/c6scx58d7tlfyghrvt2l30hc0000gn/T/ipykernel_17902/452064016.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m                                              \u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                                              \u001b[0mtextSignal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                                              HUMAN_ID, my_brain, replier)\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#     thoughts = talk.process_text_and_reply(scenario, place_id, location, HUMAN_ID, textSignal, chat, replier, my_brain)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/automatic/workspaces/robo/leolani-project/cltl-chatbots/src/chatbots/intentions/talk.py\u001b[0m in \u001b[0;36mprocess_text_and_think\u001b[0;34m(scenario, place_id, location, textSignal, human_id, my_brain, replier)\u001b[0m\n\u001b[1;32m    154\u001b[0m                                                                   chat.last_utterance.triple)\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_brain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapsule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0mresponse_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrain_response_to_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/automatic/workspaces/robo/leolani-project/cltl-chatbots/venv/lib/python3.7/site-packages/cltl/brain/long_term_memory.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, capsule, reason_types, create_label)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mcapsule\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'triple'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rdf_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_triple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapsule\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'subject'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapsule\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predicate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapsule\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mcapsule\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'perspective'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rdf_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_perspective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapsule\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'perspective'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;34m'perspective'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcapsule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rdf_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_perspective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mcapsule\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUtteranceType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTATEMENT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/automatic/workspaces/robo/leolani-project/cltl-chatbots/venv/lib/python3.7/site-packages/cltl/brain/infrastructure/rdf_builder.py\u001b[0m in \u001b[0;36mfill_perspective\u001b[0;34m(self, perpective_dict)\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0mPerspective\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mfactuality\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiment\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0memotion\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \"\"\"\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0mcertainty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperpective_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'certainty'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCertainty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNDERSPECIFIED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m         \u001b[0mpolarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperpective_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'polarity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPolarity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNDERSPECIFIED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0msentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperpective_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNDERSPECIFIED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "### First prompt\n",
    "response = \"How are you doing \" + HUMAN_NAME\n",
    "textSignal = d_util.create_text_signal(scenario, response)\n",
    "scenario.append_signal(textSignal)\n",
    "\n",
    "print(AGENT + \": \" + response)\n",
    "\n",
    "#### Get the input from the user\n",
    "utterance = input(\"\\n\")\n",
    "print(HUMAN_NAME + \": \" + utterance)\n",
    "chat = Chat(HUMAN_ID)\n",
    "\n",
    "replier = None\n",
    "\n",
    "while not (utterance.lower() == \"stop\" or utterance.lower() == \"bye\"):\n",
    "    textSignal = d_util.create_text_signal(scenario, utterance)\n",
    "    scenario.append_signal(textSignal)\n",
    "\n",
    "    # @TODO: also annotate the textSignal\n",
    "    # Apply some processing to the textSignal and add annotations\n",
    "\n",
    "    # We process the utterance and store triples in the brain\n",
    "    # We catch the thoughts as the response\n",
    "    thoughts = talk.process_text_and_think(scenario,\n",
    "                                             place_id,\n",
    "                                             location,\n",
    "                                             textSignal,\n",
    "                                             HUMAN_ID, my_brain, replier)\n",
    "\n",
    "#     thoughts = talk.process_text_and_reply(scenario, place_id, location, HUMAN_ID, textSignal, chat, replier, my_brain)\n",
    "    print(AGENT + \": \" + thoughts)\n",
    "    textSignal = d_util.create_text_signal(scenario, thoughts)\n",
    "    scenario.append_signal(textSignal)\n",
    "\n",
    "    ## We capture the image again\n",
    "    ## For now, we only take pictures of faces of the user\n",
    "    ## @TODO add object recognition and check if there are other people detected than the user\n",
    "\n",
    "    success, frame = camera.read()\n",
    "    if success:\n",
    "        current_time = str(datetime.now().microsecond)\n",
    "        imagepath = imagefolder + \"/\" + current_time + \".png\"\n",
    "        cv2.imwrite(imagepath, frame)\n",
    "        faces = f_util.detect_faces(friends_path, imagepath)\n",
    "\n",
    "        image_bbox = (0, 0, frame.shape[1], frame.shape[0])\n",
    "        imageSignal = d_util.create_image_signal(scenario, imagepath, image_bbox)\n",
    "        container_id = str(uuid.uuid4())\n",
    "\n",
    "        #### Properties are now stored as annotations\n",
    "        #### We do not store these proeprties again to the BRAIN\n",
    "        # Here we assume that only one face is in the image\n",
    "        # TODO: deal with multiple people.\n",
    "        if len(faces) != 1:\n",
    "            raise ValueError(\"Only single face is supported, detected \" + len(faces))\n",
    "\n",
    "        face = faces[0]\n",
    "        age = round(face.age[\"mean\"])\n",
    "        gender = \"male\" if face.gender[\"m\"] > 0.5 else \"female\"\n",
    "        bbox = [int(num) for num in face.bbox.tolist()]\n",
    "        uuid_name = face.face_id\n",
    "        faceprob = face.det_score\n",
    "        embedding = face.embedding\n",
    "\n",
    "        mention = f_util.create_face_mention(imageSignal, \"front_camera\", current_time,\n",
    "                                   bbox, HUMAN_ID, HUMAN_NAME, age, gender, faceprob)\n",
    "        imageSignal.mentions.append(mention)\n",
    "        scenario.append_signal(imageSignal)\n",
    "\n",
    "        ### We created a perceivedBy triple for this experience, \n",
    "        ### @TODO we need to include the bouding box somehow in the object\n",
    "        capsule = c_util.scenario_image_triple_to_capsule(scenario,\n",
    "                                                          place_id,\n",
    "                                                          location,\n",
    "                                                          imageSignal,\n",
    "                                                          \"front_camera\",\n",
    "                                                          HUMAN_ID,\n",
    "                                                          \"perceivedBy\",\n",
    "                                                          imageSignal.id)\n",
    "\n",
    "        response = my_brain.update(capsule, reason_types=True, create_label=False)\n",
    "        ## use this version if you want to use the URI as subject\n",
    "\n",
    "    # Create the response from the system and store this as a new signal\n",
    "    # We could use the throughts to respond\n",
    "    # @TODO generate a response from the thoughts or based on the user query\n",
    "\n",
    "    # Getting the next input signals\n",
    "    utterance = input(\"\\n\")\n",
    "\n",
    "    ### We now have a new input check if the user wants to continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the end time of the scenario, save it and stop the containers\n",
    "\n",
    "After we stopped the interaction, we set the end time and save the scenario as EMISSOR data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scenario.scenario.end = datetime.now().microsecond\n",
    "scenarioStorage.save_scenario(scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stopping the docker containers\n",
    "### This is only needed of you started them in this notebook\n",
    "\n",
    "#f_util.kill_container(container_fdr)\n",
    "#f_util.kill_container(container_ag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Stop the camera when we are done\n",
    "camera.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
