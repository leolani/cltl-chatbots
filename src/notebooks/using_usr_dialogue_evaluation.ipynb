{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using USR to score dialogues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://shikib.com/usr\n",
    "\n",
    "Downloaded the models locally for speed but also because the config.json for the \"uk\" and \"ctx\" models lack the model_type parameter. They also lack the finetuning_task setting although this is not needed for doing predictions. Add the following config attributes to both the \"uk\" and \"ctx\" model config.json files:\n",
    "\n",
    "```\n",
    "  \"model_type\": \"roberta\",\n",
    "  \"finetuning_task\": \"qqp\",\n",
    "```\n",
    "\n",
    "Checking out the training and development data in the folder \"both\", I suspect it is better to downcase all text and use \"_eos\t_go\" to separate the context from the target sentence and to end the target sentence with \"_eos\"\n",
    "\n",
    "```\n",
    "0\t1\t2\t_nofact _eos\t_go i like hockey and soccer . what teams do you support ? _eos\t0\n",
    "0\t1\t2\tamazon ceo jeff bezos built a clock into a mountain that should run for 10,000 years . _eos\t_go i think he is a great ceo , he is a great ceo and a genius _eos\t0\n",
    "```\n",
    "\n",
    "There are three models given, which all have two classes as output:\n",
    "\n",
    "* MLM metric that estimates the likelihood of a response (fine tuned on Topical-Chat or PersonChat), likelihood is used as a measure for the \"Understandability\" and the \"Naturalness\" of a response: The input sequence to MLM is a concatenation of a dialog context, c, and a response, r. One word at a time, each word in r is masked and its log likelihood is computed. Model name = roberta_ft, test data is given in undr/test.lm\":\n",
    "\n",
    "```\n",
    "yeah i would feel bad too . bad sportsmanship . is n't it odd that pro bowlers used to make more money than pro football players in the 1960 's ! _eos _go yeah , i guess football has changed so much . i wonder if bowling is more popular in the 60 's than football . _eos\n",
    "yeah i would feel bad too . bad sportsmanship . is n't it odd that pro bowlers used to make more money than pro football players in the 1960 's ! _eos _go yeah i guess so . do you like fantasy ? _eos\n",
    "\n",
    "```\n",
    "\n",
    "* Dialog retrieval (DR) for evaluating generative models is an intuitive choice, especially for metrics like \"Maintains Context\", \"Interesting\" and \"Uses Knowledge\". The model fine-tuned for likelihood, is further fine-tuned for the retrieval task.  The model is trained given a context x, a response r, and a binary label y indicating whether r is the true response or randomly sampled. The context x may consist of the dialog history and the fact, denoted c, or just the fact, denoted f .\n",
    "* Model name \"uk\": \"Uses Knowledge\", trained from the data the *fct* folder:\n",
    "\n",
    "```\n",
    "0\t1\t2\tin september of 2010 , the united nations appointed official ambassador to extraterrestrials in they case they would ever make contact with earth _eos\t_go i 'm not sure . i wonder if the un has an ambassador to aliens . _eos\t0\n",
    "0\t1\t2\tin september of 2010 , the united nations appointed official ambassador to extraterrestrials in they case they would ever make contact with earth _eos\t_go i think it 's because of the atmosphere , it 's not all that old . _eos\t0\n",
    "0\t1\t2\tin september of 2010 , the united nations appointed official ambassador to extraterrestrials in they case they would ever make contact with earth _eos\t_go i do n't know , but maybe it 's due to the fact that we have no longer have an ambassador to extraterrestrials _eos\t0\n",
    "0\t1\t2\tin september of 2010 , the united nations appointed official ambassador to extraterrestrials in they case they would ever make contact with earth _eos\t_go i think that was for sure , we should grow , i wonder what planet they are able to make the earth , they will have an alien ambassador to extraterrestrials _eos\t0\n",
    "0\t1\t2\tin september of 2010 , the united nations appointed official ambassador to extraterrestrials in they case they would ever make contact with earth _eos\t_go wow . the un appointed an official ambassador to aliens ! maybe we can ask them for help if we run out of helium on earth . _eos\t0\n",
    "0\t1\t2\t_nofact _eos\t_go i do n't really know much about sports . i do like to watch the olympics and i have been swimming in the summer olympics . _eos\t0\n",
    "0\t1\t2\t_nofact _eos\t_go i have never swam competitively , but i did n't have it . i do like it though . _eos\t0\n",
    "0\t1\t2\t_nofact _eos\t_go i do not but i am more into swimming myself . i do n't like sports , but i do know there are some really boring swimming competitions _eos\t0\n",
    "0\t1\t2\t_nofact _eos\t_go yes . i think that 's why i live in the usa . i 've seen some swimming around the world where i live . _eos\t0\n",
    "0\t1\t2\t_nofact _eos\t_go i like hockey and soccer . what teams do you support ? _eos\t0\n",
    "```\n",
    "* Mode name \"ctx\": \"The context x is the dialog history, trained from *both*, \"_nofact\":\n",
    "\n",
    "```\n",
    "0\t1\t2\tthanks , grandpa ! i bet grandpa wishes he had cashed them in before he cashed out . _eos _nofact _eos\t_go i bet he was a great player . nice chat _eos\t0\n",
    "0\t1\t2\tthanks , grandpa ! i bet grandpa wishes he had cashed them in before he cashed out . _eos _nofact _eos\t_go i wonder if he was a fan of his music ? _eos\t0\n",
    "0\t1\t2\tthanks , grandpa ! i bet grandpa wishes he had cashed them in before he cashed out . _eos _nofact _eos\t_go i 'm sure he was , it was great chatting with you ! _eos\t0\n",
    "0\t1\t2\tthanks , grandpa ! i bet grandpa wishes he had cashed them in before he cashed out . _eos _nofact _eos\t_go he was an all star and did the money . i think it 's interesting . i wonder how many albums he had . _eos\t0\n",
    "0\t1\t2\tthanks , grandpa ! i bet grandpa wishes he had cashed them in before he cashed out . _eos _nofact _eos\t_go maybe it's time to collect some baseball cards now , so you can cash out when you 're older ! _eos\t0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/piek/Desktop/t-MA-Combots-2021/code/usr/examples/roberta_ft were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /Users/piek/Desktop/t-MA-Combots-2021/code/usr/examples/roberta_ft and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at /Users/piek/Desktop/t-MA-Combots-2021/code/usr/examples/uk were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /Users/piek/Desktop/t-MA-Combots-2021/code/usr/examples/ctx were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "usr_rft_classifier_qqp = pipeline(\"text-classification\", model='/Users/piek/Desktop/t-MA-Combots-2021/code/usr/examples/roberta_ft')\n",
    "usr_uk_classifier_qqp = pipeline(\"text-classification\", model='/Users/piek/Desktop/t-MA-Combots-2021/code/usr/examples/uk')\n",
    "usr_ctx_classifier_qqp = pipeline(\"text-classification\", model='/Users/piek/Desktop/t-MA-Combots-2021/code/usr/examples/ctx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"hi there how are you doing this evening ?\\nhi , sitting here with my three dogs watching the olympics !\\nnice i do not want to go back to work i am a waitress\\ni love being in a polyamorous open relationship !\\niol well i wish i was brave enough to do that\\nmy father was a salesman , helps my dog walking business now\\nthat is nice i've a motorbike don't know what car to get for winter\\nvery very cool . sounds fun\\nyes i had them put red with blue stripes to be shinny for when racing\\nso is my dog , wow so cool\\nso what do you do in your spare time ?\\nlead singer for a band , music teacher\\nwow nice are you really good ?\\nmillions of plays on soundcloud\\nreally would you share or are you shy\\ni know what you mean spend most nights cuddling my dog and star watching\\n\"\n",
    "#sequence = \"b'A woman looks at a duck as she walks behind it.' b'A woman is going on a walk with her dog.'\"\n",
    "#sequence = '<s>I am a chef in a restaurant</s><s>What dishes do you cook?</s>'\n",
    "#sequence = 'I am a chef in a restaurant. What dishes do you cook?'\n",
    "sequence = \"yeah i would feel bad too . bad sportsmanship . is n't it odd that pro bowlers used to make more money than pro football players in the 1960 's ! _eos people fantasy draft the national spelling bee _eos\t_go yeah , i guess football has changed so much . i wonder if bowling is more popular in the 60 's than football . _eos\"\n",
    "sequence = \"amazon ceo jeff bezos built a clock into a mountain that should run for 10,000 years . _eos\t_go i think he is a great ceo , he is a great ceo and a genius _eos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned [[{'label': 'LABEL_0', 'score': 0.49284106492996216}, {'label': 'LABEL_1', 'score': 0.5071589350700378}]]\n",
      "Use knowledge [[{'label': 'LABEL_0', 'score': 0.0045419964008033276}, {'label': 'LABEL_1', 'score': 0.9954580068588257}]]\n",
      "Coherence [[{'label': 'LABEL_0', 'score': 0.012619041837751865}, {'label': 'LABEL_1', 'score': 0.9873809814453125}]]\n"
     ]
    }
   ],
   "source": [
    "print('Fine-tuned',usr_rft_classifier_qqp(sequence, return_all_scores=True))\n",
    "print('Use knowledge', usr_uk_classifier_qqp(sequence, return_all_scores=True))\n",
    "print('Coherence', usr_ctx_classifier_qqp(sequence, return_all_scores=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned [[{'label': 'LABEL_0', 'score': 0.492765337228775}, {'label': 'LABEL_1', 'score': 0.5072346329689026}]]\n",
      "Use knowledge [[{'label': 'LABEL_0', 'score': 0.06817938387393951}, {'label': 'LABEL_1', 'score': 0.9318206310272217}]]\n",
      "Coherence [[{'label': 'LABEL_0', 'score': 0.010731802321970463}, {'label': 'LABEL_1', 'score': 0.9892682433128357}]]\n"
     ]
    }
   ],
   "source": [
    "sequence = \"amazon ceo jeff bezos built a clock into a mountain that should run for 10,000 years . _eos\t_go I think he is a great CEO , he is a great CEO and a genius _eos\"\n",
    "print('Fine-tuned',usr_rft_classifier_qqp(sequence, return_all_scores=True))\n",
    "print('Use knowledge', usr_uk_classifier_qqp(sequence, return_all_scores=True))\n",
    "print('Coherence', usr_ctx_classifier_qqp(sequence, return_all_scores=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sys\n",
    "import os\n",
    "import emissor as em\n",
    "from emissor.persistence import ScenarioStorage\n",
    "from emissor.representation.annotation import AnnotationType, Token, NER\n",
    "from emissor.representation.container import Index\n",
    "from emissor.representation.scenario import Modality, ImageSignal, TextSignal, Mention, Annotation, Scenario\n",
    "\n",
    "\n",
    "src_path = os.path.abspath(os.path.join('..'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "#### The next utils are needed for the interaction and creating triples and capsules\n",
    "import chatbots.util.driver_util as d_util\n",
    "import chatbots.util.text_util as t_util\n",
    "\n",
    "scenario_path = \"/Users/piek/PycharmProjects/cltl-chatbots/data\"\n",
    "### The name of your scenario\n",
    "scenario_id = \"2021-11-30-09:08:06\"\n",
    "\n",
    "### Create the scenario folder, the json files and a scenarioStorage and scenario in memory\n",
    "scenarioStorage = ScenarioStorage(scenario_path)\n",
    "scenario_ctrl = scenarioStorage.load_scenario(scenario_id)\n",
    "signals = scenario_ctrl.get_signals(Modality.TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "leolani = []\n",
    "dialoggpt = []\n",
    "\n",
    "for index, signal in enumerate(signals):\n",
    "    if index % 2== 0:\n",
    "        utterance = ''.join(signal.seq)\n",
    "        leolani.append(utterance)\n",
    "print(len(leolani))\n",
    "for index, signal in enumerate(signals):\n",
    "    if not (index % 2== 0):\n",
    "        utterance = ''.join(signal.seq)\n",
    "        dialoggpt.append(utterance)\n",
    "print(len(dialoggpt))\n",
    "\n",
    "rft0 = []\n",
    "rft1 = []\n",
    "uk0 = []\n",
    "uk1 = []\n",
    "ctx0 = []\n",
    "ctx1 = []\n",
    "for q,r in (zip(leolani, dialoggpt)):\n",
    "    sequence = q.lower() + ' _eos _go ' + r.lower() + ' _eos'\n",
    "    results = usr_rft_classifier_qqp(sequence, return_all_scores=True)\n",
    "    for result in results[0]:\n",
    "        if result['label']=='LABEL_0':\n",
    "            rft0.append(result['score'])\n",
    "        else:\n",
    "            rft1.append(result['score'])\n",
    "            \n",
    "    results = usr_uk_classifier_qqp(sequence, return_all_scores=True)\n",
    "    for result in results[0]:\n",
    "        if result['label']=='LABEL_0':\n",
    "            uk0.append(result['score'])\n",
    "        else:\n",
    "            uk1.append(result['score'])\n",
    "\n",
    "    results = usr_ctx_classifier_qqp(sequence, return_all_scores=True)\n",
    "    for result in results[0]:\n",
    "        if result['label']=='LABEL_0':\n",
    "            ctx0.append(result['score'])\n",
    "        else:\n",
    "            ctx1.append(result['score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roberta_fine_tuned: LABEL_0 0.4673770070075989 LABEL_1 0.5326229814560183\n",
      "Use Knowledge: LABEL_0 0.21310751541187206 LABEL_1 0.7868924833113148\n",
      "Context coherence: LABEL_0 0.37423608579763 LABEL_1 0.6257639068268961\n"
     ]
    }
   ],
   "source": [
    "rft0_score = sum(rft0)/len(rft0)\n",
    "rft1_score = sum(rft1)/len(rft1)\n",
    "print('Roberta_fine_tuned:', 'LABEL_0', rft0_score, 'LABEL_1', rft1_score)\n",
    "\n",
    "uk0_score = sum(uk0)/len(uk0)\n",
    "uk1_score = sum(uk1)/len(uk1)\n",
    "print('Use Knowledge:', 'LABEL_0', uk0_score, 'LABEL_1', uk1_score)\n",
    "\n",
    "ctx0_score = sum(ctx0)/len(ctx0)\n",
    "ctx1_score = sum(ctx1)/len(ctx1)\n",
    "print('Maintains context:', 'LABEL_0', ctx0_score, 'LABEL_1', ctx1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
