{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zqKXlENEtN5"
   },
   "source": [
    "## <b>USR</b>: <i>An <b>U</b>n<b>S</b>upervised and <b>R</b>eference Free Evaluation Metric for Dialog Generation</i>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits: the first part of this notebook *1. MaintainsContext (MCtx) Metric* was created by Thomas Bellucci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we show how dialogues can be evaluated with the Unsupervised and Reference Free (USR) Evaluation Metric, as described in: \n",
    "\n",
    "* Mehri, Shikib, and Maxine Eskenazi. \"Usr: An unsupervised and reference free evaluation metric for dialog generation.\" arXiv preprint arXiv:2005.00456 (2020). https://arxiv.org/pdf/2005.00456.pdf\n",
    "\n",
    "USR is also used in Track-3 of the DSTC9 conference: http://dialog.speech.cs.cmu.edu:8003\n",
    "\n",
    "The soure code USR is given in: https://github.com/Shikib/usr. However, the github contains many other tasks and it is not clear how the models should be used. We therefore present here a easy to follow notebook implementation that mimics their apporach as explained in the paper.\n",
    "\n",
    "**WARNING!**\n",
    "Note that this is not the same implementation. There are very likely to be differences between the intended scores of the paper and our implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MaintainsContext (MCtx) Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the *Think Aloud* project primarily concerns the selection of thoughts from the brain that yield coherent follow-ups to the dialogue, and does not concern particularly their exact phrasing (e.g. <i>Naturalness</i>) nor the information communicated (i.e. <i>Uses Knowledge, Interestingness</i>), we strict the evaluation to those metrics deemed most relevant to the tested component; that is, the <i>Maintains Context</i> metric.\n",
    "\n",
    "However, implementing the MLM metric to measure *naturalness* is as simple as loading another model (e.g. `'adamlin/usr-topicalchat-roberta_ft'`) and changing the model type to `RobertaForMaskedLM`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xL-qyDckHqdv"
   },
   "source": [
    "In what follows we will implement this metric with the pretrained `Pytorch` model provided by (Mehri et al., 2020)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HJ3tWqaH-jr"
   },
   "source": [
    "### Dependencies\n",
    "\n",
    "First we'll install all required packages and import all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gy5OKa2LasoW"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8MVwKyZLbEYH"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaForSequenceClassification, RobertaModelForMaskedLM, RobertaTokenizer, RobertaConfig, AdamW\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQK7YpJrIy_S"
   },
   "source": [
    "### MaintainsContext (MCtx) Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "eZjguEHEb6ni"
   },
   "outputs": [],
   "source": [
    "class USR_CTX:\n",
    "    def __init__(self, path=None):\n",
    "        \"\"\" Load pretrained and finetuned RoBERTa model for ctx. \n",
    "        \n",
    "            params\n",
    "            str path: path to stored model or None\n",
    "\n",
    "            returns: None\n",
    "        \"\"\"\n",
    "        self.__config = RobertaConfig.from_pretrained('adamlin/usr-topicalchat-ctx')\n",
    "        self.__tokenizer = RobertaTokenizer.from_pretrained('adamlin/usr-topicalchat-ctx')\n",
    "\n",
    "        if path is not None:\n",
    "            self.__model = RobertaForSequenceClassification.from_pretrained(path, config=self.__config)\n",
    "        else:\n",
    "            self.__model = RobertaForSequenceClassification.from_pretrained('adamlin/usr-topicalchat-ctx', config=self.__config)\n",
    "\n",
    "        self.__device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.__model.to(self.__device)\n",
    "\n",
    "    def MCtx(self, context, response):\n",
    "        \"\"\" Scores an input consisting of a (context, response) pair using RoBERTa.\n",
    "\n",
    "            params\n",
    "            str context:  the context strings\n",
    "            sre response: response to the context\n",
    "\n",
    "            returns: score\n",
    "        \"\"\"\n",
    "        # Concatenates and encodes context-response pair\n",
    "        inputs = self.__tokenizer(context + \" [SEP] \" + response, return_tensors='pt') # TODO verify separator token used in paper (standard </s> gives bad results)\n",
    "\n",
    "        inputs['input_ids'] = inputs['input_ids'].to(self.__device)\n",
    "        inputs['attention_mask'] = inputs['attention_mask'].to(self.__device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = self.__model(**inputs)\n",
    "        logits = outputs.logits.detach().cpu().numpy()\n",
    "        \n",
    "        # Returns the softmax score of the positive class, i.e. P(y=1|context, response)\n",
    "        outputs = np.exp(logits) / np.sum(np.exp(logits))\n",
    "        return outputs[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmsb3pOVIQD2"
   },
   "source": [
    "Having defined a general class for the MCtx USR metric, we can test it on a number of context-response pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default context model: usr-topicalchat-ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zqpdjUepJyDB",
    "outputId": "80a39d39-61c1-4106-e3a9-ab02d97141d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at adamlin/usr-topicalchat-ctx were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_xtc = USR_CTX() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Puah4sGckmWB",
    "outputId": "5b50d2dd-15b8-49c9-c20c-a78565c42ede"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.9965068 \t Do you have a cat? I do not have a cat\n",
      "score: 0.652586 \t Do you have a cat? I like cats\n",
      "score: 0.20761395 \t Do you have a cat? I like kittens\n",
      "score: 0.0024435509 \t Do you have a cat? I want a turtle\n"
     ]
    }
   ],
   "source": [
    "pairs = [('Do you have a cat?', 'I do not have a cat'), # good\n",
    "         ('Do you have a cat?', 'I like cats'),         # not as good\n",
    "         ('Do you have a cat?', 'I like kittens'),      # worse\n",
    "         ('Do you have a cat?', 'I want a turtle')]     # what are we even saying\n",
    "\n",
    "for context, response in pairs:\n",
    "    score = model_xtc.MCtx(context, response)\n",
    "    print('score:', score, '\\t', context, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fact context model: adamlin/usr-topicalchat-uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at adamlin/usr-topicalchat-uk were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_uk = USR_CTX(path='adamlin/usr-topicalchat-uk') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.63138205 \t Do you have a cat? I do not have a cat\n",
      "score: 0.6389494 \t Do you have a cat? I like cats\n",
      "score: 0.6534759 \t Do you have a cat? I like kittens\n",
      "score: 0.62958485 \t Do you have a cat? I want a turtle\n"
     ]
    }
   ],
   "source": [
    "for context, response in pairs:\n",
    "    score = model_uk.MCtx(context, response)\n",
    "    print('score:', score, '\\t', context, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performs less well on our text contexts. If the conversations is more factual scores may be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Likelihood metric of a target sentence by transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLM model of the USR paper is not fine-tuned but pretrained with topical-chat. We cannot use the same task as for the fine-tuned models. Instead, we define a likelihood function for a target sentence by creating a masked-task for all the tokens in a target sentence. By taking the score for the target token from the predicted results, we can obtain an averaged score for the whole target, given a context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a utility function that creates from a context and a target sentence a list of all masked sentences and the tokens that have been masked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def mask_target_sentence(pair:[], mask_token:str):\n",
    "    context = pair[0]\n",
    "    target = pair[1]\n",
    "    masked_targets = []\n",
    "    target_tokens = re.split(' ', target)\n",
    "    for index, token in enumerate(target_tokens):\n",
    "        sequence = context+\" \"\n",
    "        for token in target_tokens[:index]:\n",
    "            sequence+= token+\" \"\n",
    "        sequence += mask_token\n",
    "        for token in target_tokens[index+1:]:\n",
    "            sequence+= \" \"+token\n",
    "        masked_targets.append(sequence)\n",
    "    return masked_targets, target_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need a function that, given a model and a context, target pair applies the masked task to each token and gets the score from the results. If the token is not in the result, we add a zero score. At the end, we take the same and divide by the number of tokens in the target sentence. \n",
    "\n",
    "For comparison, we also get the best score and create the target sentence according the best prediction. We can compare the actual score with the best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_likelihood(pipeline, pair:[], masked_token:str):\n",
    "    masked_targets, target_tokens = mask_target_sentence(pair, masked_token)\n",
    "    expected_target = \"\"\n",
    "    max_scores = []\n",
    "    scores = []\n",
    "    for masked_target, token in zip(masked_targets, target_tokens):\n",
    "        results = pipeline(masked_target)\n",
    "        expected_target+= results[0]['token_str']+\" \"\n",
    "        max_scores.append(results[0]['score'])\n",
    "        match=False\n",
    "        for result in results:\n",
    "            if result['token_str'].lower().strip()==token.lower():\n",
    "                scores.append(result['score'])\n",
    "                match=True\n",
    "                break\n",
    "                \n",
    "        if not match:\n",
    "            scores.append(0)\n",
    "    likelihood = sum(scores)/len(scores)\n",
    "    max_likelihood =  sum(max_scores)/len(max_scores)\n",
    "\n",
    "    return likelihood, expected_target, max_likelihood          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the transformers pipeline to do the task. We first load the USR topicalchat-roberta_ft model and apply it to the above pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "model_name = 'adamlin/usr-topicalchat-roberta_ft'\n",
    "usr_rft_fillmask= pipeline(\"fill-mask\", model=model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set the number of results to make it more finegrained or not. More results will result in less zero scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mask>\n",
      "('Do you have a cat?', 'I do not have a cat')\n",
      "Likelihood: 0.9511937399705251 Max score: 0.9511937399705251 Best sentence:  i  do  not  have  a  cat \n",
      "('Do you have a cat?', 'I like cats')\n",
      "Likelihood: 0.6525216996669769 Max score: 0.750761349995931 Best sentence: i  love  cats \n",
      "('Do you have a cat?', 'I like kittens')\n",
      "Likelihood: 0.2641774927227137 Max score: 0.7216621239980062 Best sentence: i  have  cats \n",
      "('Do you have a cat?', 'I want a turtle')\n",
      "Likelihood: 0.4552861073752865 Max score: 0.8906680643558502 Best sentence:  i  have  a  cat \n"
     ]
    }
   ],
   "source": [
    "usr_rft_fillmask.top_k=20 ### we get the top 20 results\n",
    "for pair in pairs:\n",
    "    llh, best_sentence, max_score = sentence_likelihood(usr_rft_fillmask, pair, tokenizer.mask_token)\n",
    "    print(pair)\n",
    "    print('Likelihood:',llh, 'Max score:', max_score, 'Best sentence:', best_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the roberta_ft that was used, we can also use any other transformer for the *fill-mask* task. Below, we use the cross-lingual roberta model, which can be applied to dialogues in 150 languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'xlm-roberta-base'\n",
    "xlm_rob_base_fillmask= pipeline(\"fill-mask\", model=model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Do you have a cat?', 'I do not have a cat')\n",
      "Likelihood: 0.8351189295450846 Max score: 0.8351189295450846 Best sentence: I do not have a cat \n",
      "('Do you have a cat?', 'I like cats')\n",
      "Likelihood: 0.3044796958565712 Max score: 0.7233533263206482 Best sentence: I love it \n",
      "('Do you have a cat?', 'I like kittens')\n",
      "Likelihood: 0.32319798072179157 Max score: 0.6229558785756429 Best sentence: I love it \n",
      "('Do you have a cat?', 'I want a turtle')\n",
      "Likelihood: 0.43128971802070737 Max score: 0.797596886754036 Best sentence: I have a cat \n"
     ]
    }
   ],
   "source": [
    "xlm_rob_base_fillmask.top_k=20 ### we get the top 20 results\n",
    "\n",
    "for pair in pairs:\n",
    "    score = llh, best_sentence, max_score = sentence_likelihood(xlm_rob_base_fillmask, pair, tokenizer.mask_token)\n",
    "    print(pair)\n",
    "    print('Likelihood:',llh, 'Max score:', max_score, 'Best sentence:', best_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that roberta pretrained with topical chat performs a bit better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b1424bae1fa44b2b8592c4c5fe913ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'roberta-base'\n",
    "roberta_fillmask= pipeline(\"fill-mask\", model=model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Do you have a cat?', 'I do not have a cat')\n",
      "Likelihood: 0.9438453912734985 Max score: 0.9438453912734985 Best sentence:  I  do  not  have  a  cat \n",
      "('Do you have a cat?', 'I like cats')\n",
      "Likelihood: 0.567038098971049 Max score: 0.6775117516517639 Best sentence:  I  love  cats \n",
      "('Do you have a cat?', 'I like kittens')\n",
      "Likelihood: 0.2505231909453869 Max score: 0.61153111855189 Best sentence:  I  have  cats \n",
      "('Do you have a cat?', 'I want a turtle')\n",
      "Likelihood: 0.3528149656485766 Max score: 0.8265911191701889 Best sentence:  You  have  a  cat \n"
     ]
    }
   ],
   "source": [
    "for pair in pairs:\n",
    "    score = llh, best_sentence, max_score = sentence_likelihood(roberta_fillmask, pair, tokenizer.mask_token)\n",
    "    print(pair)\n",
    "    print('Likelihood:',llh, 'Max score:', max_score, 'Best sentence:', best_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Perplexity\n",
    "\n",
    "From: https://huggingface.co/docs/transformers/perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Dialogue Evaluation with USR (v2).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
