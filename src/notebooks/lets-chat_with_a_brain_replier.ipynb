{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uses interaction to push triples to the brain and query it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook assumes you already understand the following notebooks:\n",
    "\n",
    "* lets-chat.ipynb\n",
    "* roboGrasp-api.ipynb\n",
    "\n",
    "In this notebook, we will combine the interaction modeled through EMISSOR with the interaction throught *capsules* with the BRAIN. As auxiliary modules, we will use the *cltl-knowledgeextraction* and *cltl-language-generation*. The *cltl-knowledgeextraction* will extract triples from the utterances either for posting or for querying. In the former case, it also extracts the source perspective from the text. The triples and source perspectives are represented in enriched capsules. We will also use a replier that is include in the *cltl-languagegeneration* package. This replier transfers the response from the BRAIN into natural language expressions and possibly gestures. \n",
    "\n",
    "In order to connect to the EMISSOR scenario, we need to align the meta properties of the scenario with the meta data in the capsules. However,  the *cltl-knowledgeextraction* uses a similar data object to keep track of the conversation history. This is a Chat object that needs to be created. Through this Chat object, we keep track of what was said before and deal with coreference to earlier utterances.\n",
    "\n",
    "Combining EMISSOR and the BRAIN, we can model the interaction in a infite while loop where we go through the following steps:\n",
    "\n",
    "* We create an EMISSOR scenario at the start of the interaction and a corresponding Chat object to keep track of the dialogue history\n",
    "* while the user does not stop:\n",
    "    * create TextSignals for each utterance from the user and store these in an EMISSOR scenario\n",
    "    * add the utterance also to the Chat instance\n",
    "    * process the latest utterance in the Chat object using the *cltl-knowledgeextraction* to get triples and perspectives\n",
    "    * create a capsule from the triples, perspectives and the text signal meta data\n",
    "    * post the triple to the BRAIN\n",
    "    * get the answer or throughts as a response from the BRAIN\n",
    "    * verbalise the answer or thoughts\n",
    "    * create a new TextSignal for the system response and add it to the EMISSOR scenario\n",
    "* When the user stops, we save the scenario in the EMISSOR format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running, start GraphDB and make sure that there is a sandbox repository.\n",
    "GraphDB can be downloaded from:\n",
    "\n",
    "https://graphdb.ontotext.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import uuid\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from random import getrandbits, choice\n",
    "import pathlib\n",
    "import pprint\n",
    "\n",
    "# general imports for EMISSOR and the BRAIN\n",
    "import emissor as em\n",
    "import requests\n",
    "from cltl import brain\n",
    "from cltl.brain.long_term_memory import LongTermMemory\n",
    "from cltl.brain.utils.helper_functions import brain_response_to_json\n",
    "from cltl.combot.backend.api.discrete import UtteranceType\n",
    "from cltl.reply_generation.data.sentences import GREETING, ASK_NAME, ELOQUENCE, TALK_TO_ME\n",
    "from cltl.reply_generation.lenka_replier import LenkaReplier\n",
    "from cltl.triple_extraction.api import Chat, UtteranceHypothesis\n",
    "from emissor.persistence import ScenarioStorage\n",
    "from emissor.representation.annotation import AnnotationType, Token, NER\n",
    "from emissor.representation.container import Index\n",
    "from emissor.representation.scenario import Modality, ImageSignal, TextSignal, Mention, Annotation, Scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the chatbot utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "src_path = os.path.abspath(os.path.join('..'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "#### The next utils are needed for the interaction and creating triples and capsules\n",
    "import chatbots.util.driver_util as d_util\n",
    "import chatbots.util.capsule_util as c_util\n",
    "import chatbots.intentions.talk as talk\n",
    "import chatbots.intentions.get_to_know_you as friend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard initialisation of a scenario\n",
    "\n",
    "We initialise a scenario in the standard way by creating a unique folder and setting the AGENT and HUMAN_NAME and HUMAN_ID variables. Throughout this scenario, the HUMAN_NAME and HUMAN_ID will be used as the source for the utterances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories for 2021-11-15-14:49:08 created in /Users/piek/PycharmProjects/cltl-chatbots/data\n"
     ]
    }
   ],
   "source": [
    "from random import getrandbits\n",
    "import requests\n",
    "##### Setting the location\n",
    "place_id = getrandbits(8)\n",
    "location = None\n",
    "try:\n",
    "    location = requests.get(\"https://ipinfo.io\").json()\n",
    "except:\n",
    "    print(\"failed to get the IP location\")\n",
    "    \n",
    "##### Setting the agents\n",
    "AGENT = \"Leolani2\"\n",
    "HUMAN_NAME = \"Stranger\"\n",
    "HUMAN_ID = \"stranger\"\n",
    "\n",
    "### The name of your scenario\n",
    "scenario_id = datetime.today().strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "\n",
    "### Specify the path to an existing data folder where your scenario is created and saved as a subfolder\n",
    "# Find the repository root dir\n",
    "parent, dir_name = (d_util.__file__, \"_\")\n",
    "while dir_name and dir_name != \"src\":\n",
    "    parent, dir_name = os.path.split(parent)\n",
    "root_dir = parent\n",
    "scenario_path = os.path.abspath(os.path.join(root_dir, 'data'))\n",
    "\n",
    "if not os.path.exists(scenario_path) :\n",
    "    os.mkdir(scenario_path)\n",
    "    print(\"Created a data folder for storing the scenarios\", scenario_path)\n",
    "\n",
    "### Define the folders where the images and rdf triples are saved\n",
    "imagefolder = scenario_path + \"/\" + scenario_id + \"/\" + \"image\"\n",
    "rdffolder = scenario_path + \"/\" + scenario_id + \"/\" + \"rdf\"\n",
    "\n",
    "### Create the scenario folder, the json files and a scenarioStorage and scenario in memory\n",
    "scenarioStorage = d_util.create_scenario(scenario_path, scenario_id)\n",
    "scenario_ctrl = scenarioStorage.create_scenario(scenario_id, int(time.time() * 1e3), None, AGENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying the BRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify the BRAIN in GraphDB and use the scenario path just defined for storing the RDF triple produced in EMISSOR.\n",
    "\n",
    "If you set *clear_all* to *True*, the sandbox triple store is emptied (memory erased) and the basic ontological models are reloaded. Setting it to *False* means you add things to the current memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-15 14:49:10,810 -     INFO -    cltl.brain.basic_brain.LongTermMemory - Booted\n",
      "2021-11-15 14:49:17,088 -     INFO -    cltl.brain.basic_brain.LongTermMemory - Uploading ontology to brain\n",
      "2021-11-15 14:49:20,677 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Booted\n",
      "2021-11-15 14:49:20,679 -     INFO -  cltl.brain.basic_brain.LocationReasoner - Booted\n",
      "2021-11-15 14:49:20,681 -     INFO -      cltl.brain.basic_brain.TypeReasoner - Booted\n",
      "2021-11-15 14:49:20,683 -     INFO -   cltl.brain.basic_brain.TrustCalculator - Booted\n",
      "2021-11-15 14:49:20,930 -     INFO -   cltl.brain.basic_brain.TrustCalculator - Computed trust for all known agents\n"
     ]
    }
   ],
   "source": [
    "log_path = pathlib.Path(rdffolder)\n",
    "my_brain = brain.LongTermMemory(address=\"http://localhost:7200/repositories/sandbox\",\n",
    "                                log_dir=log_path,\n",
    "                                clear_all=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an instance of a replier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-15 14:49:23,898 -     INFO -   cltl.reply_generation.api.LenkaReplier - Booted\n"
     ]
    }
   ],
   "source": [
    "replier = LenkaReplier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish the speaker identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leolani2: Hi! What is your name? Stranger?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " Piek\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Piek\n",
      "Leolani2: So your name is Piek?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Piek\n",
      "Id: Piek\n",
      "2021-11-15 14:49:34,964 -  WARNING -      cltl.brain.basic_brain.TypeReasoner - Failed to query Wikidata: HTTPSConnectionPool(host='query.wikidata.org', port=443): Read timed out. (read timeout=3)\n",
      "2021-11-15 14:49:36,255 -     INFO -      cltl.brain.basic_brain.TypeReasoner - Reasoned type of Piek to: None\n",
      "2021-11-15 14:49:36,377 -     INFO -    cltl.brain.basic_brain.LongTermMemory - Triple in statement: leolani2_know_piek [person_->_])\n",
      "2021-11-15 14:49:36,477 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Entity Novelty: new subject - new object \n",
      "2021-11-15 14:49:39,226 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Negation Conflicts: piek on November,2021 about UNDERSPECIFIED\n",
      "2021-11-15 14:49:39,333 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Gaps: 26 gaps as subject: e.g. dislike interest - 15 gaps as object: e.g. be-child-of agent\n",
      "2021-11-15 14:49:39,427 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Gaps: 25 gaps as subject: e.g. travel-to location - 14 gaps as object: e.g. read-by book\n"
     ]
    }
   ],
   "source": [
    "#### Small sequence to learn name of speaker\n",
    "initial_prompt = f\"{choice(GREETING)} {choice(ASK_NAME)} {HUMAN_NAME}?\"\n",
    "print(AGENT + \": \" + initial_prompt)\n",
    "textSignal = d_util.create_text_signal(scenario_ctrl, initial_prompt)\n",
    "scenario_ctrl.append_signal(textSignal)\n",
    "\n",
    "#### Get name from person \n",
    "HUMAN_NAME, HUMAN_ID = friend.get_a_name_and_id(scenario_ctrl, AGENT)\n",
    "HUMAN_ID = HUMAN_NAME  ### Hack because we cannot force the namespace through capsules, name and identity are the same till this is fixed\n",
    "\n",
    "print(\"Name:\", HUMAN_NAME)\n",
    "print(\"Id:\", HUMAN_ID)\n",
    "                \n",
    "capsule = c_util.scenario_utterance_to_capsule(scenario_ctrl,place_id,location, textSignal,HUMAN_ID,AGENT,\"know\", HUMAN_ID)\n",
    "\n",
    "name_thoughts = my_brain.update(capsule, reason_types=True, create_label=True)\n",
    "\n",
    "#pprint.pprint(capsule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise a chat with the HUMAN_ID to keep track of the dialogue history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-15 14:49:45,770 -     INFO - cltl.triple_extraction.api.Chat (Piek)              000 - << Start of Chat with Piek >>\n"
     ]
    }
   ],
   "source": [
    "chat = Chat(HUMAN_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leolani2: Do you have any gossip?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " I like pizza\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Piek: I like pizza\n",
      "2021-11-15 14:50:04,715 -     INFO -               cltl.triple_extraction.api - Started POS tagger\n",
      "2021-11-15 14:50:04,716 -     INFO -               cltl.triple_extraction.api - Started NER tagger\n",
      "2021-11-15 14:50:04,721 -     INFO -               cltl.triple_extraction.api - Loaded grammar\n",
      "2021-11-15 14:50:06,148 -     INFO - cltl.triple_extraction.api.Chat (Piek)              001 -       Piek: \"I like pizza\"\n",
      "2021-11-15 14:50:07,677 -     INFO -    cltl.brain.basic_brain.LongTermMemory - Triple in statement: piek_like_pizza [agent_->_food])\n",
      "2021-11-15 14:50:07,774 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Entity Novelty: existing subject - new object \n",
      "2021-11-15 14:50:10,274 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Negation Conflicts: piek on November,2021 about POSITIVE\n",
      "2021-11-15 14:50:10,333 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Gaps: 26 gaps as subject: e.g. be-member-of institution - 15 gaps as object: e.g. cook-by food\n",
      "2021-11-15 14:50:10,421 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Gaps: 3 gaps as subject: e.g. cook-by agent - 2 gaps as object: e.g. favorite agent\n",
      "Leolani2: Exciting news! I am excited to get to know about you!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " Fred likes pizza\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Piek: Fred likes pizza\n",
      "2021-11-15 14:50:21,851 -     INFO - cltl.triple_extraction.api.Chat (Piek)              002 -       Piek: \"Fred likes pizza\"\n",
      "2021-11-15 14:50:25,021 -  WARNING -      cltl.brain.basic_brain.TypeReasoner - Failed to query Wikidata: HTTPSConnectionPool(host='query.wikidata.org', port=443): Read timed out. (read timeout=3)\n",
      "2021-11-15 14:50:25,939 -     INFO -      cltl.brain.basic_brain.TypeReasoner - Reasoned type of fred to: None\n",
      "2021-11-15 14:50:26,027 -     INFO -    cltl.brain.basic_brain.LongTermMemory - Triple in statement: fred_like_pizza [_->_food])\n",
      "2021-11-15 14:50:26,125 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Entity Novelty: new subject - existing object \n",
      "2021-11-15 14:50:26,181 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Overlaps: 0 subject overlaps: e.g. '' - 1 object overlaps: e.g. piek on November,2021 about piek\n",
      "2021-11-15 14:50:28,673 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Negation Conflicts: piek on November,2021 about POSITIVE\n",
      "2021-11-15 14:50:28,775 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Gaps: 3 gaps as subject: e.g. favorite-of agent - 2 gaps as object: e.g. dislike agent\n",
      "Leolani2: Cool! Did you know that piek also like pizza\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " Who pizza likes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Piek: Who pizza likes\n",
      "2021-11-15 14:50:36,477 -     INFO - cltl.triple_extraction.api.Chat (Piek)              003 -       Piek: \"Who pizza likes\"\n",
      "2021-11-15 14:50:36,479 -     INFO -    cltl.brain.basic_brain.LongTermMemory - Triple in question: likes_pizza_? [cognition_->_])\n",
      "Leolani2: I have no idea.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " Who does like pizza\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Piek: Who does like pizza\n",
      "2021-11-15 14:50:43,578 -     INFO - cltl.triple_extraction.api.Chat (Piek)              004 -       Piek: \"Who does like pizza\"\n",
      "2021-11-15 14:50:43,582 -     INFO -    cltl.brain.basic_brain.LongTermMemory - Triple in question: pizza_like_? [food_->_])\n",
      "Leolani2: I have no idea.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " Who likes pizza\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Piek: Who likes pizza\n",
      "2021-11-15 14:50:51,041 -     INFO - cltl.triple_extraction.api.Chat (Piek)              005 -       Piek: \"Who likes pizza\"\n",
      "2021-11-15 14:50:51,044 -     INFO -    cltl.brain.basic_brain.LongTermMemory - Triple in question: pizza_like_? [food_->_])\n",
      "Leolani2: I wouldn't know!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " What do I like\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Piek: What do I like\n",
      "2021-11-15 14:51:01,729 -     INFO - cltl.triple_extraction.api.Chat (Piek)              006 -       Piek: \"What do I like\"\n",
      "2021-11-15 14:51:01,732 -     INFO -    cltl.brain.basic_brain.LongTermMemory - Triple in question: piek_like_? [agent_->_])\n",
      "Leolani2: you told me you verb.emotion pizza\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " What does Fred like\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Piek: What does Fred like\n",
      "2021-11-15 14:51:08,841 -     INFO - cltl.triple_extraction.api.Chat (Piek)              007 -       Piek: \"What does Fred like\"\n",
      "2021-11-15 14:51:08,845 -     INFO -    cltl.brain.basic_brain.LongTermMemory - Triple in question: fred_like_? [_->_])\n",
      "Leolani2: you told me fred verb.emotion pizza\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Piek: bye\n",
      "2021-11-15 14:51:18,556 -     INFO - cltl.triple_extraction.api.Chat (Piek)              008 -       Piek: \"bye\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't parse input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leolani2: So let it be written, ... so let it be done\n"
     ]
    }
   ],
   "source": [
    "print_details=False\n",
    "\n",
    "\n",
    "#### Initial prompt by the system from which we create a TextSignal and store it\n",
    "initial_prompt = f\"{choice(TALK_TO_ME)}\"\n",
    "print(AGENT + \": \" + initial_prompt)\n",
    "textSignal = d_util.create_text_signal(scenario_ctrl, initial_prompt)\n",
    "scenario_ctrl.append_signal(textSignal)\n",
    "\n",
    "utterance = \"\"\n",
    "#### Get input and loop\n",
    "while not (utterance.lower() == 'stop' or utterance.lower() == 'bye'):\n",
    "    ###### Getting the next input signals\n",
    "    utterance = input('\\n')\n",
    "    print(HUMAN_NAME + \": \" + utterance)\n",
    "    textSignal = d_util.create_text_signal(scenario_ctrl, utterance)\n",
    "    scenario_ctrl.append_signal(textSignal)\n",
    "\n",
    "    #### Process input and generate reply\n",
    "    \n",
    "    capsule, reply = talk.process_text_and_reply(scenario_ctrl,\n",
    "                           place_id,\n",
    "                           location,\n",
    "                           HUMAN_ID,\n",
    "                           textSignal,\n",
    "                           chat,\n",
    "                           replier,\n",
    "                           my_brain,\n",
    "                           print_details)\n",
    "\n",
    "    print(AGENT + \": \" + reply)\n",
    "    textSignal = d_util.create_text_signal(scenario_ctrl, reply)\n",
    "    scenario_ctrl.append_signal(textSignal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the scenario data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_ctrl.scenario.ruler.end = int(time.time() * 1e3)\n",
    "scenarioStorage.save_scenario(scenario_ctrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
