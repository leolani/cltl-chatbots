{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uses interaction to push triples to the brain and query it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook assumes you already understand the following notebooks:\n",
    "\n",
    "* lets-chat.ipynb\n",
    "* roboGrasp-api.ipynb\n",
    "\n",
    "In this notebook, we will combine the interaction modeled through EMISSOR with the interaction throught *capsules* with the BRAIN. As auxiliary modules, we will use the *cltl-knowledgeextraction* and *cltl-language-generation*. The *cltl-knowledgeextraction* will extract triples from the utterances either for posting or for querying. In the former case, it also extracts the source perspective from the text. The triples and source perspectives are represented in enriched capsules. We will also use a replier that is include in the *cltl-languagegeneration* package. This replier transfers the response from the BRAIN into natural language expressions and possibly gestures. \n",
    "\n",
    "In order to connect to the EMISSOR scenario, we need to align the meta properties of the scenario with the meta data in the capsules. However,  the *cltl-knowledgeextraction* uses a similar data object to keep track of the conversation history. This is a Chat object that needs to be created. Through this Chat object, we keep track of what was said before and deal with coreference to earlier utterances.\n",
    "\n",
    "Combining EMISSOR and the BRAIN, we can model the interaction in a infite while loop where we go through the following steps:\n",
    "\n",
    "* We create an EMISSOR scenario at the start of the interaction and a corresponding Chat object to keep track of the dialogue history\n",
    "* while the user does not stop:\n",
    "    * create TextSignals for each utterance from the user and store these in an EMISSOR scenario\n",
    "    * add the utterance also to the Chat instance\n",
    "    * process the latest utterance in the Chat object using the *cltl-knowledgeextraction* to get triples and perspectives\n",
    "    * create a capsule from the triples, perspectives and the text signal meta data\n",
    "    * post the triple to the BRAIN\n",
    "    * get the answer or throughts as a response from the BRAIN\n",
    "    * verbalise the answer or thoughts\n",
    "    * create a new TextSignal for the system response and add it to the EMISSOR scenario\n",
    "* When the user stops, we save the scenario in the EMISSOR format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running, start GraphDB and make sure that there is a sandbox repository.\n",
    "GraphDB can be downloaded from:\n",
    "\n",
    "https://graphdb.ontotext.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import uuid\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from random import getrandbits, choice\n",
    "import pathlib\n",
    "import pprint\n",
    "\n",
    "# general imports for EMISSOR and the BRAIN\n",
    "import emissor as em\n",
    "import requests\n",
    "from cltl import brain\n",
    "from cltl.brain.long_term_memory import LongTermMemory\n",
    "from cltl.brain.utils.helper_functions import brain_response_to_json\n",
    "from cltl.combot.backend.api.discrete import UtteranceType\n",
    "from cltl.reply_generation.data.sentences import GREETING, ASK_NAME, ELOQUENCE, TALK_TO_ME\n",
    "from cltl.reply_generation.lenka_replier import LenkaReplier\n",
    "from cltl.triple_extraction.api import Chat, UtteranceHypothesis\n",
    "from emissor.persistence import ScenarioStorage\n",
    "from emissor.representation.annotation import AnnotationType, Token, NER\n",
    "from emissor.representation.container import Index\n",
    "from emissor.representation.scenario import Modality, ImageSignal, TextSignal, Mention, Annotation, Scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the chatbot utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "src_path = os.path.abspath(os.path.join('..'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "#### The next utils are needed for the interaction and creating triples and capsules\n",
    "import chatbots.util.driver_util as d_util\n",
    "import chatbots.util.capsule_util as c_util\n",
    "import chatbots.intentions.talk as talk\n",
    "import chatbots.intentions.get_to_know_you as friend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard initialisation of a scenario\n",
    "\n",
    "We initialise a scenario in the standard way by creating a unique folder and setting the AGENT and HUMAN_NAME and HUMAN_ID variables. Throughout this scenario, the HUMAN_NAME and HUMAN_ID will be used as the source for the utterances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories for 2021-12-04-07:44:04 created in /Users/piek/PycharmProjects/cltl-chatbots/data\n"
     ]
    }
   ],
   "source": [
    "from random import getrandbits\n",
    "import requests\n",
    "##### Setting the location\n",
    "place_id = getrandbits(8)\n",
    "location = None\n",
    "try:\n",
    "    location = requests.get(\"https://ipinfo.io\").json()\n",
    "except:\n",
    "    print(\"failed to get the IP location\")\n",
    "    \n",
    "##### Setting the agents\n",
    "AGENT = \"Leolani2\"\n",
    "HUMAN_NAME = \"Stranger\"\n",
    "HUMAN_ID = \"stranger\"\n",
    "\n",
    "### The name of your scenario\n",
    "scenario_id = datetime.today().strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "\n",
    "### Specify the path to an existing data folder where your scenario is created and saved as a subfolder\n",
    "# Find the repository root dir\n",
    "parent, dir_name = (d_util.__file__, \"_\")\n",
    "while dir_name and dir_name != \"src\":\n",
    "    parent, dir_name = os.path.split(parent)\n",
    "root_dir = parent\n",
    "scenario_path = os.path.abspath(os.path.join(root_dir, 'data'))\n",
    "\n",
    "if not os.path.exists(scenario_path) :\n",
    "    os.mkdir(scenario_path)\n",
    "    print(\"Created a data folder for storing the scenarios\", scenario_path)\n",
    "\n",
    "### Define the folders where the images and rdf triples are saved\n",
    "imagefolder = scenario_path + \"/\" + scenario_id + \"/\" + \"image\"\n",
    "rdffolder = scenario_path + \"/\" + scenario_id + \"/\" + \"rdf\"\n",
    "\n",
    "### Create the scenario folder, the json files and a scenarioStorage and scenario in memory\n",
    "scenarioStorage = d_util.create_scenario(scenario_path, scenario_id)\n",
    "scenario_ctrl = scenarioStorage.create_scenario(scenario_id, int(time.time() * 1e3), None, AGENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying the BRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify the BRAIN in GraphDB and use the scenario path just defined for storing the RDF triple produced in EMISSOR.\n",
    "\n",
    "If you set *clear_all* to *True*, the sandbox triple store is emptied (memory erased) and the basic ontological models are reloaded. Setting it to *False* means you add things to the current memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-04 07:44:09,590 -     INFO -    cltl.brain.basic_brain.LongTermMemory - Booted\n",
      "2021-12-04 07:44:12,358 -     INFO -    cltl.brain.basic_brain.LongTermMemory - Uploading ontology to brain\n",
      "2021-12-04 07:44:14,803 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Booted\n",
      "2021-12-04 07:44:14,805 -     INFO -  cltl.brain.basic_brain.LocationReasoner - Booted\n",
      "2021-12-04 07:44:14,808 -     INFO -      cltl.brain.basic_brain.TypeReasoner - Booted\n",
      "2021-12-04 07:44:14,810 -     INFO -   cltl.brain.basic_brain.TrustCalculator - Booted\n",
      "2021-12-04 07:44:15,057 -     INFO -   cltl.brain.basic_brain.TrustCalculator - Computed trust for all known agents\n"
     ]
    }
   ],
   "source": [
    "log_path = pathlib.Path(rdffolder)\n",
    "my_brain = brain.LongTermMemory(address=\"http://localhost:7200/repositories/sandbox\",\n",
    "                                log_dir=log_path,\n",
    "                                clear_all=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an instance of a replier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-04 07:44:18,818 -     INFO -   cltl.reply_generation.api.LenkaReplier - Booted\n"
     ]
    }
   ],
   "source": [
    "replier = LenkaReplier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish the speaker identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leolani2: Hey! I've told you my name, but what about yours? Stranger?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " Piek\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Piek\n",
      "Leolani2: So your name is Piek?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Piek\n",
      "Id: Piek\n",
      "2021-12-04 07:44:29,715 -  WARNING -      cltl.brain.basic_brain.TypeReasoner - Failed to query Wikidata: HTTPSConnectionPool(host='query.wikidata.org', port=443): Read timed out. (read timeout=3)\n",
      "2021-12-04 07:44:30,518 -     INFO -      cltl.brain.basic_brain.TypeReasoner - Reasoned type of Piek to: None\n",
      "2021-12-04 07:44:30,604 -     INFO -    cltl.brain.basic_brain.LongTermMemory - Triple in statement: leolani2_know_piek [person_->_])\n",
      "2021-12-04 07:44:30,656 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Entity Novelty: new subject - new object \n",
      "2021-12-04 07:44:32,760 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Negation Conflicts: piek on December,2021 about UNDERSPECIFIED\n",
      "2021-12-04 07:44:32,811 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Gaps: 26 gaps as subject: e.g. be-friends-with person - 15 gaps as object: e.g. be-ancestor-of person\n",
      "2021-12-04 07:44:32,860 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Gaps: 25 gaps as subject: e.g. own object - 14 gaps as object: e.g. like-by interest\n"
     ]
    }
   ],
   "source": [
    "#### Small sequence to learn name of speaker\n",
    "initial_prompt = f\"{choice(GREETING)} {choice(ASK_NAME)} {HUMAN_NAME}?\"\n",
    "print(AGENT + \": \" + initial_prompt)\n",
    "textSignal = d_util.create_text_signal(scenario_ctrl, initial_prompt)\n",
    "scenario_ctrl.append_signal(textSignal)\n",
    "\n",
    "#### Get name from person \n",
    "HUMAN_NAME, HUMAN_ID = friend.get_a_name_and_id(scenario_ctrl, AGENT)\n",
    "HUMAN_ID = HUMAN_NAME  ### Hack because we cannot force the namespace through capsules, name and identity are the same till this is fixed\n",
    "\n",
    "print(\"Name:\", HUMAN_NAME)\n",
    "print(\"Id:\", HUMAN_ID)\n",
    "                \n",
    "capsule = c_util.scenario_utterance_to_capsule(scenario_ctrl,place_id,location, textSignal,HUMAN_ID,AGENT,\"know\", HUMAN_ID)\n",
    "\n",
    "name_thoughts = my_brain.update(capsule, reason_types=True, create_label=True)\n",
    "\n",
    "#pprint.pprint(capsule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise a chat with the HUMAN_ID to keep track of the dialogue history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-04 07:44:32,905 -     INFO - cltl.triple_extraction.api.Chat (Piek)              000 - << Start of Chat with Piek >>\n"
     ]
    }
   ],
   "source": [
    "chat = Chat(HUMAN_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leolani2: Would you like to chat? I'll do my best to keep up\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " keys are under table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Piek: keys are under table\n",
      "2021-12-04 07:44:42,359 -     INFO -               cltl.triple_extraction.api - Started POS tagger\n",
      "2021-12-04 07:44:42,361 -     INFO -               cltl.triple_extraction.api - Started NER tagger\n",
      "2021-12-04 07:44:42,366 -     INFO -               cltl.triple_extraction.api - Loaded grammar\n",
      "2021-12-04 07:44:43,760 -     INFO - cltl.triple_extraction.api.Chat (Piek)              001 -       Piek: \"keys are under table\"\n",
      "initial triple: {'subject': 'keys', 'predicate': 'are', 'object': 'under-table'}\n",
      "Triple:\n",
      "{'object': {'label': 'table', 'type': ['noun.group']},\n",
      " 'predicate': {'label': 'be-under', 'type': ['verb.stative', 'prep']},\n",
      " 'subject': {'label': 'keys', 'type': ['noun.artifact']}}\n",
      "Perspective:\n",
      "{'certainty': 1, 'emotion': <Emotion.NEUTRAL: 7>, 'polarity': 1, 'sentiment': 0}\n",
      "Capsule:\n",
      "{'author': 'Piek',\n",
      " 'chat': '2021-12-04-07:44:04',\n",
      " 'city': 'Weesp',\n",
      " 'context_id': 'Leolani2',\n",
      " 'country': 'NL',\n",
      " 'date': datetime.date(2021, 12, 4),\n",
      " 'object': {'label': 'table', 'type': 'noun.group'},\n",
      " 'objects': [],\n",
      " 'people': [],\n",
      " 'perspective': {'certainty': 1,\n",
      "                 'emotion': <Emotion.NEUTRAL: 7>,\n",
      "                 'polarity': 1,\n",
      "                 'sentiment': 0},\n",
      " 'place': 'Weesp',\n",
      " 'place_id': 219,\n",
      " 'position': '0-20',\n",
      " 'predicate': {'label': 'be-under', 'type': 'verb.stative'},\n",
      " 'region': 'North Holland',\n",
      " 'subject': {'label': 'keys', 'type': 'noun.artifact'},\n",
      " 'turn': '19b5d818-7aae-42c0-b848-c2d09fbf36c4',\n",
      " 'utterance': 'keys are under table',\n",
      " 'utterance_type': <UtteranceType.STATEMENT: 0>}\n",
      "2021-12-04 07:44:45,205 -     INFO -    cltl.brain.basic_brain.LongTermMemory - Triple in statement: keys_be-under_table [artifact or object_->_group])\n",
      "2021-12-04 07:44:45,259 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Entity Novelty: new subject - new object \n",
      "2021-12-04 07:44:47,359 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Negation Conflicts: piek on December,2021 about POSITIVE\n",
      "2021-12-04 07:44:47,406 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Gaps: 0 gaps as subject: e.g. '' - 2 gaps as object: e.g. own person\n",
      "Leolani2: I am curious. What types of group orInstance like table do artifact orobject orInstance usually be under\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " where are under keys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Piek: where are under keys\n",
      "2021-12-04 07:44:53,853 -     INFO - cltl.triple_extraction.api.Chat (Piek)              002 -       Piek: \"where are under keys\"\n",
      "Triple:\n",
      "{'object': {'label': '', 'type': []},\n",
      " 'predicate': {'label': 'be-under', 'type': ['verb.stative', 'prep']},\n",
      " 'subject': {'label': 'keys', 'type': ['noun.artifact']}}\n",
      "Perspective:\n",
      "None\n",
      "Capsule:\n",
      "{'author': 'Piek',\n",
      " 'chat': '2021-12-04-07:44:04',\n",
      " 'city': 'Weesp',\n",
      " 'context_id': 'Leolani2',\n",
      " 'country': 'NL',\n",
      " 'date': datetime.date(2021, 12, 4),\n",
      " 'object': {'label': '', 'type': []},\n",
      " 'objects': [],\n",
      " 'people': [],\n",
      " 'place': 'Weesp',\n",
      " 'place_id': 219,\n",
      " 'position': '0-20',\n",
      " 'predicate': {'label': 'be-under', 'type': 'verb.stative'},\n",
      " 'region': 'North Holland',\n",
      " 'subject': {'label': 'keys', 'type': 'noun.artifact'},\n",
      " 'turn': '071bac82-a37e-4c08-970f-6bf2c1a57258',\n",
      " 'utterance': 'where are under keys',\n",
      " 'utterance_type': <UtteranceType.QUESTION: 1>}\n",
      "2021-12-04 07:44:53,858 -     INFO -    cltl.brain.basic_brain.LongTermMemory - Triple in question: keys_be-under_? [artifact or object_->_])\n",
      "Leolani2: you told me keys be under table\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Piek: stop\n",
      "2021-12-04 07:44:58,341 -     INFO - cltl.triple_extraction.api.Chat (Piek)              003 -       Piek: \"stop\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't parse input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leolani2: Many of us feel that way\n"
     ]
    }
   ],
   "source": [
    "print_details=True\n",
    "\n",
    "\n",
    "#### Initial prompt by the system from which we create a TextSignal and store it\n",
    "initial_prompt = f\"{choice(TALK_TO_ME)}\"\n",
    "print(AGENT + \": \" + initial_prompt)\n",
    "textSignal = d_util.create_text_signal(scenario_ctrl, initial_prompt)\n",
    "scenario_ctrl.append_signal(textSignal)\n",
    "\n",
    "utterance = \"\"\n",
    "#### Get input and loop\n",
    "while not (utterance.lower() == 'stop' or utterance.lower() == 'bye'):\n",
    "    ###### Getting the next input signals\n",
    "    utterance = input('\\n')\n",
    "    print(HUMAN_NAME + \": \" + utterance)\n",
    "    textSignal = d_util.create_text_signal(scenario_ctrl, utterance)\n",
    "    scenario_ctrl.append_signal(textSignal)\n",
    "\n",
    "    #### Process input and generate reply\n",
    "    \n",
    "    capsule, reply = talk.process_text_and_reply(scenario_ctrl,\n",
    "                           place_id,\n",
    "                           location,\n",
    "                           HUMAN_ID,\n",
    "                           textSignal,\n",
    "                           chat,\n",
    "                           replier,\n",
    "                           my_brain,\n",
    "                           print_details)\n",
    "\n",
    "    print(AGENT + \": \" + reply)\n",
    "    textSignal = d_util.create_text_signal(scenario_ctrl, reply)\n",
    "    scenario_ctrl.append_signal(textSignal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the scenario data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_ctrl.scenario.ruler.end = int(time.time() * 1e3)\n",
    "scenarioStorage.save_scenario(scenario_ctrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
