{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's chat with a friend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo chat with Leolani. Leolani uses face recognition and gender/age\n",
    "estimation to estiablish your identity. When you are new, it will add you to her friends.\n",
    "\n",
    "To use the face functions, you need to install Docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.11.1-cp37-cp37m-macosx_10_9_x86_64.whl (1.2 MB)\n",
      "     |████████████████████████████████| 1.2 MB 3.8 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: pillow!=8.3.0,>=5.3.0 in /Users/piek/PycharmProjects/cltl-chatbots/venv/lib/python3.7/site-packages (from torchvision) (8.4.0)\n",
      "Requirement already satisfied: numpy in /Users/piek/PycharmProjects/cltl-chatbots/venv/lib/python3.7/site-packages (from torchvision) (1.21.3)\n",
      "Collecting torch==1.10.0\n",
      "  Downloading torch-1.10.0-cp37-none-macosx_10_9_x86_64.whl (147.1 MB)\n",
      "     |████████████████████████████████| 147.1 MB 7.9 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /Users/piek/PycharmProjects/cltl-chatbots/venv/lib/python3.7/site-packages (from torch==1.10.0->torchvision) (3.10.0.2)\n",
      "Installing collected packages: torch, torchvision\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.9.0\n",
      "    Uninstalling torch-1.9.0:\n",
      "      Successfully uninstalled torch-1.9.0\n",
      "Successfully installed torch-1.10.0 torchvision-0.11.1\n"
     ]
    }
   ],
   "source": [
    "#! pip install matplotlib\n",
    "#! pip install pandas\n",
    "#! pip install seaborn\n",
    "#! pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/PycharmProjects/cltl-chatbots/venv/lib/python3.7/site-packages/rdflib_jsonld/__init__.py:12: DeprecationWarning: The rdflib-jsonld package has been integrated into rdflib as of rdflib==6.0.1.  Please remove rdflib-jsonld from your project's dependencies.\n",
      "  DeprecationWarning,\n"
     ]
    }
   ],
   "source": [
    "import emissor as em\n",
    "from emissor.persistence import ScenarioStorage\n",
    "from emissor.representation.annotation import AnnotationType, Token, NER\n",
    "from emissor.representation.container import Index\n",
    "from emissor.representation.scenario import Modality, ImageSignal, TextSignal, Mention, Annotation, Scenario\n",
    "from cltl import brain\n",
    "from cltl.brain.utils.helper_functions import brain_response_to_json\n",
    "\n",
    "#Others\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the chatbot utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "src_path = os.path.abspath(os.path.join('..'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "#### The next utils are needed for the interaction and creating triples and capsules\n",
    "import chatbots.util.driver_util as d_util\n",
    "import chatbots.util.capsule_util as c_util\n",
    "import chatbots.util.face_util as f_util\n",
    "import chatbots.intentions.talk as talk\n",
    "import chatbots.intentions.get_to_know_you as friend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Link your camera\n",
    "camera = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard initialisation of a scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  /Users/piek/PycharmProjects/cltl-chatbots/data/2021-11-15-09:46:49  Created \n",
      "Directory  /Users/piek/PycharmProjects/cltl-chatbots/data/2021-11-15-09:46:49/image  Created \n"
     ]
    }
   ],
   "source": [
    "from random import getrandbits\n",
    "import requests\n",
    "##### Setting the location\n",
    "place_id = getrandbits(8)\n",
    "location = requests.get(\"https://ipinfo.io\").json()\n",
    "\n",
    "##### Setting the agents\n",
    "AGENT = \"Leolani2\"\n",
    "HUMAN_NAME = \"Stranger\"\n",
    "HUMAN_ID = \"stranger\"\n",
    "\n",
    "### The name of your scenario\n",
    "scenario_id = datetime.today().strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "\n",
    "### Specify the path to an existing data folder where your scenario is created and saved as a subfolder\n",
    "scenario_path = os.path.abspath(os.path.join('../../data'))\n",
    "if scenario_path not in sys.path:\n",
    "    sys.path.append(scenario_path)\n",
    "\n",
    "    ### Specify the path to an existing data folder where your scenario is created and saved as a subfolder\n",
    "scenario_path = os.path.abspath(os.path.join('../../data'))\n",
    "if scenario_path not in sys.path:\n",
    "    sys.path.append(scenario_path)\n",
    "\n",
    "if not os.path.exists(scenario_path) :\n",
    "    os.mkdir(scenario_path)\n",
    "    print(\"Created a data folder for storing the scenarios\", scenario_path)\n",
    "\n",
    "### Create the scenario folder, the json files and a scenarioStorage and scenario in memory\n",
    "scenarioStorage = d_util.create_scenario(scenario_path, scenario_id)\n",
    "scenario = scenarioStorage.create_scenario(scenario_id, datetime.now().microsecond, datetime.now().microsecond, AGENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the location of the face embedding information for her friends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The faces of friends are stored in a folder as embeddings. Every friend is identified through a name, gender and age property detected by the software. The name and the system time is used to create a unique identifier. We now save this in the file name of the mebdding file. A future version, we will create a json structure with the meta data on identities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paths with the friends: /Users/piek/PycharmProjects/cltl-chatbots/friend_embeddings\n"
     ]
    }
   ],
   "source": [
    "### Specify the path to an existing folder with the embeddings of your friends\n",
    "friends_path = os.path.abspath(os.path.join('../../friend_embeddings'))\n",
    "if friends_path not in sys.path:\n",
    "    sys.path.append(friends_path)\n",
    "\n",
    "print(\"The paths with the friends:\", friends_path)\n",
    "\n",
    "### Define the folder where the images are saved\n",
    "imagefolder = scenario_path + \"/\" + scenario_id + \"/\" + \"image\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the docker containers for face detection and face property detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You only need to load the dockers once. The first time you load the docker, the images will be donwloaded from the DockerHub. This may take a few minutes depending on the speed of the internet connection. The images are cached in your local Docker installation.\n",
    "\n",
    "One the images are in your local Docker, they are loaded instantaniously. Once the docker is started you do not need to start it again and you can skip the next commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#container_fdr = f_util.start_docker_container(\"tae898/face-detection-recognition\", 10002)\n",
    "#container_ag = f_util.start_docker_container(\"tae898/age-gender\", 10003)\n",
    "#container_yolo = f_util.start_docker_container(\"tae898/yolov5\", 10004)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is a problem starting the dockers, you may need to kill them and start them again. Use the following command to kill and rerun the previous command. Note that if there are running already you should not restart. Starting it again gives an error that the port is occupied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404c4820ea1d\n",
      "28309175abc8\n"
     ]
    }
   ],
   "source": [
    "#!docker kill $(docker ps -q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are now set to make a new friend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions in *intentions/get_to_know_you.py* are needed to get the properties and visual information for identifying a new friend.\n",
    "\n",
    "The visual information is based on the camera images of the uses from which we extract an averaged embedding.\n",
    "These embeddings are store in the *friend_embeddings* folder. \n",
    "\n",
    "By comparing an image with the stored embeddings, the system decides whether a person is a *stranger*.\n",
    "In case the user is a *stranger*, the system will try to get to know him/her.\n",
    "\n",
    "If you delete someone's embeddings from the *friend_embeddings* folder. This person will become a *stranger* again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First signals to get started\n",
    "success, frame = camera.read()\n",
    "imagepath = \"\"\n",
    "if success:\n",
    "    current_time = str(datetime.now().microsecond)\n",
    "    imagepath = imagefolder + \"/\" + current_time + \".png\"\n",
    "    cv2.imwrite(imagepath, frame)\n",
    "    (\n",
    "        genders,\n",
    "        ages,\n",
    "        face_bboxes,\n",
    "        faces_detected,\n",
    "        det_scores,\n",
    "        embeddings,\n",
    "        yolo_results\n",
    "    ) = f_util.do_stuff_with_image(friends_path, imagepath)\n",
    "\n",
    "    # Initial prompt by the system from which we create a TextSignal and store it\n",
    "\n",
    "    # Here we assume that only one face is in the image\n",
    "    # TODO: deal with multiple people.\n",
    "    for k, (gender, age, face_bbox, uuid_name, faceprob, embedding) in enumerate(\n",
    "        zip(genders, ages, face_bboxes, faces_detected, det_scores, embeddings)\n",
    "    ):\n",
    "        age = round(age[\"mean\"])\n",
    "        gender = \"male\" if gender[\"m\"] > 0.5 else \"female\"\n",
    "        face_bbox = [int(num) for num in face_bbox.tolist()]\n",
    "\n",
    "    assert k == 0\n",
    "\n",
    "    if uuid_name[\"name\"] is None:\n",
    "        ### This is a stranger\n",
    "        ### We create the agent response and store it as a text signal\n",
    "        \n",
    "        HUMAN_ID, HUMAN_NAME, textSignal = friend.get_to_know_person(scenario, AGENT, gender, age, uuid_name, embedding, friends_path)\n",
    "        HUMAN_ID = HUMAN_NAME  ### Hack because we cannot force the namespace through capsules, name and identity are the same till this is fixed\n",
    "\n",
    "        ### The system responds to the processing of the new name input and stores it as a textsignal\n",
    "        response = f\": Nice to meet you, {HUMAN_NAME}\"\n",
    "        print(f\"{AGENT}: {response}\\n\")\n",
    "        textSignal = d_util.create_text_signal(scenario, response)\n",
    "        scenario.append_signal(textSignal)\n",
    "\n",
    "    else:\n",
    "        ### We know this person\n",
    "        HUMAN_ID= uuid_name['name']\n",
    "        HUMAN_NAME = HUMAN_ID  ### Hack because we cannot force the namespace through capsules, name and identity are the same till this is fixed \n",
    "        # HUMAN_NAME = HUMAN_ID.split(\"_t_\")[0]\n",
    "\n",
    "        response = f\"Hi {HUMAN_NAME}. Nice to see you again. How are you today?\"\n",
    "        print(f\"{AGENT}: {response}\\n\")\n",
    "        textSignal = d_util.create_text_signal(scenario, response)\n",
    "        scenario.append_signal(textSignal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have a conversation with a friend\n",
    "\n",
    "Below is a simple chat scenario in which we can say anything to our identified friend and store images and conversation in the EMISSOR scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### First prompt\n",
    "\n",
    "#response = \"How are you doing today, \"+HUMAN_NAME\n",
    "print(f\"{AGENT}: {response}\\n\")\n",
    "#textSignal = d_util.create_text_signal(scenario, response)\n",
    "#scenario.append_signal(textSignal)\n",
    "\n",
    "utterance = input(\"\\n\")\n",
    "print(f\"{HUMAN_NAME}: {utterance}\\n\")\n",
    "\n",
    "while not (utterance.lower() == \"stop\" or utterance.lower() == \"bye\"):\n",
    "    textSignal = d_util.create_text_signal(scenario, utterance)\n",
    "    scenario.append_signal(textSignal)\n",
    "\n",
    "    # @TODO: also annotate the textSignal\n",
    "    # Apply some processing to the textSignal and add annotations\n",
    "        \n",
    "        \n",
    "    ## We capture the image again\n",
    "    if success:\n",
    "        imageSignal = d_util.create_image_signal(scenario, imagepath)\n",
    "        container_id = str(uuid.uuid4())\n",
    "\n",
    "        #### Properties are now stored as annotations\n",
    "        #### We do not store these proeprties again to the BRAIN\n",
    "        for gender, age, face_bbox, name, faceprob in zip(\n",
    "            genders, ages, face_bboxes, faces_detected, det_scores\n",
    "        ):\n",
    "\n",
    "            age = round(age[\"mean\"])\n",
    "            gender = \"male\" if gender[\"m\"] > 0.5 else \"female\"\n",
    "            face_bbox = [int(num) for num in face_bbox.tolist()]\n",
    "        \n",
    "        f_util.add_face_annotation(imageSignal, container_id, \"front_camera\", container_id, current_time,\n",
    "                                   face_bbox, HUMAN_ID, HUMAN_NAME, age, gender, faceprob)\n",
    " \n",
    "        scenario.append_signal(imageSignal)\n",
    "\n",
    "\n",
    "    # Create the response from the system and store this as a new signal\n",
    "    # We could use the throughts to respond\n",
    "    # @TODO generate a response from the thoughts\n",
    "\n",
    "    response = \"So you what do you want to talk about \" + HUMAN_NAME\n",
    "    print(f\"{AGENT}: {response}\\n\")\n",
    "    textSignal = d_util.create_text_signal(scenario, utterance)\n",
    "    scenario.append_signal(textSignal)\n",
    "          \n",
    "    # Getting the next input signals\n",
    "    utterance = input(\"\\n\")\n",
    "\n",
    "    success, frame = camera.read()\n",
    "    if success:\n",
    "        current_time = str(datetime.now().microsecond)\n",
    "        imagepath = imagefolder + \"/\" + current_time + \".png\"\n",
    "        cv2.imwrite(imagepath, frame)\n",
    "        (\n",
    "            genders,\n",
    "            ages,\n",
    "            face_bboxes,\n",
    "            faces_detected,\n",
    "            det_scores,\n",
    "            embeddings,\n",
    "            yolo_results\n",
    "        ) = f_util.do_stuff_with_image(friends_path, imagepath)\n",
    "        \n",
    "        \n",
    "        # Here we assume that only one face is in the image\n",
    "        # TODO: deal with multiple people.\n",
    "        for k, (gender, age, face_bbox, uuid_name, faceprob, embedding) in enumerate(\n",
    "            zip(genders, ages, face_bboxes, faces_detected, det_scores, embeddings)\n",
    "        ):\n",
    "            age = round(age[\"mean\"])\n",
    "            gender = \"male\" if gender[\"m\"] > 0.5 else \"female\"\n",
    "            face_bbox = [int(num) for num in face_bbox.tolist()]\n",
    "\n",
    "        assert k == 0\n",
    "\n",
    "        if uuid_name[\"name\"] is None:\n",
    "            ### This is a stranger\n",
    "            ### We create the agent response and store it as a text signal\n",
    "            \n",
    "            ### The system responds to the user switch\n",
    "            response = f\": Goodbye, {HUMAN_NAME}. And who are you?\"\n",
    "            print(f\"{AGENT}: {response}\\n\")\n",
    "            textSignal = d_util.create_text_signal(scenario, response)\n",
    "            scenario.append_signal(textSignal)\n",
    "            \n",
    "            ### Establish a new name and id\n",
    "            HUMAN_ID, HUMAN_NAME, textSignal = friend.get_to_know_person(scenario, AGENT, gender, age, uuid_name, embedding, friends_path)\n",
    "\n",
    "            ### The system responds to the processing of the new name input and stores it as a textsignal\n",
    "            response = f\": Nice to meet you, {HUMAN_NAME}\"\n",
    "            print(f\"{AGENT}: {response}\\n\")\n",
    "            textSignal = d_util.create_text_signal(scenario, response)\n",
    "            scenario.append_signal(textSignal)\n",
    "\n",
    "        else:\n",
    "            ### We know this person but it is a different person then the one we were talking to\n",
    "            if not HUMAN_ID == uuid_name['name']:\n",
    "                \n",
    "                ### The system responds to the user switch\n",
    "                response = f\": Goodbye, {HUMAN_NAME}. And who are you?\"\n",
    "                print(f\"{AGENT}: {response}\\n\")\n",
    "                textSignal = d_util.create_text_signal(scenario, response)\n",
    "                scenario.append_signal(textSignal)\n",
    "                \n",
    "                ### set the name and id for this other friend\n",
    "                HUMAN_ID =  uuid_name['name']\n",
    "                HUMAN_NAME = HUMAN_ID.split(\"_t_\")[0]\n",
    "                response = f\"Hi {HUMAN_NAME}. Nice to see you too.)\"\n",
    "                print(f\"{AGENT}: {response}\\n\")\n",
    "                textSignal = d_util.create_text_signal(scenario, response)\n",
    "                scenario.append_signal(textSignal)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the end time of the scenario, save it and stop the containers\n",
    "\n",
    "After we stopped the interaction, we set the end time and save the scenario as EMISSOR data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scenario.scenario.end = datetime.now().microsecond\n",
    "scenarioStorage.save_scenario(scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stopping the docker containers\n",
    "### This is only needed of you started them in this notebook\n",
    "\n",
    "f_util.kill_container(container_fdr)\n",
    "f_util.kill_container(container_ag)\n",
    "f_util.kill_container(container_yolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Stop the camera when we are done\n",
    "camera.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dced07e6e00e0657358888cb7cf20ae6e3a0ff710d8ed1b629e094220a24ec6d"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
