{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMISSOR chat bot with a BRAIN that interacts through camera and NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we create capsules with triples from interaction through EMISSOR that are send to the Brain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates a simple chatbot that also takes pictures through your camera while interacting with you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Installing the platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "virtualenv venv\n",
    "\n",
    "# install the brain\n",
    "#pip install --extra-index-url https://test.pypi.org/simple cltl.brain\n",
    "\n",
    "# install the Leolani baseline triple extractor\n",
    "#pip install git+git://github.com/leolani/cltl-knowledgeextraction.git@main\n",
    "\n",
    "# install emissor\n",
    "#%python3 -m pip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple emissor\n",
    "\n",
    "Alternatively:\n",
    "* git clone https://github.com/cltl/EMISSOR --branch issue-53-processing\n",
    "* cd EMISSOR\n",
    "* python install.py install\n",
    "\n",
    "# install spacy\n",
    "#%pip install -U spacy\n",
    "\n",
    "# install language models\n",
    "#%python -m spacy download en_core_web_sm\n",
    "\n",
    "# install cv2\n",
    "#%pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+git://github.com/leolani/cltl-knowledgeextraction.git@main\n",
      "  Cloning git://github.com/leolani/cltl-knowledgeextraction.git (to revision main) to /private/var/folders/6w/bw7dqbl9727c48pcjjh32r140000gn/T/pip-req-build-dps1yd1a\n",
      "  Running command git clone -q git://github.com/leolani/cltl-knowledgeextraction.git /private/var/folders/6w/bw7dqbl9727c48pcjjh32r140000gn/T/pip-req-build-dps1yd1a\n",
      "  Resolved git://github.com/leolani/cltl-knowledgeextraction.git to commit e07bd458cdd61c5cea4e075faee9798c8227eb7c\n",
      "Requirement already satisfied: nltk~=3.4.4 in ./venv/lib/python3.7/site-packages (from cltl.triple-extraction==0.0.dev3) (3.4.4)\n",
      "Requirement already satisfied: six in ./venv/lib/python3.7/site-packages (from nltk~=3.4.4->cltl.triple-extraction==0.0.dev3) (1.16.0)\n",
      "Building wheels for collected packages: cltl.triple-extraction\n",
      "  Building wheel for cltl.triple-extraction (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for cltl.triple-extraction: filename=cltl.triple_extraction-0.0.dev3-py3-none-any.whl size=70736753 sha256=da00cdda632ac9213fe9dddbfeea248851a91eda3d79e61a94ac056ebb18ba99\n",
      "  Stored in directory: /private/var/folders/6w/bw7dqbl9727c48pcjjh32r140000gn/T/pip-ephem-wheel-cache-4hyp9wll/wheels/15/c1/aa/628ed9d1c391e8f80121d46d441a472ae5c6491e7edf80d37a\n",
      "Successfully built cltl.triple-extraction\n",
      "Installing collected packages: cltl.triple-extraction\n",
      "Successfully installed cltl.triple-extraction-0.0.dev3\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Users/piek/PycharmProjects/cltl-chatbots/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install git+git://github.com/leolani/cltl-knowledgeextraction.git@main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting the scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emissor\n",
    "from cltl import brain\n",
    "from cltl.triple_extraction.api import Chat, UtteranceHypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import time\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from emissor.persistence import ScenarioStorage\n",
    "from emissor.representation.annotation import AnnotationType, Token, NER\n",
    "from emissor.representation.container import Index\n",
    "from emissor.representation.scenario import Modality, ImageSignal, TextSignal, Mention, Annotation, Scenario\n",
    "from cltl.brain.long_term_memory import LongTermMemory\n",
    "from cltl.combot.backend.api.discrete import UtteranceType\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from random import getrandbits\n",
    "import requests\n",
    "\n",
    "#### The next utils are needed for the interaction and creating triples and capsules\n",
    "import util.driver_util as d_util\n",
    "import util.capsule_util as c_util\n",
    "import dummies.text_to_triple as ttt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Creating the scenario and basic values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  ./data/myscenario3  Created \n",
      "Directory  ./data/myscenario3/image  Created \n"
     ]
    }
   ],
   "source": [
    "from random import getrandbits\n",
    "\n",
    "##### Setting the location\n",
    "place_id = getrandbits(8)\n",
    "location = requests.get(\"https://ipinfo.io\").json()\n",
    "\n",
    "##### Setting the agents\n",
    "agent = \"Leolani2\"\n",
    "human = \"Stranger\"\n",
    "\n",
    "### The name of your scenario\n",
    "scenarioid = \"myscenario3\"\n",
    "\n",
    "### Specify the path to an existing data folder where your scenario is created and saved as a subfolder\n",
    "scenario_path = \"./data\"\n",
    "\n",
    "### Define the folder where the images are saved\n",
    "imagefolder = scenario_path + \"/\" + scenarioid + \"/\" + \"image\"\n",
    "\n",
    "\n",
    "### Create the scenario folder, the json files and a scenarioStorage and scenario in memory\n",
    "scenarioStorage = d_util.create_scenario(scenario_path, scenarioid)\n",
    "scenario = scenarioStorage.create_scenario(scenarioid, datetime.now().microsecond, datetime.now().microsecond, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Adding NERC to a TextSignal as annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Example of an annotation function that adds annotations to a Signal\n",
    "#### It adds NERC annotations to the TextSignal and returns a list of entities detected\n",
    "\n",
    "def add_ner_annotation(signal: TextSignal):\n",
    "    processor_name = \"spaCy\"\n",
    "    utterance = ''.join(signal.seq)\n",
    "\n",
    "    doc = nlp(utterance)\n",
    "\n",
    "    offsets, tokens = zip(*[(Index(signal.id, token.idx, token.idx + len(token)), Token.for_string(token.text))\n",
    "                            for token in doc])\n",
    "\n",
    "    ents = [NER.for_string(ent.label_) for ent in doc.ents]\n",
    "    entity_list = [ent.text for ent in doc.ents]\n",
    "    segments = [token.ruler for token in tokens if token.value in entity_list]\n",
    "\n",
    "    annotations = [Annotation(AnnotationType.TOKEN.name.lower(), token, processor_name, int(time.time()))\n",
    "                   for token in tokens]\n",
    "    ner_annotations = [Annotation(AnnotationType.NER.name.lower(), ent, processor_name, int(time.time()))\n",
    "                       for ent in ents]\n",
    "\n",
    "    signal.mentions.extend([Mention(str(uuid.uuid4()), [offset], [annotation])\n",
    "                            for offset, annotation in zip(offsets, annotations)])\n",
    "    signal.mentions.extend([Mention(str(uuid.uuid4()), [segment], [annotation])\n",
    "                            for segment, annotation in zip(segments, ner_annotations)])\n",
    "    print(entity_list)\n",
    "    return entity_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Creating a capsule from the interaction with a triple and perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function creates a capsule using the Scenario and Signal in combination with values for the perspectives, subject, predicate and object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_text (seq):\n",
    "    text = \"\"\n",
    "    for c in seq:\n",
    "        text+=c\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Starting the interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Creating a camera and nlp module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a camera and load the NLP model for spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Link your camera\n",
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "### Load a language model in spaCy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Initialising a BRAIN in GraphDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we need to create an empty Brain or load an existing Brain. The next code assumes we have a repository in GraphDB with the name sandbox as a brain. By setting clear_all=True it is emptied and next loaded with the background ontologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pathlib.PosixPath'>\n",
      "2021-10-27 09:32:39,448 -    DEBUG -    cltl.brain.basic_brain.LongTermMemory - Booted\n",
      "2021-10-27 09:32:39,449 -    DEBUG -    cltl.brain.basic_brain.LongTermMemory - Clearing brain\n",
      "2021-10-27 09:32:43,455 -    DEBUG -    cltl.brain.basic_brain.LongTermMemory - Checking if ontology is in brain\n",
      "2021-10-27 09:32:43,459 -    DEBUG -    cltl.brain.basic_brain.LongTermMemory - Posting query\n",
      "2021-10-27 09:32:44,208 -     INFO -    cltl.brain.basic_brain.LongTermMemory - Uploading ontology to brain\n",
      "2021-10-27 09:32:46,103 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Booted\n",
      "2021-10-27 09:32:46,105 -    DEBUG -  cltl.brain.basic_brain.LocationReasoner - Booted\n",
      "2021-10-27 09:32:46,106 -    DEBUG -      cltl.brain.basic_brain.TypeReasoner - Booted\n",
      "2021-10-27 09:32:46,108 -    DEBUG -   cltl.brain.basic_brain.TrustCalculator - Booted\n",
      "2021-10-27 09:32:46,196 -    DEBUG -   cltl.brain.basic_brain.TrustCalculator - Posting query\n",
      "2021-10-27 09:32:46,216 -     INFO -   cltl.brain.basic_brain.TrustCalculator - Computed trust for all known agents\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "log_path=pathlib.Path('./logs')\n",
    "print(type(log_path))\n",
    "my_brain = brain.LongTermMemory(address=\"http://localhost:7200/repositories/sandbox\",\n",
    "                           log_dir=log_path,\n",
    "                           clear_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Starting the CHAT which creates a scenario and saves triples to the BRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leolani2: Hi there. Who are you Stranger?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " Hi there\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stranger: Hi there\n",
      "[]\n",
      "Leolani2: Any gossip\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " No news. I am Peter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Peter']\n",
      "Subject: Peter Predicate: denotedBy Object: 640724cf-9907-4c67-a054-05ec054dfa9a\n",
      "2021-10-27 12:34:20,077 -    DEBUG -    cltl.brain.basic_brain.LongTermMemory - Posting query\n",
      "2021-10-27 12:34:20,142 -    DEBUG -  cltl.brain.basic_brain.LocationReasoner - Posting query\n",
      "2021-10-27 12:34:20,301 -     INFO -    cltl.brain.basic_brain.LongTermMemory - Triple in statement: peter_denotedby_640724cf-9907-4c67-a054-05ec054dfa9a [person_->_object])\n",
      "2021-10-27 12:34:20,303 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n",
      "2021-10-27 12:34:20,343 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n",
      "2021-10-27 12:34:20,389 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n",
      "2021-10-27 12:34:20,396 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Entity Novelty: new subject - new object \n",
      "2021-10-27 12:34:20,397 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n",
      "2021-10-27 12:34:20,446 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n",
      "2021-10-27 12:34:21,595 -    DEBUG -    cltl.brain.basic_brain.LongTermMemory - Posting triples\n",
      "2021-10-27 12:34:23,894 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n",
      "2021-10-27 12:34:23,987 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n",
      "2021-10-27 12:34:24,100 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n",
      "2021-10-27 12:34:24,188 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Gaps: 26 gaps as subject: e.g. be-ancestor-of person - 15 gaps as object: e.g. write-by book\n",
      "2021-10-27 12:34:24,189 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n",
      "2021-10-27 12:34:24,238 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n",
      "2021-10-27 12:34:24,287 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Gaps: 0 gaps as subject: e.g. '' - 2 gaps as object: e.g. own agent\n",
      "2021-10-27 12:34:24,289 -    DEBUG -   cltl.brain.basic_brain.TrustCalculator - Posting query\n",
      "Leolani2: I am thinking: new subject - new object; 0 gaps as subject: e.g. '' - 2 gaps as object: e.g. own agent; 0 subject overlaps: e.g. '' - 0 object overlaps: e.g. '';26 gaps as subject: e.g. play song - 15 gaps as object: e.g. like-by interest; \n"
     ]
    }
   ],
   "source": [
    "##### First signals to get started\n",
    "success, frame = camera.read()\n",
    "imagepath = \"\"\n",
    "if success:\n",
    "    imagepath = imagefolder + \"/\" + str(datetime.now().microsecond) + \".png\"\n",
    "    cv2.imwrite(imagepath, frame)\n",
    "\n",
    "#### Initial prompt by the system from which we create a TextSignal and store it\n",
    "initial_prompt = \"Hi there. Who are you \" + human + \"?\"\n",
    "print(agent + \": \" + initial_prompt)\n",
    "textSignal = d_util.create_text_signal(scenario, initial_prompt)\n",
    "scenario.append_signal(textSignal)\n",
    "\n",
    "utterance = input('\\n')\n",
    "print(human + \": \" + utterance)\n",
    "while not (utterance.lower() == 'stop' or utterance.lower() == 'bye'):\n",
    "    textSignal = d_util.create_text_signal(scenario, utterance)\n",
    "    # @TODO\n",
    "    ### Apply some processing to the textSignal and add annotations\n",
    "    entityText = add_ner_annotation(textSignal)\n",
    "    scenario.append_signal(textSignal)\n",
    "    ## Post triples to the brain:\n",
    "\n",
    "    subj, pred, obj = ttt.getTriplesFromEntities(entityText, textSignal.id)\n",
    "\n",
    "    response = {}\n",
    "    if not subj==\"\":\n",
    "        print('Subject:', subj, 'Predicate:', pred, 'Object:', obj)\n",
    "        perspective = {\"certainty\": 1, \"polarity\": 1, \"sentiment\": 1}\n",
    "        capsule = c_util.scenario_utterance_to_capsule(scenario, textSignal, human, perspective, subj, pred, obj)\n",
    "        #print('Capsule:', capsule)\n",
    "        response = my_brain.update(capsule, reason_types=True)\n",
    "        #print(thoughts)\n",
    "        \n",
    "    if success:\n",
    "        imageSignal = d_util.create_image_signal(scenario, imagepath)\n",
    "        # @TODO\n",
    "        ### Apply some processing to the imageSignal and add annotations\n",
    "        ### when done\n",
    "\n",
    "        scenario.append_signal(imageSignal)\n",
    "\n",
    "    # Create the response from the system and store this as a new signal\n",
    "    utterance = ttt.getTextFromTriples(response)\n",
    "    if not utterance:\n",
    "        if not entityText:\n",
    "            utterance = \"Any gossip\" + '\\n'\n",
    "        else:\n",
    "            utterance = \"So you what do you want to talk about \" + entityText[0] + '\\n'\n",
    "\n",
    "    response = utterance[::-1]\n",
    "    print(agent + \": \" + utterance)\n",
    "    textSignal = d_util.create_text_signal(scenario, utterance)\n",
    "    scenario.append_signal(textSignal)\n",
    "\n",
    "    ###### Getting the next input signals\n",
    "    utterance = input('\\n')\n",
    "\n",
    "    success, frame = camera.read()\n",
    "    if success:\n",
    "        imagepath = imagefolder + \"/\" + str(datetime.now().microsecond) + \".png\"\n",
    "        cv2.imwrite(imagepath, frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After we stopped the interaction, we set the end time of the scenarion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario.scenario.end = datetime.now().microsecond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Saving the Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarioStorage.save_scenario(scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
