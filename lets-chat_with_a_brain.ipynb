{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMISSOR chat bot with a BRAIN that interacts through camera and NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we create capsules with triples from inteaction through EMISSOR that are send to the Brain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates a simple chatbot that also takes pictures through your camera while interacting with you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Installing the platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "virtualenv venv\n",
    "\n",
    "# install the brain\n",
    "#pip install --extra-index-url https://test.pypi.org/simple cltl.brain\n",
    "\n",
    "# install emissor\n",
    "#%python3 -m pip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple emissor\n",
    "\n",
    "Alternatively:\n",
    "* git clone https://github.com/cltl/EMISSOR --branch issue-53-processing\n",
    "* cd EMISSOR\n",
    "* python install.py install\n",
    "\n",
    "# install spacy\n",
    "#%pip install -U spacy\n",
    "\n",
    "# install language models\n",
    "#%python -m spacy download en_core_web_sm\n",
    "\n",
    "# install cv2\n",
    "#%pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting the scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emissor\n",
    "from cltl import brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import time\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from emissor.persistence import ScenarioStorage\n",
    "from emissor.representation.annotation import AnnotationType, Token, NER\n",
    "from emissor.representation.container import Index\n",
    "from emissor.representation.scenario import Modality, ImageSignal, TextSignal, Mention, Annotation, Scenario\n",
    "from cltl.brain.long_term_memory import LongTermMemory\n",
    "from cltl.combot.backend.api.discrete import UtteranceType\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from random import getrandbits\n",
    "import requests\n",
    "\n",
    "#### The next utils are needed for the interaction and creating triples and capsules\n",
    "import driver_util as d_util\n",
    "import capsule_utils as c_util\n",
    "import text_to_triple as ttt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Creating the scenario and basic values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  ./data/myscenario3  already exists\n",
      "Directory  ./data/myscenario3/image  already exists\n"
     ]
    }
   ],
   "source": [
    "from random import getrandbits\n",
    "\n",
    "##### Setting the location\n",
    "place_id = getrandbits(8)\n",
    "location = requests.get(\"https://ipinfo.io\").json()\n",
    "\n",
    "##### Setting the agents\n",
    "agent = \"Leolani2\"\n",
    "human = \"Stranger\"\n",
    "\n",
    "### The name of your scenario\n",
    "scenarioid = \"myscenario3\"\n",
    "\n",
    "### Specify the path to an existing data folder where your scenario is created and saved as a subfolder\n",
    "scenario_path = \"./data\"\n",
    "\n",
    "### Define the folder where the images are saved\n",
    "imagefolder = scenario_path + \"/\" + scenarioid + \"/\" + \"image\"\n",
    "\n",
    "\n",
    "### Create the scenario folder, the json files and a scenarioStorage and scenario in memory\n",
    "scenarioStorage = d_util.create_scenario(scenario_path, scenarioid)\n",
    "scenario = scenarioStorage.create_scenario(scenarioid, datetime.now().microsecond, datetime.now().microsecond, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Adding NERC to a TextSignal as annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Example of an annotation function that adds annotations to a Signal\n",
    "#### It adds NERC annotations to the TextSignal and returns a list of entities detected\n",
    "\n",
    "def add_ner_annotation(signal: TextSignal):\n",
    "    processor_name = \"spaCy\"\n",
    "    utterance = ''.join(signal.seq)\n",
    "\n",
    "    doc = nlp(utterance)\n",
    "\n",
    "    offsets, tokens = zip(*[(Index(signal.id, token.idx, token.idx + len(token)), Token.for_string(token.text))\n",
    "                            for token in doc])\n",
    "\n",
    "    ents = [NER.for_string(ent.label_) for ent in doc.ents]\n",
    "    entity_list = [ent.text for ent in doc.ents]\n",
    "    segments = [token.ruler for token in tokens if token.value in entity_list]\n",
    "\n",
    "    annotations = [Annotation(AnnotationType.TOKEN.name.lower(), token, processor_name, int(time.time()))\n",
    "                   for token in tokens]\n",
    "    ner_annotations = [Annotation(AnnotationType.NER.name.lower(), ent, processor_name, int(time.time()))\n",
    "                       for ent in ents]\n",
    "\n",
    "    signal.mentions.extend([Mention(str(uuid.uuid4()), [offset], [annotation])\n",
    "                            for offset, annotation in zip(offsets, annotations)])\n",
    "    signal.mentions.extend([Mention(str(uuid.uuid4()), [segment], [annotation])\n",
    "                            for segment, annotation in zip(segments, ner_annotations)])\n",
    "    print(entity_list)\n",
    "    return entity_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Creating a capsule from the interaction with a triple and perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function creates a capsule using the Scenario and Signal in combination with values for the perspectives, subject, predicate and object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_text (seq):\n",
    "    text = \"\"\n",
    "    for c in seq:\n",
    "        text+=c\n",
    "    return text\n",
    "\n",
    "\n",
    "def scenario_utterance_to_capsule(scenario: Scenario, signal: TextSignal, author:str, perspective:str, subj: str, pred:str, obj:str):\n",
    "    place_id = getrandbits(8)\n",
    "    location = requests.get(\"https://ipinfo.io\").json()\n",
    "\n",
    "    capsule = {\"chat\":scenario.id,\n",
    "                   \"turn\":signal.id,\n",
    "                   \"author\": \"carl\",\n",
    "                    \"utterance\": seq_to_text(signal.seq),\n",
    "                    \"utterance_type\": UtteranceType.STATEMENT,\n",
    "                    \"position\": \"0-\"+str(len(signal.seq)),  #TODO generate the true offset range\n",
    "                    \"subject\": {\"label\": subj, \"type\": \"person\"},\n",
    "                    \"predicate\": {\"type\": pred},\n",
    "                    \"object\":  {\"label\": obj, \"type\": \"object\"},\n",
    "                    \"perspective\": perspective ,\n",
    "                    \"context_id\": scenario.scenario.context,\n",
    "                    \"date\": date.today(),\n",
    "                    \"place\": location['city'],\n",
    "                    \"place_id\": place_id,\n",
    "                    \"country\": location['country'],\n",
    "                    \"region\": location['region'],\n",
    "                    \"city\": location['city'],\n",
    "                    \"objects\": [{'type': 'chair', 'confidence': 0.59, 'id': 1},\n",
    "                                {'type': 'table', 'confidence': 0.73, 'id': 1},\n",
    "                               {'type': 'pillbox', 'confidence': 0.32, 'id': 1}],\n",
    "                            \"people\": [{'name': 'Carl', 'confidence': 0.98, 'id': 1}]\n",
    "                  }\n",
    "    return capsule          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Starting the interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Creating a camera and nlp module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a camera and load the NLP model for spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Link your camera\n",
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "### Load a language model in spaCy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Initialising a BRAIN in GraphDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we need to create an empty Brain or load an existing Brain. The next code assumes we have a repository in GraphDB with the name sandbox as a brain. By setting clear_all=True it is emptied and next loaded with the background ontologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pathlib.PosixPath'>\n",
      "2021-09-12 08:46:20,197 -    DEBUG -    cltl.brain.basic_brain.LongTermMemory - Booted\n",
      "2021-09-12 08:46:20,198 -    DEBUG -    cltl.brain.basic_brain.LongTermMemory - Clearing brain\n",
      "2021-09-12 08:46:22,855 -    DEBUG -    cltl.brain.basic_brain.LongTermMemory - Checking if ontology is in brain\n",
      "2021-09-12 08:46:22,858 -    DEBUG -    cltl.brain.basic_brain.LongTermMemory - Posting query\n",
      "2021-09-12 08:46:23,537 -     INFO -    cltl.brain.basic_brain.LongTermMemory - Uploading ontology to brain\n",
      "2021-09-12 08:46:26,507 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Booted\n",
      "2021-09-12 08:46:26,510 -    DEBUG -  cltl.brain.basic_brain.LocationReasoner - Booted\n",
      "2021-09-12 08:46:26,511 -    DEBUG -      cltl.brain.basic_brain.TypeReasoner - Booted\n",
      "2021-09-12 08:46:26,513 -    DEBUG -   cltl.brain.basic_brain.TrustCalculator - Booted\n",
      "2021-09-12 08:46:26,904 -    DEBUG -   cltl.brain.basic_brain.TrustCalculator - Posting query\n",
      "2021-09-12 08:46:26,952 -     INFO -   cltl.brain.basic_brain.TrustCalculator - Computed trust for all known agents\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "log_path=pathlib.Path('./logs')\n",
    "print(type(log_path))\n",
    "my_brain = brain.LongTermMemory(address=\"http://localhost:7200/repositories/sandbox\",\n",
    "                           log_dir=log_path,\n",
    "                           clear_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Starting the CHAT which creates a scenario and saves triples to the BRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leolani2: Hi there. Who are you Stranger?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " Hi there\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stranger: Hi there\n",
      "[]\n",
      "Leolani2: Any gossip\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " I am Piek\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Piek']\n",
      "Subject: Piek Predicate: denotedBy Object: c9c0d54d-301a-4537-b38f-0dc16c363247\n",
      "2021-09-12 08:49:29,304 -    DEBUG -    cltl.brain.basic_brain.LongTermMemory - Posting query\n",
      "2021-09-12 08:49:29,361 -    DEBUG -  cltl.brain.basic_brain.LocationReasoner - Posting query\n",
      "2021-09-12 08:49:29,421 -     INFO -    cltl.brain.basic_brain.LongTermMemory - Triple in statement: piek_denotedby_c9c0d54d-301a-4537-b38f-0dc16c363247 [person_->_object])\n",
      "2021-09-12 08:49:29,423 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n",
      "2021-09-12 08:49:29,458 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n",
      "2021-09-12 08:49:29,505 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n",
      "2021-09-12 08:49:29,511 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Entity Novelty: new subject - new object \n",
      "2021-09-12 08:49:29,513 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n",
      "2021-09-12 08:49:29,558 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n",
      "2021-09-12 08:49:30,664 -    DEBUG -    cltl.brain.basic_brain.LongTermMemory - Posting triples\n",
      "2021-09-12 08:49:31,707 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n",
      "2021-09-12 08:49:31,756 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n",
      "2021-09-12 08:49:31,805 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n",
      "2021-09-12 08:49:31,811 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Gaps: 26 gaps as subject: e.g. play song - 15 gaps as object: e.g. be-family-of person\n",
      "2021-09-12 08:49:31,813 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n",
      "2021-09-12 08:49:31,856 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n",
      "2021-09-12 08:49:31,902 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Gaps: 0 gaps as subject: e.g. '' - 2 gaps as object: e.g. own agent\n",
      "2021-09-12 08:49:31,904 -    DEBUG -   cltl.brain.basic_brain.TrustCalculator - Posting query\n",
      "Leolani2: I am thinking: new subject - new object; 0 gaps as subject: e.g. '' - 2 gaps as object: e.g. own agent; 0 subject overlaps: e.g. '' - 0 object overlaps: e.g. '';26 gaps as subject: e.g. experience touch - 15 gaps as object: e.g. be-family-of person; \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " You are weird\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Leolani2: Any gossip\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " I am from Weesp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Leolani2: Any gossip\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " Weesp is a little town close to AMsterdam\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AMsterdam']\n",
      "Subject: AMsterdam Predicate: denotedBy Object: e255deaf-5de3-475a-bd94-eccdc37e5afd\n",
      "2021-09-12 08:49:53,515 -    DEBUG -    cltl.brain.basic_brain.LongTermMemory - Posting query\n",
      "2021-09-12 08:49:53,557 -    DEBUG -  cltl.brain.basic_brain.LocationReasoner - Posting query\n",
      "2021-09-12 08:49:53,615 -     INFO -    cltl.brain.basic_brain.LongTermMemory - Triple in statement: amsterdam_denotedby_e255deaf-5de3-475a-bd94-eccdc37e5afd [person_->_object])\n",
      "2021-09-12 08:49:53,617 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n",
      "2021-09-12 08:49:53,704 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n",
      "2021-09-12 08:49:53,710 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n",
      "2021-09-12 08:49:53,754 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Entity Novelty: new subject - new object \n",
      "2021-09-12 08:49:53,756 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n",
      "2021-09-12 08:49:53,763 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n",
      "2021-09-12 08:49:54,834 -    DEBUG -    cltl.brain.basic_brain.LongTermMemory - Posting triples\n",
      "2021-09-12 08:49:56,005 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n",
      "2021-09-12 08:49:56,013 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n",
      "2021-09-12 08:49:56,056 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n",
      "2021-09-12 08:49:56,062 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Gaps: 26 gaps as subject: e.g. travel-to location - 15 gaps as object: e.g. read-by book\n",
      "2021-09-12 08:49:56,063 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n",
      "2021-09-12 08:49:56,104 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n",
      "2021-09-12 08:49:56,110 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Gaps: 0 gaps as subject: e.g. '' - 2 gaps as object: e.g. own agent\n",
      "2021-09-12 08:49:56,111 -    DEBUG -   cltl.brain.basic_brain.TrustCalculator - Posting query\n",
      "Leolani2: I am thinking: new subject - new object; 0 gaps as subject: e.g. '' - 2 gaps as object: e.g. own person; 0 subject overlaps: e.g. '' - 0 object overlaps: e.g. '';26 gaps as subject: e.g. like-by agent - 15 gaps as object: e.g. know agent; \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6w/bw7dqbl9727c48pcjjh32r140000gn/T/ipykernel_22508/1122568621.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m###### Getting the next input signals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mutterance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcamera\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/r-EMISSOR/code/notebooks/venv/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1008\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m         )\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/r-EMISSOR/code/notebooks/venv/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "##### First signals to get started\n",
    "success, frame = camera.read()\n",
    "imagepath = \"\"\n",
    "if success:\n",
    "    imagepath = imagefolder + \"/\" + str(datetime.now().microsecond) + \".png\"\n",
    "    cv2.imwrite(imagepath, frame)\n",
    "\n",
    "#### Initial prompt by the system from which we create a TextSignal and store it\n",
    "response = \"Hi there. Who are you \" + human + \"?\"\n",
    "print(agent + \": \" + response)\n",
    "textSignal = d_util.create_text_signal(scenario, response)\n",
    "scenario.append_signal(textSignal)\n",
    "\n",
    "utterance = input('\\n')\n",
    "print(human + \": \" + utterance)\n",
    "while not (utterance.lower() == 'stop' or utterance.lower() == 'bye'):\n",
    "    textSignal = d_util.create_text_signal(scenario, utterance)\n",
    "    # @TODO\n",
    "    ### Apply some processing to the textSignal and add annotations\n",
    "    entityText = add_ner_annotation(textSignal)\n",
    "    scenario.append_signal(textSignal)\n",
    "    ## Post triples to the brain:\n",
    "\n",
    "    subj, pred, obj = ttt.getTriplesFromEntities(entityText, textSignal.id)\n",
    "\n",
    "    response = {}\n",
    "    if not subj==\"\":\n",
    "        print('Subject:', subj, 'Predicate:', pred, 'Object:', obj)\n",
    "        perspective = {\"certainty\": 1, \"polarity\": 1, \"sentiment\": 1}\n",
    "        capsule = scenario_utterance_to_capsule(scenario, textSignal, human, perspective, subj, pred, obj)\n",
    "        #print('Capsule:', capsule)\n",
    "        response = my_brain.update(capsule, reason_types=True)\n",
    "        #print(thoughts)\n",
    "        \n",
    "    if success:\n",
    "        imageSignal = d_util.create_image_signal(scenario, imagepath)\n",
    "        # @TODO\n",
    "        ### Apply some processing to the imageSignal and add annotations\n",
    "        ### when done\n",
    "\n",
    "        scenario.append_signal(imageSignal)\n",
    "\n",
    "    # Create the response from the system and store this as a new signal\n",
    "    utterance = ttt.getTextFromTriples(response)\n",
    "    if not utterance:\n",
    "        if not entityText:\n",
    "            utterance = \"Any gossip\" + '\\n'\n",
    "        else:\n",
    "            utterance = \"So you what do you want to talk about \" + entityText[0] + '\\n'\n",
    "\n",
    "    response = utterance[::-1]\n",
    "    print(agent + \": \" + utterance)\n",
    "    textSignal = d_util.create_text_signal(scenario, utterance)\n",
    "    scenario.append_signal(textSignal)\n",
    "\n",
    "    ###### Getting the next input signals\n",
    "    utterance = input('\\n')\n",
    "\n",
    "    success, frame = camera.read()\n",
    "    if success:\n",
    "        imagepath = imagefolder + \"/\" + str(datetime.now().microsecond) + \".png\"\n",
    "        cv2.imwrite(imagepath, frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Saving the Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarioStorage.save_scenario(scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
