{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's chat with a friend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo chat with Leolani. Leolani uses face recognition and gender/age\n",
    "estimation.\n",
    "\n",
    "Don't forget to install emissor by `pip install .` at the root of this repo.\n",
    "Install the requirements `pip install -r requirements.txt`\n",
    "you might also have to run `python -m spacy download en`\n",
    "\n",
    "Occasionally you have to kill the docker containers if you force close the chat.\n",
    "`docker kill $(docker ps -q)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/Desktop/t-MA-Combots-2021/code/venv/lib/python3.7/site-packages/rdflib_jsonld/__init__.py:12: DeprecationWarning: The rdflib-jsonld package has been integrated into rdflib as of rdflib==6.0.1.  Please remove rdflib-jsonld from your project's dependencies.\n",
      "  DeprecationWarning,\n"
     ]
    }
   ],
   "source": [
    "#import jsonpickle\n",
    "import emissor as em\n",
    "#import spacy\n",
    "#import time\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from emissor.persistence import ScenarioStorage\n",
    "from emissor.representation.annotation import AnnotationType, Token, NER\n",
    "from emissor.representation.container import Index\n",
    "from emissor.representation.scenario import (\n",
    "    Modality,\n",
    "    ImageSignal,\n",
    "    TextSignal,\n",
    "    Mention,\n",
    "    Annotation,\n",
    "    Scenario,\n",
    ")\n",
    "import cv2\n",
    "import argparse\n",
    "#import python_on_whales\n",
    "import requests\n",
    "#import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "#### The next utils are needed for the interaction and creating triples and capsules\n",
    "import util.driver_util as d_util\n",
    "import util.capsule_util as c_util\n",
    "import util.face_util as f_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Link your camera\n",
    "camera = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard initialisation of a scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  ./data/2021-10-28-11:44:49  Created \n",
      "Directory  ./data/2021-10-28-11:44:49/image  Created \n"
     ]
    }
   ],
   "source": [
    "from random import getrandbits\n",
    "\n",
    "##### Setting the location\n",
    "place_id = getrandbits(8)\n",
    "location = requests.get(\"https://ipinfo.io\").json()\n",
    "\n",
    "##### Setting the agents\n",
    "agent = \"Leolani2\"\n",
    "human = \"Stranger\"\n",
    "\n",
    "### The name of your scenario\n",
    "scenario_id = datetime.today().strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "\n",
    "### Specify the path to an existing data folder where your scenario is created and saved as a subfolder\n",
    "scenario_path = \"./data\"\n",
    "\n",
    "### Define the folder where the images are saved\n",
    "imagefolder = scenario_path + \"/\" + scenario_id + \"/\" + \"image\"\n",
    "\n",
    "\n",
    "### Create the scenario folder, the json files and a scenarioStorage and scenario in memory\n",
    "scenarioStorage = d_util.create_scenario(scenario_path, scenario_id)\n",
    "scenario = scenarioStorage.create_scenario(scenario_id, datetime.now().microsecond, datetime.now().microsecond, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the docker containers for face detection and face property detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You only need to load the dockers once. The first time you load the docker, the images will be donwloaded from the DockerHub. This may take a few minutes depending on the speed of the internet connection. The images are cached in your local Docker installation.\n",
    "\n",
    "One the images are in your local Docker, they are loaded instantaniously. Once the docker is started you do not need to start it again and you can skip the next commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-28 09:29:12.348 INFO face_util - start_docker_container: starting a tae898/face-detection-recognition:v0.1 container ...\n",
      "2021-10-28 09:29:18.268 INFO face_util - start_docker_container: starting a tae898/age-gender:v0.2 container ...\n"
     ]
    }
   ],
   "source": [
    "container_fdr = f_util.start_docker_container(\"tae898/face-detection-recognition:v0.1\", 10002)\n",
    "container_ag = f_util.start_docker_container(\"tae898/age-gender:v0.2\", 10003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is a problem starting the dockers, you may need to kill them and start them again. Use the following command to kill and rerun the previous command. Note that if there are running already you should not restart. Starting it again gives an error that the port is occupied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!docker kill $(docker ps -q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are now set to make a new friend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Friends of Leolani are saved in the embeddings folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-28 11:44:55.315 INFO face_util - load_binary_image: ./data/2021-10-28-11:44:49/image/280755.png image loaded!\n",
      "2021-10-28 11:44:56.065 INFO face_util - run_face_api: got <Response [200]> from server!...\n",
      "2021-10-28 11:44:56.067 INFO face_util - run_face_api: 1 faces deteced!\n",
      "2021-10-28 11:44:56.109 INFO face_util - run_age_gender_api: got <Response [200]> from server!...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leolani2: Hi Peter. Nice to see you again :)\n",
      "Leolani2: How are you doing?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " I am great\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peter: I am great\n",
      "Leolani2: So you what do you want to talk about Peter\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " life\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-28 11:45:07.709 INFO face_util - load_binary_image: ./data/2021-10-28-11:44:49/image/674728.png image loaded!\n",
      "2021-10-28 11:45:08.449 INFO face_util - run_face_api: got <Response [200]> from server!...\n",
      "2021-10-28 11:45:08.451 INFO face_util - run_face_api: 1 faces deteced!\n",
      "2021-10-28 11:45:08.494 INFO face_util - run_age_gender_api: got <Response [200]> from server!...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leolani2: So you what do you want to talk about Peter\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " bye\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-28 11:45:11.009 INFO face_util - load_binary_image: ./data/2021-10-28-11:44:49/image/970983.png image loaded!\n",
      "2021-10-28 11:45:11.771 INFO face_util - run_face_api: got <Response [200]> from server!...\n",
      "2021-10-28 11:45:11.772 INFO face_util - run_face_api: 1 faces deteced!\n",
      "2021-10-28 11:45:11.814 INFO face_util - run_age_gender_api: got <Response [200]> from server!...\n"
     ]
    }
   ],
   "source": [
    "# First signals to get started\n",
    "success, frame = camera.read()\n",
    "imagepath = \"\"\n",
    "if success:\n",
    "    current_time = str(datetime.now().microsecond)\n",
    "    imagepath = imagefolder + \"/\" + current_time + \".png\"\n",
    "    cv2.imwrite(imagepath, frame)\n",
    "    (\n",
    "        genders,\n",
    "        ages,\n",
    "        bboxes,\n",
    "        faces_detected,\n",
    "        det_scores,embeddings,\n",
    "    ) = f_util.do_stuff_with_image(imagepath)\n",
    "\n",
    "    # Initial prompt by the system from which we create a TextSignal and store it\n",
    "\n",
    "    # Here we assume that only one face is in the image\n",
    "    # TODO: deal with multiple people.\n",
    "    for k, (gender, age, bbox, uuid_name, faceprob, embedding) in enumerate(\n",
    "        zip(genders, ages, bboxes, faces_detected, det_scores, embeddings)\n",
    "    ):\n",
    "        age = round(age[\"mean\"])\n",
    "        gender = \"male\" if gender[\"m\"] > 0.5 else \"female\"\n",
    "        bbox = [int(num) for num in bbox.tolist()]\n",
    "\n",
    "    assert k == 0\n",
    "\n",
    "    if uuid_name[\"name\"] is None:\n",
    "        ### This is a stranger\n",
    "        ### We create the agent response and store it as a text signal\n",
    "        response = (\n",
    "            f\"Hi there. We haven't met. I only know that \"\n",
    "            f\"your estimated age is {age} and that your estimated gender is \"\n",
    "            f\"{gender}. What's your name?\"\n",
    "        )\n",
    "        print(f\"{agent}: {response}\")\n",
    "        textSignal = d_util.create_text_signal(scenario, response)\n",
    "        scenario.append_signal(textSignal)\n",
    "\n",
    "        ### We take the response from the user and store it as a text signal\n",
    "        utterance = input(\"\\n\")\n",
    "        textSignal = d_util.create_text_signal(scenario, utterance)\n",
    "        scenario.append_signal(textSignal)\n",
    "\n",
    "        #### We hack the response to find the name of our new fiend\n",
    "        #### This name needs to be set in the scenario and assigned to the global variable human\n",
    "        human = \" \".join([foo.title() for foo in utterance.strip().split()])\n",
    "        human = \"_\".join(human.split())\n",
    "\n",
    "        #### We create the embedding\n",
    "        to_save = {\"uuid\": uuid_name[\"uuid\"], \"embedding\": embedding}\n",
    "\n",
    "\n",
    "        with open(f\"./friend_embeddings/{human}.pkl\", \"wb\") as stream:\n",
    "            pickle.dump(to_save, stream)\n",
    "        \n",
    "        ### The system responds to the processing of the new name input and stores it as a textsignal\n",
    "        print(agent + f\": Nice to meet you, {human}\")\n",
    "        response = f\": Nice to meet you, {human}\"\n",
    "        textSignal = d_util.create_text_signal(scenario, response)\n",
    "        scenario.append_signal(textSignal)\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "        ### We know this person\n",
    "        human = uuid_name['name']\n",
    "        response = f\"Hi {human}. Nice to see you again :)\"\n",
    "        print(f\"{agent}: {response}\")\n",
    "        textSignal = d_util.create_text_signal(scenario, response)\n",
    "        scenario.append_signal(textSignal)\n",
    "\n",
    "    response = \"How are you doing?\"\n",
    "    textSignal = d_util.create_text_signal(scenario, response)\n",
    "    scenario.append_signal(textSignal)\n",
    "\n",
    "    print(agent + \": \" + response)\n",
    "\n",
    "    utterance = input(\"\\n\")\n",
    "    print(human + \": \" + utterance)\n",
    "\n",
    "    while not (utterance.lower() == \"stop\" or utterance.lower() == \"bye\"):\n",
    "        textSignal = d_util.create_text_signal(scenario, utterance)\n",
    "        # @TODO\n",
    "        # Apply some processing to the textSignal and add annotations\n",
    "\n",
    "        scenario.append_signal(textSignal)\n",
    "\n",
    "        if success:\n",
    "            imageSignal = d_util.create_image_signal(scenario, imagepath)\n",
    "            container_id = str(uuid.uuid4())\n",
    "\n",
    "            for gender, age, bbox, name, faceprob in zip(\n",
    "                genders, ages, bboxes, faces_detected, det_scores\n",
    "            ):\n",
    "\n",
    "                age = round(age[\"mean\"])\n",
    "                gender = \"male\" if gender[\"m\"] > 0.5 else \"female\"\n",
    "                bbox = [int(num) for num in bbox.tolist()]\n",
    "\n",
    "                annotations = []\n",
    "\n",
    "                annotations.append(\n",
    "                    {\n",
    "                        \"source\": \"machine\",\n",
    "                        \"timestamp\": current_time,\n",
    "                        \"type\": \"person\",\n",
    "                        \"value\": {\n",
    "                            \"name\": name,\n",
    "                            \"age\": age,\n",
    "                            \"gender\": gender,\n",
    "                            \"faceprob\": faceprob,\n",
    "                        },\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                mention_id = str(uuid.uuid4())\n",
    "                segment = [\n",
    "                    {\"bounds\": bbox, \"container_id\": container_id, \"type\": \"MultiIndex\"}\n",
    "                ]\n",
    "                imageSignal.mentions.append(\n",
    "                    {\"annotations\": annotations, \"id\": mention_id, \"segment\": segment}\n",
    "                )\n",
    "\n",
    "            scenario.append_signal(imageSignal)\n",
    "\n",
    "        # Create the response from the system and store this as a new signal\n",
    "\n",
    "\n",
    "        utterance = \"So you what do you want to talk about \" + human + \"\\n\"\n",
    "        response = utterance[::-1]\n",
    "        print(agent + \": \" + utterance)\n",
    "        textSignal = d_util.create_text_signal(scenario, utterance)\n",
    "        scenario.append_signal(textSignal)\n",
    "\n",
    "        # Getting the next input signals\n",
    "        utterance = input(\"\\n\")\n",
    "\n",
    "        success, frame = camera.read()\n",
    "        if success:\n",
    "            current_time = str(datetime.now().microsecond)\n",
    "            imagepath = imagefolder + \"/\" + current_time + \".png\"\n",
    "            cv2.imwrite(imagepath, frame)\n",
    "            (\n",
    "                genders,\n",
    "                ages,\n",
    "                bboxes,\n",
    "                faces_detected,\n",
    "                det_scores,\n",
    "                embeddings,\n",
    "            ) = f_util.do_stuff_with_image(imagepath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the end time of the scenario, save it and stop the containers\n",
    "\n",
    "After we stopped the interaction, we set the end time and save the scenario as EMISSOR data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scenario.scenario.end = datetime.now().microsecond\n",
    "scenarioStorage.save_scenario(scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stopping the docker containers\n",
    "### This is only needed of you started them in this notebook\n",
    "\n",
    "#f_util.kill_container(container_fdr)\n",
    "#f_util.kill_container(container_ag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Stop the camera when we are done\n",
    "camera.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
