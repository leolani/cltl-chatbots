{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's chat with a friend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo chat with Leolani. Leolani uses face recognition and gender/age\n",
    "estimation.\n",
    "\n",
    "Don't forget to install emissor by `pip install .` at the root of this repo.\n",
    "Install the requirements `pip install -r requirements.txt`\n",
    "you might also have to run `python -m spacy download en`\n",
    "\n",
    "Occasionally you have to kill the docker containers if you force close the chat.\n",
    "`docker kill $(docker ps -q)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/Desktop/t-MA-Combots-2021/code/venv/lib/python3.7/site-packages/rdflib_jsonld/__init__.py:12: DeprecationWarning: The rdflib-jsonld package has been integrated into rdflib as of rdflib==6.0.1.  Please remove rdflib-jsonld from your project's dependencies.\n",
      "  DeprecationWarning,\n"
     ]
    }
   ],
   "source": [
    "import emissor as em\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from emissor.persistence import ScenarioStorage\n",
    "from emissor.representation.annotation import AnnotationType, Token, NER\n",
    "from emissor.representation.container import Index\n",
    "from emissor.representation.scenario import (\n",
    "    Modality,\n",
    "    ImageSignal,\n",
    "    TextSignal,\n",
    "    Mention,\n",
    "    Annotation,\n",
    "    Scenario,\n",
    ")\n",
    "import cv2\n",
    "import requests\n",
    "import pickle\n",
    "\n",
    "#### The next utils are needed for the interaction and creating triples and capsules\n",
    "import util.driver_util as d_util\n",
    "import util.capsule_util as c_util\n",
    "import util.face_util as f_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Link your camera\n",
    "camera = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard initialisation of a scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  ./data/2021-10-29-00:05:03  Created \n",
      "Directory  ./data/2021-10-29-00:05:03/image  Created \n"
     ]
    }
   ],
   "source": [
    "from random import getrandbits\n",
    "\n",
    "##### Setting the location\n",
    "place_id = getrandbits(8)\n",
    "location = requests.get(\"https://ipinfo.io\").json()\n",
    "\n",
    "##### Setting the agents\n",
    "agent = \"Leolani2\"\n",
    "human = \"Stranger\"\n",
    "\n",
    "### The name of your scenario\n",
    "scenario_id = datetime.today().strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "\n",
    "### Specify the path to an existing data folder where your scenario is created and saved as a subfolder\n",
    "scenario_path = \"./data\"\n",
    "\n",
    "### Define the folder where the images are saved\n",
    "imagefolder = scenario_path + \"/\" + scenario_id + \"/\" + \"image\"\n",
    "\n",
    "\n",
    "### Create the scenario folder, the json files and a scenarioStorage and scenario in memory\n",
    "scenarioStorage = d_util.create_scenario(scenario_path, scenario_id)\n",
    "scenario = scenarioStorage.create_scenario(scenario_id, datetime.now().microsecond, datetime.now().microsecond, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the docker containers for face detection and face property detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You only need to load the dockers once. The first time you load the docker, the images will be donwloaded from the DockerHub. This may take a few minutes depending on the speed of the internet connection. The images are cached in your local Docker installation.\n",
    "\n",
    "One the images are in your local Docker, they are loaded instantaniously. Once the docker is started you do not need to start it again and you can skip the next commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-28 09:29:12.348 INFO face_util - start_docker_container: starting a tae898/face-detection-recognition:v0.1 container ...\n",
      "2021-10-28 09:29:18.268 INFO face_util - start_docker_container: starting a tae898/age-gender:v0.2 container ...\n"
     ]
    }
   ],
   "source": [
    "#container_fdr = f_util.start_docker_container(\"tae898/face-detection-recognition:v0.1\", 10002)\n",
    "#container_ag = f_util.start_docker_container(\"tae898/age-gender:v0.2\", 10003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is a problem starting the dockers, you may need to kill them and start them again. Use the following command to kill and rerun the previous command. Note that if there are running already you should not restart. Starting it again gives an error that the port is occupied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!docker kill $(docker ps -q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are now set to make a new friend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Friends of Leolani are saved in the embeddings folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_to_know_person(scenario: Scenario, agent:str, gender:str, age: str, uuid_name: str, embedding):\n",
    "        ### This is a stranger\n",
    "        ### We create the agent response and store it as a text signal\n",
    "        human_name = \"Stranger\"\n",
    "        response = (\n",
    "            f\"Hi there. We haven't met. I only know that \\n\"\n",
    "            f\"your estimated age is {age} \\n and that your estimated gender is \"\n",
    "            f\"{gender}. What's your name?\"\n",
    "        )\n",
    "        print(f\"{agent}: {response}\")\n",
    "        textSignal = d_util.create_text_signal(scenario, response)\n",
    "        scenario.append_signal(textSignal)\n",
    "        \n",
    "        confirm = \"\"\n",
    "        while confirm.lower().find(\"yes\")==-1:\n",
    "            ### We take the response from the user and store it as a text signal\n",
    "            utterance = input(\"\\n\")\n",
    "            textSignal = d_util.create_text_signal(scenario, utterance)\n",
    "            scenario.append_signal(textSignal)\n",
    "            print(utterance)\n",
    "            #### We hack the response to find the name of our new fiend\n",
    "            #### This name needs to be set in the scenario and assigned to the global variable human\n",
    "            human_name = \" \".join([foo.title() for foo in utterance.strip().split()])\n",
    "            human_name = \"_\".join(human_name.split())\n",
    "        \n",
    "            response = (f\"So your name is {human_name}?\")\n",
    "            print(f\"{agent}: {response}\")\n",
    "            textSignal = d_util.create_text_signal(scenario, response)\n",
    "            scenario.append_signal(textSignal)\n",
    "            \n",
    "            ### We take the response from the user and store it as a text signal\n",
    "            confirm = input(\"\\n\")\n",
    "            textSignal = d_util.create_text_signal(scenario, confirm)\n",
    "            scenario.append_signal(textSignal)\n",
    "\n",
    "\n",
    "        current_time = str(datetime.now().microsecond)\n",
    "        human_id = human_name+\"_t_\"+current_time\n",
    "        #### We create the embedding\n",
    "        to_save = {\"uuid\": uuid_name[\"uuid\"], \"embedding\": embedding}\n",
    "\n",
    "        with open(f\"./friend_embeddings/{human_id}.pkl\", \"wb\") as stream:\n",
    "            pickle.dump(to_save, stream)\n",
    "            \n",
    "        return human_id, human_name, textSignal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 00:05:14.042 INFO face_util - load_binary_image: ./data/2021-10-29-00:05:03/image/2816.png image loaded!\n",
      "2021-10-29 00:05:14.800 INFO face_util - run_face_api: got <Response [200]> from server!...\n",
      "2021-10-29 00:05:14.801 INFO face_util - run_face_api: 1 faces deteced!\n",
      "2021-10-29 00:05:14.803 INFO face_util - face_recognition: new face!\n",
      "2021-10-29 00:05:14.883 INFO face_util - run_age_gender_api: got <Response [200]> from server!...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leolani2: Hi there. We haven't met. I only know that \n",
      "your estimated age is 34 \n",
      " and that your estimated gender is male. What's your name?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " James\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James\n",
      "Leolani2: So your name is James?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leolani2: Nice to meet you, James\n"
     ]
    }
   ],
   "source": [
    "# First signals to get started\n",
    "success, frame = camera.read()\n",
    "imagepath = \"\"\n",
    "if success:\n",
    "    current_time = str(datetime.now().microsecond)\n",
    "    imagepath = imagefolder + \"/\" + current_time + \".png\"\n",
    "    cv2.imwrite(imagepath, frame)\n",
    "    (\n",
    "        genders,\n",
    "        ages,\n",
    "        bboxes,\n",
    "        faces_detected,\n",
    "        det_scores,embeddings,\n",
    "    ) = f_util.do_stuff_with_image(imagepath)\n",
    "\n",
    "    # Initial prompt by the system from which we create a TextSignal and store it\n",
    "\n",
    "    # Here we assume that only one face is in the image\n",
    "    # TODO: deal with multiple people.\n",
    "    for k, (gender, age, bbox, uuid_name, faceprob, embedding) in enumerate(\n",
    "        zip(genders, ages, bboxes, faces_detected, det_scores, embeddings)\n",
    "    ):\n",
    "        age = round(age[\"mean\"])\n",
    "        gender = \"male\" if gender[\"m\"] > 0.5 else \"female\"\n",
    "        bbox = [int(num) for num in bbox.tolist()]\n",
    "\n",
    "    assert k == 0\n",
    "\n",
    "    if uuid_name[\"name\"] is None:\n",
    "        ### This is a stranger\n",
    "        ### We create the agent response and store it as a text signal\n",
    "        \n",
    "        human_id, human, textSignal = get_to_know_person(scenario, agent, gender, age, uuid_name, embedding)\n",
    "\n",
    "\n",
    "        ### The system responds to the processing of the new name input and stores it as a textsignal\n",
    "        print(agent + f\": Nice to meet you, {human}\")\n",
    "        response = f\": Nice to meet you, {human}\"\n",
    "        textSignal = d_util.create_text_signal(scenario, response)\n",
    "        scenario.append_signal(textSignal)\n",
    "\n",
    "    else:\n",
    "        ### We know this person\n",
    "        human_id= uuid_name['name']\n",
    "        human = human_id.split(\"_t_\")[0]\n",
    "        response = f\"Hi {human}. Nice to see you again :)\"\n",
    "        print(f\"{agent}: {response}\")\n",
    "        textSignal = d_util.create_text_signal(scenario, response)\n",
    "        scenario.append_signal(textSignal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leolani2: How are you doing James\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " Great\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James: Great\n",
      "Leolani2: So you what do you want to talk about James\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " All\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 00:06:22.843 INFO face_util - load_binary_image: ./data/2021-10-29-00:05:03/image/801455.png image loaded!\n",
      "2021-10-29 00:06:23.587 INFO face_util - run_face_api: got <Response [200]> from server!...\n",
      "2021-10-29 00:06:23.588 INFO face_util - run_face_api: 1 faces deteced!\n",
      "2021-10-29 00:06:23.631 INFO face_util - run_age_gender_api: got <Response [200]> from server!...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leolani2: So you what do you want to talk about James\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " Bye\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 00:06:28.324 INFO face_util - load_binary_image: ./data/2021-10-29-00:05:03/image/282335.png image loaded!\n",
      "2021-10-29 00:06:29.091 INFO face_util - run_face_api: got <Response [200]> from server!...\n",
      "2021-10-29 00:06:29.093 INFO face_util - run_face_api: 1 faces deteced!\n",
      "2021-10-29 00:06:29.139 INFO face_util - run_age_gender_api: got <Response [200]> from server!...\n"
     ]
    }
   ],
   "source": [
    "### First prompt\n",
    "response = \"How are you doing \"+human\n",
    "textSignal = d_util.create_text_signal(scenario, response)\n",
    "scenario.append_signal(textSignal)\n",
    "\n",
    "print(agent + \": \" + response)\n",
    "\n",
    "utterance = input(\"\\n\")\n",
    "print(human + \": \" + utterance)\n",
    "\n",
    "while not (utterance.lower() == \"stop\" or utterance.lower() == \"bye\"):\n",
    "    textSignal = d_util.create_text_signal(scenario, utterance)\n",
    "    scenario.append_signal(textSignal)\n",
    "\n",
    "    # @TODO: also annotate the textSignal\n",
    "    # Apply some processing to the textSignal and add annotations\n",
    "        \n",
    "        \n",
    "    ## We capture the image again\n",
    "    if success:\n",
    "        imageSignal = d_util.create_image_signal(scenario, imagepath)\n",
    "        container_id = str(uuid.uuid4())\n",
    "\n",
    "        #### Properties are now stored as annotations\n",
    "        #### We do not store these proeprties again to the BRAIN\n",
    "        for gender, age, bbox, name, faceprob in zip(\n",
    "            genders, ages, bboxes, faces_detected, det_scores\n",
    "        ):\n",
    "\n",
    "            age = round(age[\"mean\"])\n",
    "            gender = \"male\" if gender[\"m\"] > 0.5 else \"female\"\n",
    "            bbox = [int(num) for num in bbox.tolist()]\n",
    "        \n",
    "        f_util.add_face_annotation(imageSignal,\n",
    "                                       \"front_camera\",\n",
    "                                        str(uuid.uuid4(), \n",
    "                                        current_time,\n",
    "                                        bbox,\n",
    "                                        human_id,\n",
    "                                        human_name,\n",
    "                                        age, \n",
    "                                        gender, \n",
    "                                        faceprob)\n",
    "        scenario.append_signal(imageSignal)\n",
    "\n",
    "\n",
    "    # Create the response from the system and store this as a new signal\n",
    "    # We could use the throughts to respond\n",
    "    # @TODO generate a response from the thoughts\n",
    "\n",
    "    utterance = \"So you what do you want to talk about \" + human + \"\\n\"\n",
    "    response = utterance[::-1]\n",
    "    print(agent + \": \" + utterance)\n",
    "    textSignal = d_util.create_text_signal(scenario, utterance)\n",
    "    scenario.append_signal(textSignal)\n",
    "\n",
    "    # Getting the next input signals\n",
    "    utterance = input(\"\\n\")\n",
    "\n",
    "    success, frame = camera.read()\n",
    "    if success:\n",
    "        current_time = str(datetime.now().microsecond)\n",
    "        imagepath = imagefolder + \"/\" + current_time + \".png\"\n",
    "        cv2.imwrite(imagepath, frame)\n",
    "        (\n",
    "            genders,\n",
    "            ages,\n",
    "            bboxes,\n",
    "            faces_detected,\n",
    "            det_scores,\n",
    "            embeddings,\n",
    "        ) = f_util.do_stuff_with_image(imagepath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the end time of the scenario, save it and stop the containers\n",
    "\n",
    "After we stopped the interaction, we set the end time and save the scenario as EMISSOR data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scenario.scenario.end = datetime.now().microsecond\n",
    "scenarioStorage.save_scenario(scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stopping the docker containers\n",
    "### This is only needed of you started them in this notebook\n",
    "\n",
    "#f_util.kill_container(container_fdr)\n",
    "#f_util.kill_container(container_ag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Stop the camera when we are done\n",
    "camera.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
