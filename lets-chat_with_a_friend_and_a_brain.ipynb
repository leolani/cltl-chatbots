{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's chat with a friend and I have a brain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo chat with Leolani that combine face recognition and storage in the BRAIN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/piek/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# general imports for EMISSOR and the BRAIN\n",
    "import emissor as em\n",
    "from cltl import brain\n",
    "from cltl.triple_extraction.api import Chat, UtteranceHypothesis\n",
    "from emissor.persistence import ScenarioStorage\n",
    "from emissor.representation.annotation import AnnotationType, Token, NER\n",
    "from emissor.representation.container import Index\n",
    "from emissor.representation.scenario import Modality, ImageSignal, TextSignal, Mention, Annotation, Scenario\n",
    "from cltl.brain.long_term_memory import LongTermMemory\n",
    "from cltl.combot.backend.api.discrete import UtteranceType\n",
    "\n",
    "# specific imports\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "import requests\n",
    "import pickle\n",
    "\n",
    "#### The next utils are needed for the interaction and creating triples and capsules\n",
    "import util.driver_util as d_util\n",
    "import util.capsule_util as c_util\n",
    "import util.face_util as f_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Link your camera\n",
    "camera = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise the BRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pathlib.PosixPath'>\n",
      "2021-10-29 15:22:30,706 -    DEBUG -    cltl.brain.basic_brain.LongTermMemory - Booted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 15:22:30.706 DEBUG basic_brain - __init__: Booted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-29 15:22:30,708 -    DEBUG -    cltl.brain.basic_brain.LongTermMemory - Clearing brain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 15:22:30.708 DEBUG basic_brain - clear_brain: Clearing brain\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-29 15:22:33,072 -    DEBUG -    cltl.brain.basic_brain.LongTermMemory - Checking if ontology is in brain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 15:22:33.072 DEBUG basic_brain - ontology_is_uploaded: Checking if ontology is in brain\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-29 15:22:33,077 -    DEBUG -    cltl.brain.basic_brain.LongTermMemory - Posting query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 15:22:33.077 DEBUG basic_brain - _submit_query: Posting query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-29 15:22:33,788 -     INFO -    cltl.brain.basic_brain.LongTermMemory - Uploading ontology to brain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 15:22:33.788 INFO basic_brain - upload_ontology: Uploading ontology to brain\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-29 15:22:36,921 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Booted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 15:22:36.921 DEBUG basic_brain - __init__: Booted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-29 15:22:36,923 -    DEBUG -  cltl.brain.basic_brain.LocationReasoner - Booted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 15:22:36.923 DEBUG basic_brain - __init__: Booted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-29 15:22:36,926 -    DEBUG -      cltl.brain.basic_brain.TypeReasoner - Booted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 15:22:36.926 DEBUG basic_brain - __init__: Booted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-29 15:22:36,928 -    DEBUG -   cltl.brain.basic_brain.TrustCalculator - Booted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 15:22:36.928 DEBUG basic_brain - __init__: Booted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-29 15:22:37,223 -    DEBUG -   cltl.brain.basic_brain.TrustCalculator - Posting query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 15:22:37.223 DEBUG basic_brain - _submit_query: Posting query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-29 15:22:37,269 -     INFO -   cltl.brain.basic_brain.TrustCalculator - Computed trust for all known agents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 15:22:37.269 INFO trust_calculator - compute_trust_network: Computed trust for all known agents\n"
     ]
    }
   ],
   "source": [
    "# Initialise the brain in GraphDB\n",
    "import pathlib\n",
    "log_path=pathlib.Path('./logs')\n",
    "print(type(log_path))\n",
    "my_brain = brain.LongTermMemory(address=\"http://localhost:7200/repositories/sandbox\",\n",
    "                           log_dir=log_path,\n",
    "                           clear_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard initialisation of a scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  ./data/2021-10-29-15:22:42  Created \n",
      "Directory  ./data/2021-10-29-15:22:42/image  Created \n"
     ]
    }
   ],
   "source": [
    "from random import getrandbits\n",
    "\n",
    "##### Setting the location\n",
    "place_id = getrandbits(8)\n",
    "location = requests.get(\"https://ipinfo.io\").json()\n",
    "\n",
    "##### Setting the agents\n",
    "AGENT = \"Leolani2\"\n",
    "HUMAN_NAME = \"Stranger\"\n",
    "HUMAN_ID = \"stranger\"\n",
    "\n",
    "### The name of your scenario\n",
    "scenario_id = datetime.today().strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "\n",
    "### Specify the path to an existing data folder where your scenario is created and saved as a subfolder\n",
    "scenario_path = \"./data\"\n",
    "\n",
    "### Define the folder where the images are saved\n",
    "imagefolder = scenario_path + \"/\" + scenario_id + \"/\" + \"image\"\n",
    "\n",
    "\n",
    "### Create the scenario folder, the json files and a scenarioStorage and scenario in memory\n",
    "scenarioStorage = d_util.create_scenario(scenario_path, scenario_id)\n",
    "scenario = scenarioStorage.create_scenario(scenario_id, datetime.now().microsecond, datetime.now().microsecond, AGENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the docker containers for face detection and face property detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If the docker images are running you can skip the next part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You only need to load the dockers once. The first time you load the docker, the images will be donwloaded from the DockerHub. This may take a few minutes depending on the speed of the internet connection. The images are cached in your local Docker installation.\n",
    "\n",
    "One the images are in your local Docker, they are loaded instantaniously. Once the docker is started you do not need to start it again and you can skip the next commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#container_fdr = f_util.start_docker_container(\"tae898/face-detection-recognition:v0.1\", 10002)\n",
    "#container_ag = f_util.start_docker_container(\"tae898/age-gender:v0.2\", 10003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is a problem starting the dockers, you may need to kill them and start them again. Use the following command to kill and rerun the previous command. Note that if there are running already you should not restart. Starting it again gives an error that the port is occupied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!docker kill $(docker ps -q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting and storing triples from an utterance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function processes any textSignal using the baseline language model from the Leolani platform.\n",
    "This model uses a context-free grammar and closed-class lexicons specifically designed for social robot interaction.\n",
    "\n",
    "The function creates a capsule for posting any triples and perspective to the BRAIN.\n",
    "If no triples are extracted, a dummy prompt is returned: \"Any gossip?\".\n",
    "\n",
    "Any thoughts coming from the BRAIN are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_and_think (scenario: Scenario, \n",
    "                  place_id:str, \n",
    "                  location: str, \n",
    "                  textSignal: TextSignal,\n",
    "                  human_id: str,\n",
    "                  my_brain:LongTermMemory):\n",
    "    thoughts = \"\"\n",
    "    chat = Chat(human_id)\n",
    "    chat.add_utterance([UtteranceHypothesis(c_util.seq_to_text(textSignal.seq), 1.0)])\n",
    "    chat.last_utterance.analyze()\n",
    "    # No triple was extracted, so we missed three items (s, p, o)\n",
    "    if chat.last_utterance.triple is None:\n",
    "        utterance = \"Any gossip?\" + '\\n'\n",
    "    else:\n",
    "        triple = c_util.rephrase_triple_json_for_capsule(chat.last_utterance.triple)\n",
    "        # A triple was extracted so we compare it elementwise\n",
    "        capsule = c_util.scenario_utterance_and_triple_to_capsule(scenario, \n",
    "                                                                  place_id,\n",
    "                                                                  location,\n",
    "                                                                  textSignal, \n",
    "                                                                  human_id,\n",
    "                                                                  chat.last_utterance.perspective, \n",
    "                                                                  triple)\n",
    "        print('Capsule:', capsule)\n",
    "        thoughts = my_brain.update(capsule, reason_types=True)\n",
    "        #print(thoughts)\n",
    "    return thoughts    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new friend.\n",
    "\n",
    "The next functions are needed to get the properties and visual information for identifying a new friend.\n",
    "The visual information is based on the camera images of the uses from which we extract an averaged embedding.\n",
    "These embeddings are store in the *friend_embeddings* folder. \n",
    "\n",
    "By comparing an image with the stored embeddings, the system decides whether a person is a *stranger*.\n",
    "In case the user is a *stranger*, the system will try to get to know him/her.\n",
    "\n",
    "If you delete someone's embeddings from the *friend_embeddings* folder. This person will become a *stranger* again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function that creates capsules for the basic properties of a friend: name, age and gender.\n",
    "### The capsules are sent to the BRAIN. Thoughts are caught and returned for each property.\n",
    "\n",
    "def process_new_friend_and_think (scenario: Scenario, \n",
    "                  place_id:str, \n",
    "                  location: str, \n",
    "                  human_id: str,\n",
    "                  textSignal: TextSignal,\n",
    "                  imageSignal: ImageSignal,\n",
    "                  age: str,\n",
    "                  gender: str,\n",
    "                  human_name: str,\n",
    "                  my_brain:LongTermMemory):\n",
    "    age_thoughts = \"\"\n",
    "    gender_thoughts = \"\"\n",
    "    name_thoughts = \"\"\n",
    "\n",
    "\n",
    "    if human_name:\n",
    "        # A triple was extracted so we compare it elementwise\n",
    "        perspective = {\"certainty\": 1, \"polarity\": 1, \"sentiment\": 1}\n",
    "        capsule = c_util.scenario_utterance_to_capsule(scenario, \n",
    "                                                                  place_id,\n",
    "                                                                  location,\n",
    "                                                                  textSignal,\n",
    "                                                                  human_id,\n",
    "                                                                  perspective,\n",
    "                                                                  human_id,\n",
    "                                                                  \"label\",\n",
    "                                                                  human_name)\n",
    "\n",
    "        name_thoughts = my_brain.update(capsule, reason_types=True, create_label=False)\n",
    "        print('Name capsule:', capsule)\n",
    "\n",
    "\n",
    "    if age:\n",
    "        # A triple was extracted so we compare it elementwise\n",
    "        capsule = c_util.scenario_image_triple_to_capsule(scenario, \n",
    "                                                                  place_id,\n",
    "                                                                  location,\n",
    "                                                                  imageSignal,\n",
    "                                                                  \"front_camera\", \n",
    "                                                                  human_id,\n",
    "                                                                  \"age\",\n",
    "                                                                  str(age))\n",
    "\n",
    "        age_thoughts = my_brain.update(capsule, reason_types=True, create_label=False)\n",
    "        print('Age capsule:', capsule)\n",
    "\n",
    "    if gender:\n",
    "        capsule = c_util.scenario_image_triple_to_capsule(scenario, \n",
    "                                                                  place_id,\n",
    "                                                                  location,\n",
    "                                                                  imageSignal,\n",
    "                                                                  \"front_camera\", \n",
    "                                                                  human_id,\n",
    "                                                                  \"gender\",\n",
    "                                                                  gender)\n",
    "\n",
    "        gender_thoughts = my_brain.update(capsule, reason_types=True, create_label=False)\n",
    "        print('Gender capsule:', capsule)\n",
    "\n",
    "    return name_thoughts, age_thoughts, gender_thoughts    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function that tries to get the name for a new person. A while is used till the user is happy.\n",
    "### From the name a unique ID *human_id* is created by adding the time_stamp\n",
    "### We store the face embeddings in the friend_embeddings folder using the unique ID *human_id*\n",
    "### The function returns the human_name and the human_id.\n",
    "### Human_name is used to address the user, and human_id is used to store properties of the user\n",
    "\n",
    "def get_to_know_person(scenario: Scenario, agent:str, gender:str, age: str, uuid_name: str, embedding):\n",
    "        ### This is a stranger\n",
    "        ### We create the agent response and store it as a text signal\n",
    "        human_name = \"Stranger\"\n",
    "        response = (\n",
    "            f\"Hi there. We haven't met. I only know that \\n\"\n",
    "            f\"your estimated age is {age} \\n and that your estimated gender is \"\n",
    "            f\"{gender}. What's your name?\"\n",
    "        )\n",
    "        print(f\"{agent}: {response}\")\n",
    "        textSignal = d_util.create_text_signal(scenario, response)\n",
    "        scenario.append_signal(textSignal)\n",
    "        \n",
    "        confirm = \"\"\n",
    "        while confirm.lower().find(\"yes\")==-1:\n",
    "            ### We take the response from the user and store it as a text signal\n",
    "            utterance = input(\"\\n\")\n",
    "            textSignal = d_util.create_text_signal(scenario, utterance)\n",
    "            scenario.append_signal(textSignal)\n",
    "            print(utterance)\n",
    "            #### We hack the response to find the name of our new fiend\n",
    "            #### This name needs to be set in the scenario and assigned to the global variable human\n",
    "            human_name = \" \".join([foo.title() for foo in utterance.strip().split()])\n",
    "            human_name = \"_\".join(human_name.split())\n",
    "        \n",
    "            response = (f\"So your name is {human_name}?\")\n",
    "            print(f\"{agent}: {response}\")\n",
    "            textSignal = d_util.create_text_signal(scenario, response)\n",
    "            scenario.append_signal(textSignal)\n",
    "            \n",
    "            ### We take the response from the user and store it as a text signal\n",
    "            confirm = input(\"\\n\")\n",
    "            textSignal = d_util.create_text_signal(scenario, confirm)\n",
    "            scenario.append_signal(textSignal)\n",
    "\n",
    "\n",
    "        current_time = str(datetime.now().microsecond)\n",
    "        human_id = human_name+\"_t_\"+current_time\n",
    "        #### We create the embedding\n",
    "        to_save = {\"uuid\": uuid_name[\"uuid\"], \"embedding\": embedding}\n",
    "\n",
    "        with open(f\"./friend_embeddings/{human_id}.pkl\", \"wb\") as stream:\n",
    "            pickle.dump(to_save, stream)\n",
    "            \n",
    "        return human_id, human_name, textSignal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Friends of Leolani are saved in the embeddings folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 15:22:48.633 INFO face_util - load_binary_image: ./data/2021-10-29-15:22:42/image/592760.png image loaded!\n",
      "2021-10-29 15:22:49.365 INFO face_util - run_face_api: got <Response [200]> from server!...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "scalar() argument 1 must be numpy.dtype, not _IDProxy",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d04611596836>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mfaces_detected\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mdet_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     ) = f_util.do_stuff_with_image(imagepath)\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Here we assume that only one face is in the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/cltl-chatbots/util/face_util.py\u001b[0m in \u001b[0;36mdo_stuff_with_image\u001b[0;34m(image_path, url_face, url_age_gender)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mload_binary_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m     \u001b[0mbboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdet_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlandmarks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_face_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl_face\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0mfaces_detected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/cltl-chatbots/util/face_util.py\u001b[0m in \u001b[0;36mrun_face_api\u001b[0;34m(to_send, url_face)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_face\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_send\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"got {response} from server!...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsonpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0mface_detection_recognition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"face_detection_recognition\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/jsonpickle/unpickler.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(string, backend, context, keys, reset, safe, classes)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/jsonpickle/unpickler.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, obj, reset, classes)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swap_proxies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/jsonpickle/unpickler.py\u001b[0m in \u001b[0;36m_restore\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_restore_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/jsonpickle/unpickler.py\u001b[0m in \u001b[0;36m_restore_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    554\u001b[0m                     \u001b[0mstr_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_namestack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m                 \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_namestack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/jsonpickle/unpickler.py\u001b[0m in \u001b[0;36m_restore\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_restore_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/jsonpickle/unpickler.py\u001b[0m in \u001b[0;36m_restore_list\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mkref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m         \u001b[0mchildren\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m         \u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_obj_setvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/jsonpickle/unpickler.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mkref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m         \u001b[0mchildren\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m         \u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_obj_setvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/jsonpickle/unpickler.py\u001b[0m in \u001b[0;36m_restore\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_restore_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/jsonpickle/unpickler.py\u001b[0m in \u001b[0;36m_restore_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    554\u001b[0m                     \u001b[0mstr_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_namestack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m                 \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_namestack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/jsonpickle/unpickler.py\u001b[0m in \u001b[0;36m_restore\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_restore_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/jsonpickle/unpickler.py\u001b[0m in \u001b[0;36m_restore_reduce\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0mstage1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mstage1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: scalar() argument 1 must be numpy.dtype, not _IDProxy"
     ]
    }
   ],
   "source": [
    "#First step is to identify the speaker\n",
    "\n",
    "imagepath = \"\"\n",
    "uuid_name = \"\"\n",
    "# First signal\n",
    "success, frame = camera.read()\n",
    "\n",
    "if success:\n",
    "    ### check if we know the human\n",
    "    current_time = str(datetime.now().microsecond)\n",
    "    imagepath = imagefolder + \"/\" + current_time + \".png\"\n",
    "    cv2.imwrite(imagepath, frame)\n",
    "    imageSignal = d_util.create_image_signal(scenario, imagepath)\n",
    "    (\n",
    "        genders,\n",
    "        ages,\n",
    "        bboxes,\n",
    "        faces_detected,\n",
    "        det_scores,embeddings,\n",
    "    ) = f_util.do_stuff_with_image(imagepath)\n",
    "\n",
    "    # Here we assume that only one face is in the image\n",
    "    # TODO: deal with multiple people.\n",
    "    for k, (gender, age, bbox, uuid_name, faceprob, embedding) in enumerate(\n",
    "        zip(genders, ages, bboxes, faces_detected, det_scores, embeddings)\n",
    "    ):\n",
    "        age = round(age[\"mean\"])\n",
    "        gender = \"male\" if gender[\"m\"] > 0.5 else \"female\"\n",
    "        bbox = [int(num) for num in bbox.tolist()]\n",
    "    assert k == 0\n",
    "    if uuid_name[\"name\"] is None:\n",
    "        ### This is a stranger\n",
    "        ### We call the get_to_know function to create the data for a new friend\n",
    "        HUMAN_ID, HUMAN_NAME, textSignal = get_to_know_person(scenario, AGENT, gender, age, uuid_name, embedding)\n",
    "        ### We store the data in the BRAIN for the human_id that is returned            \n",
    "        name_thoughts, age_thoughts, gender_thoughts = process_new_friend_and_think (scenario, \n",
    "                  place_id, \n",
    "                  location, \n",
    "                  HUMAN_ID,\n",
    "                  textSignal,\n",
    "                  imageSignal,\n",
    "                  age,\n",
    "                  gender,\n",
    "                  HUMAN_NAME,\n",
    "                  my_brain)\n",
    "        \n",
    "        ### The system responds to the processing of the new name input and stores it as a textsignal\n",
    "        print(AGENT + f\": Nice to meet you, {HUMAN_NAME}\")\n",
    "        response = f\": Nice to meet you, {HUMAN_NAME}\"\n",
    "        textSignal = d_util.create_text_signal(scenario, response)\n",
    "        scenario.append_signal(textSignal)\n",
    "\n",
    "    else:\n",
    "        ### We know this person\n",
    "        HUMAN_ID= uuid_name['name']\n",
    "        HUMAN_NAME = HUMAN_ID.split(\"_t_\")[0]\n",
    "        response = f\"Hi {HUMAN_NAME}. Nice to see you again :)\"\n",
    "        print(f\"{AGENT}: {response}\")\n",
    "        textSignal = d_util.create_text_signal(scenario, response)\n",
    "        scenario.append_signal(textSignal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proceed to communicate with an identified friend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next while loop is for continuous communication with the identified user until *stop* or *bye*.\n",
    "We process the user input and the captured image sequentially step by setp in the code block of the while loop.\n",
    "At the end of the code block, a response is generated and new input from the user is requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leolani2: How are you doing Piek\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " I am great.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Piek: I am great.\n",
      "2021-10-29 10:23:32,112 -     INFO - cltl.triple_extraction.api.Chat (Piek_t_191694)     000 - << Start of Chat with Piek_t_191694 >>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:32.112 INFO api - __init__: << Start of Chat with Piek_t_191694 >>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:32,114 -     INFO -               cltl.triple_extraction.api - Started POS tagger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:32.114 INFO api - __init__: Started POS tagger\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:32,116 -     INFO -               cltl.triple_extraction.api - Started NER tagger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:32.129 INFO ner - _start_server: Started NER server\n",
      "2021-10-29 10:23:32.116 INFO api - __init__: Started NER tagger\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:32,133 -     INFO -               cltl.triple_extraction.api - Loaded grammar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:32.133 INFO api - __init__: Loaded grammar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:33,274 -     INFO - cltl.triple_extraction.api.Chat (Piek_t_191694)     001 - Piek_t_191694: \"I am great.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:33.274 INFO api - add_utterance: Piek_t_191694: \"I am great.\"\n",
      "2021-10-29 10:23:33.288 INFO ner - _start_server: Started NER server\n",
      "2021-10-29 10:23:34.670 INFO analyzer - __init__: extracted perspective: {'sentiment': 0, 'certainty': 1, 'polarity': 1, 'emotion': <Emotion.NEUTRAL: 7>}\n",
      "2021-10-29 10:23:34.674 INFO api - analyze: RDF    subject: {\"label\": \"Piek_t_191694\", \"type\": [\"agent\"]}\n",
      "2021-10-29 10:23:34.675 INFO api - analyze: RDF  predicate: {\"label\": \"be\", \"type\": [\"verb.stative\"]}\n",
      "2021-10-29 10:23:34.675 INFO api - analyze: RDF     object: {\"label\": \"great.\", \"type\": [\"agent\"]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subject': {'label': 'Piek_t_191694', 'type': ['agent']}, 'predicate': {'label': 'be', 'type': ['verb.stative']}, 'object': {'label': 'great.', 'type': ['agent']}}\n",
      "{'subject': {'Piek_t_191694', 'agent'}, 'predicate': {'be', 'stative'}, 'object': {'agent', 'great.'}}\n",
      "Capsule: {'chat': '2021-10-29-10:19:30', 'turn': '1dac8d02-0844-4c9e-9b7a-ba7c4e83c543', 'author': 'Piek_t_191694', 'utterance': 'I am great.', 'utterance_type': <UtteranceType.STATEMENT: 0>, 'position': '0-11', 'subject': {'label': 'piek', 'type': 'person'}, 'predicate': {'type': 'see'}, 'object': {'label': 'pills', 'type': 'object'}, 'perspective': {'sentiment': 0, 'certainty': 1, 'polarity': 1, 'emotion': <Emotion.NEUTRAL: 7>}, 'context_id': 'Leolani2', 'date': datetime.date(2021, 10, 29), 'place': 'Amsterdam', 'place_id': 141, 'country': 'NL', 'region': 'North Holland', 'city': 'Amsterdam', 'objects': [{'type': 'chair', 'confidence': 0.59, 'id': 1}, {'type': 'table', 'confidence': 0.73, 'id': 1}, {'type': 'pillbox', 'confidence': 0.32, 'id': 1}], 'people': [{'name': 'Piek_t_191694', 'confidence': 0.98, 'id': 1}]}\n",
      "2021-10-29 10:23:34,681 -    DEBUG -    cltl.brain.basic_brain.LongTermMemory - Posting query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:34.681 DEBUG basic_brain - _submit_query: Posting query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:34,687 -    DEBUG -  cltl.brain.basic_brain.LocationReasoner - Posting query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:34.687 DEBUG basic_brain - _submit_query: Posting query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:34,705 -     INFO -    cltl.brain.basic_brain.LongTermMemory - Triple in statement: piek_see_pills [person_->_object])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:34.705 INFO LTM_statement_processing - model_graphs: Triple in statement: piek_see_pills [person_->_object])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:34,706 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:34.706 DEBUG basic_brain - _submit_query: Posting query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:34,735 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:34.735 DEBUG basic_brain - _submit_query: Posting query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:34,741 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:34.741 DEBUG basic_brain - _submit_query: Posting query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:34,785 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Entity Novelty: new subject - new object \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:34.785 INFO thought_generator - fill_entity_novelty: Entity Novelty: new subject - new object \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:34,787 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:34.787 DEBUG basic_brain - _submit_query: Posting query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:34,794 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:34.794 DEBUG basic_brain - _submit_query: Posting query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:35,685 -    DEBUG -    cltl.brain.basic_brain.LongTermMemory - Posting triples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:35.685 DEBUG basic_brain - _upload_to_brain: Posting triples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:36,787 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:36.787 DEBUG basic_brain - _submit_query: Posting query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:36,794 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Negation Conflicts: piek-t-191694 on October,2021 about POSITIVE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:36.794 INFO thought_generator - get_negation_conflicts: Negation Conflicts: piek-t-191694 on October,2021 about POSITIVE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:36,796 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:36.796 DEBUG basic_brain - _submit_query: Posting query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:36,838 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:36.838 DEBUG basic_brain - _submit_query: Posting query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:36,894 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Gaps: 26 gaps as subject: e.g. be-family-of person - 17 gaps as object: e.g. read-by book\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:36.894 INFO thought_generator - get_entity_gaps: Gaps: 26 gaps as subject: e.g. be-family-of person - 17 gaps as object: e.g. read-by book\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:36,896 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:36.896 DEBUG basic_brain - _submit_query: Posting query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:36,936 -    DEBUG -  cltl.brain.basic_brain.ThoughtGenerator - Posting query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:36.936 DEBUG basic_brain - _submit_query: Posting query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:36,941 -     INFO -  cltl.brain.basic_brain.ThoughtGenerator - Gaps: 0 gaps as subject: e.g. '' - 2 gaps as object: e.g. own agent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:36.941 INFO thought_generator - get_entity_gaps: Gaps: 0 gaps as subject: e.g. '' - 2 gaps as object: e.g. own agent\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:36,943 -    DEBUG -   cltl.brain.basic_brain.TrustCalculator - Posting query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:36.943 DEBUG basic_brain - _submit_query: Posting query\n",
      "2021-10-29 10:23:37.047 INFO face_util - load_binary_image: ./data/2021-10-29-10:19:30/image/12736.png image loaded!\n",
      "2021-10-29 10:23:37.779 INFO face_util - run_face_api: got <Response [200]> from server!...\n",
      "2021-10-29 10:23:37.780 INFO face_util - run_face_api: 1 faces deteced!\n",
      "2021-10-29 10:23:37.831 INFO face_util - run_age_gender_api: got <Response [200]> from server!...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leolani2: So you what do you want to talk about Piek\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " Universe and about you\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:46,634 -     INFO - cltl.triple_extraction.api.Chat (Piek_t_191694)     000 - << Start of Chat with Piek_t_191694 >>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:46.634 INFO api - __init__: << Start of Chat with Piek_t_191694 >>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:47,475 -     INFO - cltl.triple_extraction.api.Chat (Piek_t_191694)     001 - Piek_t_191694: \"Universe and about you\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:23:47.475 INFO api - add_utterance: Piek_t_191694: \"Universe and about you\"\n",
      "2021-10-29 10:23:47.477 WARNING analyzer - analyze: Couldn't parse input\n",
      "2021-10-29 10:23:47.543 INFO face_util - load_binary_image: ./data/2021-10-29-10:19:30/image/510434.png image loaded!\n",
      "2021-10-29 10:23:48.296 INFO face_util - run_face_api: got <Response [200]> from server!...\n",
      "2021-10-29 10:23:48.297 INFO face_util - run_face_api: 1 faces deteced!\n",
      "2021-10-29 10:23:48.342 INFO face_util - run_age_gender_api: got <Response [200]> from server!...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leolani2: So you what do you want to talk about Piek\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " stop\n"
     ]
    }
   ],
   "source": [
    "### First prompt\n",
    "response = \"How are you doing \"+HUMAN_NAME\n",
    "textSignal = d_util.create_text_signal(scenario, response)\n",
    "scenario.append_signal(textSignal)\n",
    "\n",
    "print(AGENT + \": \" + response)\n",
    "\n",
    "#### Get the input from the user\n",
    "utterance = input(\"\\n\")\n",
    "print(HUMAN_NAME + \": \" + utterance)\n",
    "\n",
    "while not (utterance.lower() == \"stop\" or utterance.lower() == \"bye\"):\n",
    "    textSignal = d_util.create_text_signal(scenario, utterance)\n",
    "    scenario.append_signal(textSignal)\n",
    "\n",
    "    # @TODO: also annotate the textSignal\n",
    "    # Apply some processing to the textSignal and add annotations\n",
    "        \n",
    "    # We process the utterance and store triples in the brain\n",
    "    # We catch the thoughts as the response\n",
    "    thoughts = process_text_and_think (scenario, \n",
    "                  place_id, \n",
    "                  location, \n",
    "                  textSignal, \n",
    "                  HUMAN_ID, my_brain)\n",
    "        \n",
    "    ## We capture the image again\n",
    "    ## For now, we only take pictures of faces of the user\n",
    "    ## @TODO add object recognition and check if there are other people detected than the user\n",
    "   \n",
    "    success, frame = camera.read()\n",
    "    if success:\n",
    "        current_time = str(datetime.now().microsecond)\n",
    "        imagepath = imagefolder + \"/\" + current_time + \".png\"\n",
    "        cv2.imwrite(imagepath, frame)\n",
    "        (\n",
    "            genders,\n",
    "            ages,\n",
    "            bboxes,\n",
    "            faces_detected,\n",
    "            det_scores,\n",
    "            embeddings,\n",
    "        ) = f_util.do_stuff_with_image(imagepath)\n",
    "\n",
    "\n",
    "\n",
    "    if success:\n",
    "        imageSignal = d_util.create_image_signal(scenario, imagepath)\n",
    "        container_id = str(uuid.uuid4())\n",
    "\n",
    "        #### Properties are now stored as annotations\n",
    "        #### We do not store these proeprties again to the BRAIN\n",
    "        for gender, age, bbox, name, faceprob in zip(\n",
    "            genders, ages, bboxes, faces_detected, det_scores\n",
    "        ):\n",
    "\n",
    "            age = round(age[\"mean\"])\n",
    "            gender = \"male\" if gender[\"m\"] > 0.5 else \"female\"\n",
    "            bbox = [int(num) for num in bbox.tolist()]\n",
    "\n",
    "        f_util.add_face_annotation(imageSignal,container_id, \"front_camera\", container_id, current_time,\n",
    "                                        bbox,HUMAN_ID,HUMAN_NAME, age, gender,faceprob)\n",
    "                                   \n",
    "        scenario.append_signal(imageSignal)\n",
    "        \n",
    "        ### We created a perceivedBy triple for this experience, \n",
    "        ### @TODO we need to include the bouding box somehow in the object\n",
    "        capsule = c_util.scenario_image_triple_to_capsule(scenario, \n",
    "                                                                  place_id,\n",
    "                                                                  location,\n",
    "                                                                  imageSignal,\n",
    "                                                                  \"front_camera\", \n",
    "                                                                  human_id,\n",
    "                                                                  \"perceivedBy\",\n",
    "                                                                  imageSignal.id)\n",
    "\n",
    "         response = my_brain.update(capsule, reason_types=True, create_label=False) ## use this version if you want to use the URI as subject\n",
    "\n",
    "    # Create the response from the system and store this as a new signal\n",
    "    # We could use the throughts to respond\n",
    "    # @TODO generate a response from the thoughts or based on the user query\n",
    "\n",
    "    utterance = \"So you what do you want to talk about \" + HUMAN_NAME + \"\\n\"\n",
    "    response = utterance[::-1]\n",
    "    print(AGENT + \": \" + utterance)\n",
    "    textSignal = d_util.create_text_signal(scenario, utterance)\n",
    "    scenario.append_signal(textSignal)\n",
    "\n",
    "    # Getting the next input signals\n",
    "    utterance = input(\"\\n\")\n",
    "\n",
    "    ### We now have a new input check if the user wants to continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the end time of the scenario, save it and stop the containers\n",
    "\n",
    "After we stopped the interaction, we set the end time and save the scenario as EMISSOR data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scenario.scenario.end = datetime.now().microsecond\n",
    "scenarioStorage.save_scenario(scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stopping the docker containers\n",
    "### This is only needed of you started them in this notebook\n",
    "\n",
    "#f_util.kill_container(container_fdr)\n",
    "#f_util.kill_container(container_ag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Stop the camera when we are done\n",
    "camera.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
